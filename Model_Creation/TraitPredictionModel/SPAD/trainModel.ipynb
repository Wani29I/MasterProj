{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 1,
>>>>>>> 3308362f7ef1a1be0a62cfee8b161b6a0d2f5d0c
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/.pyenv/versions/3.10.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from ModelArchitecture.ConvNeXtTinyModel import ConvNeXtTinyWheatModelWithConfidence\n",
    "from ModelArchitecture.DenseNetModel import DenseNet121WheatModel\n",
    "from ModelArchitecture.EfficientNetV2Model import EfficientNetV2SWheatCountWithConfidence\n",
    "from ModelArchitecture.RepVGGA1Model import RepVGGA1WheatModelWithConfidence\n",
    "from dataLoaderFunc import loadSplitData, createLoader\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 2,
>>>>>>> 3308362f7ef1a1be0a62cfee8b161b6a0d2f5d0c
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Custom Gaussian NLL Loss\n",
    "def gaussian_nll_loss(pred_mean, pred_logvar, target):\n",
    "    precision = torch.exp(-pred_logvar)\n",
    "    return torch.mean(precision * (target - pred_mean)**2 + pred_logvar)\n",
    "\n",
    "# ✅ Laplace NLL Loss (Robust + Confidence-Aware)\n",
    "def laplace_nll_loss(pred_mean, pred_logvar, target):\n",
    "    scale = torch.exp(pred_logvar)  # predicted Laplace scale\n",
    "    loss = torch.abs(target - pred_mean) / scale + pred_logvar\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# ✅ Training Function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, fileName, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_batch, dsm_batch)  # output: [B, 2]\n",
    "            pred_mean = output[:, 0]\n",
    "            pred_logvar = output[:, 1]\n",
    "            loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                output = model(rgb_batch, dsm_batch)\n",
    "                pred_mean = output[:, 0]\n",
    "                pred_logvar = output[:, 1]\n",
    "                loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save model\n",
    "        torch.save(model.state_dict(), f\"{fileName}{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "# ✅ Full Training Function\n",
    "def train_model_laplace(model, train_loader, val_loader, optimizer, scheduler, device, fileName,  num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for rgb_batch, dsm_batch, label_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            rgb_batch = rgb_batch.to(device)\n",
    "            dsm_batch = dsm_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_batch, dsm_batch)  # [B, 2]\n",
    "            pred_mean = output[:, 0]\n",
    "            pred_logvar = output[:, 1]\n",
    "\n",
    "            loss = laplace_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch = rgb_batch.to(device)\n",
    "                dsm_batch = dsm_batch.to(device)\n",
    "                label_batch = label_batch.to(device)\n",
    "\n",
    "                output = model(rgb_batch, dsm_batch)\n",
    "                pred_mean = output[:, 0]\n",
    "                pred_logvar = output[:, 1]\n",
    "\n",
    "                loss = laplace_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save Model\n",
    "        torch.save(model.state_dict(), f\"{fileName}{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 3,
>>>>>>> 3308362f7ef1a1be0a62cfee8b161b6a0d2f5d0c
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 42848, Validation Size: 5356, Test Size: 5356\n",
      "Train Batches: 2678, Validation Batches: 335, Test Batches: 335\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_SPAD.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df, traitName = \"SPAD\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 4,
>>>>>>> 3308362f7ef1a1be0a62cfee8b161b6a0d2f5d0c
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "✅ Using device: cuda\n"
=======
      "✅ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n"
>>>>>>> 3308362f7ef1a1be0a62cfee8b161b6a0d2f5d0c
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ✅ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ✅ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ✅ Default to CPU if no GPU is available\n",
    "print(f\"✅ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNetV2Model = EfficientNetV2SWheatCountWithConfidence().to(device)\n",
    "optimizer = optim.Adam(EfficientNetV2Model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(EfficientNetV2Model, train_loader, val_loader, optimizer, scheduler, device, \"EfficientNetV2LaplaceModel\",  num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvNeXtTinyModel = ConvNeXtTinyWheatModelWithConfidence().to(device)\n",
    "optimizer = optim.Adam(ConvNeXtTinyModel.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(ConvNeXtTinyModel, train_loader, val_loader, optimizer, scheduler, device, \"ConvNeXtTinyLaplaceModel\", num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 2678/2678 [18:26<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 | Train Loss: 3.2660 | Val Loss: 2.1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2678/2678 [13:12<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 | Train Loss: 2.7113 | Val Loss: 2.3772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 2678/2678 [09:57<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 | Train Loss: 2.5815 | Val Loss: 2.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 2678/2678 [09:50<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 | Train Loss: 2.4992 | Val Loss: 1.9641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 2678/2678 [09:48<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 | Train Loss: 2.4377 | Val Loss: 1.9718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 2678/2678 [09:51<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 | Train Loss: 2.3953 | Val Loss: 1.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 2678/2678 [09:54<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 | Train Loss: 2.3622 | Val Loss: 1.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 2678/2678 [09:51<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 | Train Loss: 2.3396 | Val Loss: 1.8798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 2678/2678 [09:55<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 | Train Loss: 2.3068 | Val Loss: 1.7749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 2678/2678 [09:59<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 | Train Loss: 2.2902 | Val Loss: 1.7717\n"
     ]
    }
   ],
   "source": [
    "RepVGGA1Model = RepVGGA1WheatModelWithConfidence().to(device)\n",
    "optimizer = optim.Adam(RepVGGA1Model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(RepVGGA1Model, train_loader, val_loader, optimizer, scheduler, device, \"RepVGGA1LaplaceModel\", num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNetModel = DenseNet121WheatModel().to(device)\n",
    "optimizer = optim.Adam(DenseNetModel.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(DenseNetModel, train_loader, val_loader, optimizer, scheduler, device, \"DenseNetLaplaceModel\", num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
