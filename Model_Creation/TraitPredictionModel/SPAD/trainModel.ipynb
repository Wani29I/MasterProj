{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from ModelArchitecture.ConvNeXtTinyModel import ConvNeXtTinyWheatModelWithConfidence\n",
    "from ModelArchitecture.DenseNetModel import DenseNet121WheatModel\n",
    "from ModelArchitecture.EfficientNetV2Model import EfficientNetV2SWheatCountWithConfidence\n",
    "from ModelArchitecture.RepVGGA1Model import RepVGGA1WheatModelWithConfidence\n",
    "from dataLoaderFunc import loadSplitData, createLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Custom Gaussian NLL Loss\n",
    "def gaussian_nll_loss(pred_mean, pred_logvar, target):\n",
    "    precision = torch.exp(-pred_logvar)\n",
    "    return torch.mean(precision * (target - pred_mean)**2 + pred_logvar)\n",
    "\n",
    "# ✅ Laplace NLL Loss (Robust + Confidence-Aware)\n",
    "def laplace_nll_loss(pred_mean, pred_logvar, target):\n",
    "    scale = torch.exp(pred_logvar)  # predicted Laplace scale\n",
    "    loss = torch.abs(target - pred_mean) / scale + pred_logvar\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# ✅ Training Function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, fileName, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_batch, dsm_batch)  # output: [B, 2]\n",
    "            pred_mean = output[:, 0]\n",
    "            pred_logvar = output[:, 1]\n",
    "            loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                output = model(rgb_batch, dsm_batch)\n",
    "                pred_mean = output[:, 0]\n",
    "                pred_logvar = output[:, 1]\n",
    "                loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save model\n",
    "        torch.save(model.state_dict(), f\"{fileName}{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "# ✅ Full Training Function\n",
    "def train_model_laplace(model, train_loader, val_loader, optimizer, scheduler, device, fileName,  num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for rgb_batch, dsm_batch, label_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            rgb_batch = rgb_batch.to(device)\n",
    "            dsm_batch = dsm_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_batch, dsm_batch)  # [B, 2]\n",
    "            pred_mean = output[:, 0]\n",
    "            pred_logvar = output[:, 1]\n",
    "\n",
    "            loss = laplace_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch = rgb_batch.to(device)\n",
    "                dsm_batch = dsm_batch.to(device)\n",
    "                label_batch = label_batch.to(device)\n",
    "\n",
    "                output = model(rgb_batch, dsm_batch)\n",
    "                pred_mean = output[:, 0]\n",
    "                pred_logvar = output[:, 1]\n",
    "\n",
    "                loss = laplace_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save Model\n",
    "        torch.save(model.state_dict(), f\"{fileName}{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"C:/Users/pacha/Desktop/masterProj/MasterProj/Model_Creation/TraitPredictionModel/SPAD/RGB_DSM_SPAD.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df, traitName = \"SPAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ✅ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ✅ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ✅ Default to CPU if no GPU is available\n",
    "print(f\"✅ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNetV2Model = EfficientNetV2SWheatCountWithConfidence().to(device)\n",
    "optimizer = optim.Adam(EfficientNetV2Model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(EfficientNetV2Model, train_loader, val_loader, optimizer, scheduler, device, \"EfficientNetV2LaplaceModel\",  num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvNeXtTinyModel = ConvNeXtTinyWheatModelWithConfidence().to(device)\n",
    "optimizer = optim.Adam(ConvNeXtTinyModel.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(ConvNeXtTinyModel, train_loader, val_loader, optimizer, scheduler, device, \"ConvNeXtTinyLaplaceModel\", num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RepVGGA1Model = RepVGGA1WheatModelWithConfidence().to(device)\n",
    "optimizer = optim.Adam(RepVGGA1Model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(RepVGGA1Model, train_loader, val_loader, optimizer, scheduler, device, \"RepVGGA1LaplaceModel\", num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNetModel = DenseNet121WheatModel().to(device)\n",
    "optimizer = optim.Adam(DenseNetModel.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(DenseNetModel, train_loader, val_loader, optimizer, scheduler, device, \"DenseNetLaplaceModel\", num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
