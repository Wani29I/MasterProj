{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from efficientNetV2Model import EfficientNetV2SWheatModel\n",
    "from dataLoaderFunc import loadSplitData, createLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Compute Validation Loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                outputs = model(rgb_batch, dsm_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save Best Model\n",
    "        torch.save(model.state_dict(), \"efficientNetV2_wheat_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Initialize Model, Loss Function, and Optimizer\n",
    "\n",
    "# ✅ Universal device selection (works on both Mac M2 & Windows with RTX 4060)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ✅ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ✅ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ✅ Default to CPU if no GPU is available\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "model = EfficientNetV2SWheatModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower learning rate for EfficientNet\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "# ✅ Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 0/2990 | Loss: 112145.0703\n",
      "Epoch 1/10 | Batch 20/2990 | Loss: 42102.0312\n",
      "Epoch 1/10 | Batch 40/2990 | Loss: 9607.5234\n",
      "Epoch 1/10 | Batch 60/2990 | Loss: 5663.7197\n",
      "Epoch 1/10 | Batch 80/2990 | Loss: 17251.5469\n",
      "Epoch 1/10 | Batch 100/2990 | Loss: 6245.5566\n",
      "Epoch 1/10 | Batch 120/2990 | Loss: 5277.8018\n",
      "Epoch 1/10 | Batch 140/2990 | Loss: 8148.7012\n",
      "Epoch 1/10 | Batch 160/2990 | Loss: 9858.2168\n",
      "Epoch 1/10 | Batch 180/2990 | Loss: 5364.1572\n",
      "Epoch 1/10 | Batch 200/2990 | Loss: 8010.6533\n",
      "Epoch 1/10 | Batch 220/2990 | Loss: 3866.4529\n",
      "Epoch 1/10 | Batch 240/2990 | Loss: 3886.4302\n",
      "Epoch 1/10 | Batch 260/2990 | Loss: 2637.2451\n",
      "Epoch 1/10 | Batch 280/2990 | Loss: 6688.7969\n",
      "Epoch 1/10 | Batch 300/2990 | Loss: 2031.9623\n",
      "Epoch 1/10 | Batch 320/2990 | Loss: 3313.1519\n",
      "Epoch 1/10 | Batch 340/2990 | Loss: 3343.2329\n",
      "Epoch 1/10 | Batch 360/2990 | Loss: 4436.6436\n",
      "Epoch 1/10 | Batch 380/2990 | Loss: 6868.3984\n",
      "Epoch 1/10 | Batch 400/2990 | Loss: 4142.9385\n",
      "Epoch 1/10 | Batch 420/2990 | Loss: 3357.2744\n",
      "Epoch 1/10 | Batch 440/2990 | Loss: 6699.8921\n",
      "Epoch 1/10 | Batch 460/2990 | Loss: 5514.4326\n",
      "Epoch 1/10 | Batch 480/2990 | Loss: 5520.8750\n",
      "Epoch 1/10 | Batch 500/2990 | Loss: 5140.4297\n",
      "Epoch 1/10 | Batch 520/2990 | Loss: 3486.4072\n",
      "Epoch 1/10 | Batch 540/2990 | Loss: 3508.8831\n",
      "Epoch 1/10 | Batch 560/2990 | Loss: 5577.0850\n",
      "Epoch 1/10 | Batch 580/2990 | Loss: 3687.2627\n",
      "Epoch 1/10 | Batch 600/2990 | Loss: 5393.7471\n",
      "Epoch 1/10 | Batch 620/2990 | Loss: 5015.2910\n",
      "Epoch 1/10 | Batch 640/2990 | Loss: 4067.4736\n",
      "Epoch 1/10 | Batch 660/2990 | Loss: 2403.2849\n",
      "Epoch 1/10 | Batch 680/2990 | Loss: 4329.2178\n",
      "Epoch 1/10 | Batch 700/2990 | Loss: 3753.2617\n",
      "Epoch 1/10 | Batch 720/2990 | Loss: 7884.4111\n",
      "Epoch 1/10 | Batch 740/2990 | Loss: 1631.1276\n",
      "Epoch 1/10 | Batch 760/2990 | Loss: 2093.5103\n",
      "Epoch 1/10 | Batch 780/2990 | Loss: 2932.2424\n",
      "Epoch 1/10 | Batch 800/2990 | Loss: 3781.3247\n",
      "Epoch 1/10 | Batch 820/2990 | Loss: 1656.4142\n",
      "Epoch 1/10 | Batch 840/2990 | Loss: 2867.2063\n",
      "Epoch 1/10 | Batch 860/2990 | Loss: 2527.0483\n",
      "Epoch 1/10 | Batch 880/2990 | Loss: 2899.5073\n",
      "Epoch 1/10 | Batch 900/2990 | Loss: 1925.6897\n",
      "Epoch 1/10 | Batch 920/2990 | Loss: 3396.6274\n",
      "Epoch 1/10 | Batch 940/2990 | Loss: 1317.3169\n",
      "Epoch 1/10 | Batch 960/2990 | Loss: 2200.1372\n",
      "Epoch 1/10 | Batch 980/2990 | Loss: 1195.2903\n",
      "Epoch 1/10 | Batch 1000/2990 | Loss: 2733.1953\n",
      "Epoch 1/10 | Batch 1020/2990 | Loss: 3895.8735\n",
      "Epoch 1/10 | Batch 1040/2990 | Loss: 1775.3640\n",
      "Epoch 1/10 | Batch 1060/2990 | Loss: 2462.8193\n",
      "Epoch 1/10 | Batch 1080/2990 | Loss: 1780.0085\n",
      "Epoch 1/10 | Batch 1100/2990 | Loss: 2671.5005\n",
      "Epoch 1/10 | Batch 1120/2990 | Loss: 2396.6907\n",
      "Epoch 1/10 | Batch 1140/2990 | Loss: 3192.9858\n",
      "Epoch 1/10 | Batch 1160/2990 | Loss: 1974.4902\n",
      "Epoch 1/10 | Batch 1180/2990 | Loss: 3252.0845\n",
      "Epoch 1/10 | Batch 1200/2990 | Loss: 3030.3870\n",
      "Epoch 1/10 | Batch 1220/2990 | Loss: 2807.7090\n",
      "Epoch 1/10 | Batch 1240/2990 | Loss: 1773.6676\n",
      "Epoch 1/10 | Batch 1260/2990 | Loss: 2648.6775\n",
      "Epoch 1/10 | Batch 1280/2990 | Loss: 1447.5183\n",
      "Epoch 1/10 | Batch 1300/2990 | Loss: 1298.0598\n",
      "Epoch 1/10 | Batch 1320/2990 | Loss: 1080.7439\n",
      "Epoch 1/10 | Batch 1340/2990 | Loss: 1861.4463\n",
      "Epoch 1/10 | Batch 1360/2990 | Loss: 1657.3793\n",
      "Epoch 1/10 | Batch 1380/2990 | Loss: 1121.9888\n",
      "Epoch 1/10 | Batch 1400/2990 | Loss: 2378.2976\n",
      "Epoch 1/10 | Batch 1420/2990 | Loss: 1943.1101\n",
      "Epoch 1/10 | Batch 1440/2990 | Loss: 2039.2026\n",
      "Epoch 1/10 | Batch 1460/2990 | Loss: 1369.7406\n",
      "Epoch 1/10 | Batch 1480/2990 | Loss: 2333.5859\n",
      "Epoch 1/10 | Batch 1500/2990 | Loss: 2848.8977\n",
      "Epoch 1/10 | Batch 1520/2990 | Loss: 4685.3296\n",
      "Epoch 1/10 | Batch 1540/2990 | Loss: 1616.5068\n",
      "Epoch 1/10 | Batch 1560/2990 | Loss: 3471.4717\n",
      "Epoch 1/10 | Batch 1580/2990 | Loss: 1624.1257\n",
      "Epoch 1/10 | Batch 1600/2990 | Loss: 1828.5823\n",
      "Epoch 1/10 | Batch 1620/2990 | Loss: 3602.6516\n",
      "Epoch 1/10 | Batch 1640/2990 | Loss: 2164.5908\n",
      "Epoch 1/10 | Batch 1660/2990 | Loss: 1479.1123\n",
      "Epoch 1/10 | Batch 1680/2990 | Loss: 1744.5703\n",
      "Epoch 1/10 | Batch 1700/2990 | Loss: 1094.5950\n",
      "Epoch 1/10 | Batch 1720/2990 | Loss: 489.7643\n",
      "Epoch 1/10 | Batch 1740/2990 | Loss: 1478.2832\n",
      "Epoch 1/10 | Batch 1760/2990 | Loss: 1704.7192\n",
      "Epoch 1/10 | Batch 1780/2990 | Loss: 820.9077\n",
      "Epoch 1/10 | Batch 1800/2990 | Loss: 1772.3036\n",
      "Epoch 1/10 | Batch 1820/2990 | Loss: 2840.2957\n",
      "Epoch 1/10 | Batch 1840/2990 | Loss: 1313.8455\n",
      "Epoch 1/10 | Batch 1860/2990 | Loss: 1028.4139\n",
      "Epoch 1/10 | Batch 1880/2990 | Loss: 1850.7700\n",
      "Epoch 1/10 | Batch 1900/2990 | Loss: 897.3885\n",
      "Epoch 1/10 | Batch 1920/2990 | Loss: 1306.2058\n",
      "Epoch 1/10 | Batch 1940/2990 | Loss: 869.4705\n",
      "Epoch 1/10 | Batch 1960/2990 | Loss: 679.0522\n",
      "Epoch 1/10 | Batch 1980/2990 | Loss: 1337.7737\n",
      "Epoch 1/10 | Batch 2000/2990 | Loss: 1570.1107\n",
      "Epoch 1/10 | Batch 2020/2990 | Loss: 682.0479\n",
      "Epoch 1/10 | Batch 2040/2990 | Loss: 1391.1808\n",
      "Epoch 1/10 | Batch 2060/2990 | Loss: 3884.0994\n",
      "Epoch 1/10 | Batch 2080/2990 | Loss: 2583.9609\n",
      "Epoch 1/10 | Batch 2100/2990 | Loss: 1026.6311\n",
      "Epoch 1/10 | Batch 2120/2990 | Loss: 1391.9797\n",
      "Epoch 1/10 | Batch 2140/2990 | Loss: 1586.3735\n",
      "Epoch 1/10 | Batch 2160/2990 | Loss: 898.1083\n",
      "Epoch 1/10 | Batch 2180/2990 | Loss: 1305.4628\n",
      "Epoch 1/10 | Batch 2200/2990 | Loss: 610.4846\n",
      "Epoch 1/10 | Batch 2220/2990 | Loss: 2419.0981\n",
      "Epoch 1/10 | Batch 2240/2990 | Loss: 986.4385\n",
      "Epoch 1/10 | Batch 2260/2990 | Loss: 431.6292\n",
      "Epoch 1/10 | Batch 2280/2990 | Loss: 1367.3943\n",
      "Epoch 1/10 | Batch 2300/2990 | Loss: 1049.2822\n",
      "Epoch 1/10 | Batch 2320/2990 | Loss: 495.5689\n",
      "Epoch 1/10 | Batch 2340/2990 | Loss: 712.4291\n",
      "Epoch 1/10 | Batch 2360/2990 | Loss: 1355.6747\n",
      "Epoch 1/10 | Batch 2380/2990 | Loss: 627.0894\n",
      "Epoch 1/10 | Batch 2400/2990 | Loss: 1216.0226\n",
      "Epoch 1/10 | Batch 2420/2990 | Loss: 1586.8875\n",
      "Epoch 1/10 | Batch 2440/2990 | Loss: 2516.5273\n",
      "Epoch 1/10 | Batch 2460/2990 | Loss: 1522.1079\n",
      "Epoch 1/10 | Batch 2480/2990 | Loss: 2061.6262\n",
      "Epoch 1/10 | Batch 2500/2990 | Loss: 586.8552\n",
      "Epoch 1/10 | Batch 2520/2990 | Loss: 664.7395\n",
      "Epoch 1/10 | Batch 2540/2990 | Loss: 900.2540\n",
      "Epoch 1/10 | Batch 2560/2990 | Loss: 732.2928\n",
      "Epoch 1/10 | Batch 2580/2990 | Loss: 1819.7788\n",
      "Epoch 1/10 | Batch 2600/2990 | Loss: 657.9570\n",
      "Epoch 1/10 | Batch 2620/2990 | Loss: 1772.9708\n",
      "Epoch 1/10 | Batch 2640/2990 | Loss: 1574.0713\n",
      "Epoch 1/10 | Batch 2660/2990 | Loss: 809.4716\n",
      "Epoch 1/10 | Batch 2680/2990 | Loss: 962.0499\n",
      "Epoch 1/10 | Batch 2700/2990 | Loss: 1963.3135\n",
      "Epoch 1/10 | Batch 2720/2990 | Loss: 685.9243\n",
      "Epoch 1/10 | Batch 2740/2990 | Loss: 425.9210\n",
      "Epoch 1/10 | Batch 2760/2990 | Loss: 959.5548\n",
      "Epoch 1/10 | Batch 2780/2990 | Loss: 685.4509\n",
      "Epoch 1/10 | Batch 2800/2990 | Loss: 1187.6379\n",
      "Epoch 1/10 | Batch 2820/2990 | Loss: 1314.0209\n",
      "Epoch 1/10 | Batch 2840/2990 | Loss: 994.5399\n",
      "Epoch 1/10 | Batch 2860/2990 | Loss: 2323.2007\n",
      "Epoch 1/10 | Batch 2880/2990 | Loss: 1439.6833\n",
      "Epoch 1/10 | Batch 2900/2990 | Loss: 715.5953\n",
      "Epoch 1/10 | Batch 2920/2990 | Loss: 136.2234\n",
      "Epoch 1/10 | Batch 2940/2990 | Loss: 398.1451\n",
      "Epoch 1/10 | Batch 2960/2990 | Loss: 1653.4572\n",
      "Epoch 1/10 | Batch 2980/2990 | Loss: 625.1298\n",
      "✅ Epoch 1/10 | Train Loss: 3417.7354 | Val Loss: 1768.6980\n",
      "Epoch 2/10 | Batch 0/2990 | Loss: 912.8640\n",
      "Epoch 2/10 | Batch 20/2990 | Loss: 573.7988\n",
      "Epoch 2/10 | Batch 40/2990 | Loss: 826.1168\n",
      "Epoch 2/10 | Batch 60/2990 | Loss: 1049.1826\n",
      "Epoch 2/10 | Batch 80/2990 | Loss: 925.4854\n",
      "Epoch 2/10 | Batch 100/2990 | Loss: 429.0839\n",
      "Epoch 2/10 | Batch 120/2990 | Loss: 578.0614\n",
      "Epoch 2/10 | Batch 140/2990 | Loss: 444.3187\n",
      "Epoch 2/10 | Batch 160/2990 | Loss: 247.7056\n",
      "Epoch 2/10 | Batch 180/2990 | Loss: 682.4373\n",
      "Epoch 2/10 | Batch 200/2990 | Loss: 537.0348\n",
      "Epoch 2/10 | Batch 220/2990 | Loss: 456.9577\n",
      "Epoch 2/10 | Batch 240/2990 | Loss: 609.5704\n",
      "Epoch 2/10 | Batch 260/2990 | Loss: 798.9666\n",
      "Epoch 2/10 | Batch 280/2990 | Loss: 660.4194\n",
      "Epoch 2/10 | Batch 300/2990 | Loss: 683.3032\n",
      "Epoch 2/10 | Batch 320/2990 | Loss: 459.0761\n",
      "Epoch 2/10 | Batch 340/2990 | Loss: 461.0209\n",
      "Epoch 2/10 | Batch 360/2990 | Loss: 658.6313\n",
      "Epoch 2/10 | Batch 380/2990 | Loss: 1220.4425\n",
      "Epoch 2/10 | Batch 400/2990 | Loss: 906.8928\n",
      "Epoch 2/10 | Batch 420/2990 | Loss: 506.5027\n",
      "Epoch 2/10 | Batch 440/2990 | Loss: 671.5217\n",
      "Epoch 2/10 | Batch 460/2990 | Loss: 1214.2438\n",
      "Epoch 2/10 | Batch 480/2990 | Loss: 1314.0259\n",
      "Epoch 2/10 | Batch 500/2990 | Loss: 599.6360\n",
      "Epoch 2/10 | Batch 520/2990 | Loss: 338.4759\n",
      "Epoch 2/10 | Batch 540/2990 | Loss: 468.0887\n",
      "Epoch 2/10 | Batch 560/2990 | Loss: 1011.1005\n",
      "Epoch 2/10 | Batch 580/2990 | Loss: 2525.2769\n",
      "Epoch 2/10 | Batch 600/2990 | Loss: 2105.0378\n",
      "Epoch 2/10 | Batch 620/2990 | Loss: 519.1149\n",
      "Epoch 2/10 | Batch 640/2990 | Loss: 711.3196\n",
      "Epoch 2/10 | Batch 660/2990 | Loss: 1862.1930\n",
      "Epoch 2/10 | Batch 680/2990 | Loss: 771.0212\n",
      "Epoch 2/10 | Batch 700/2990 | Loss: 718.2230\n",
      "Epoch 2/10 | Batch 720/2990 | Loss: 458.1772\n",
      "Epoch 2/10 | Batch 740/2990 | Loss: 1298.2092\n",
      "Epoch 2/10 | Batch 760/2990 | Loss: 835.5280\n",
      "Epoch 2/10 | Batch 780/2990 | Loss: 450.2396\n",
      "Epoch 2/10 | Batch 800/2990 | Loss: 307.6815\n",
      "Epoch 2/10 | Batch 820/2990 | Loss: 501.0805\n",
      "Epoch 2/10 | Batch 840/2990 | Loss: 216.4499\n",
      "Epoch 2/10 | Batch 860/2990 | Loss: 1019.1061\n",
      "Epoch 2/10 | Batch 880/2990 | Loss: 726.5274\n",
      "Epoch 2/10 | Batch 900/2990 | Loss: 589.3639\n",
      "Epoch 2/10 | Batch 920/2990 | Loss: 814.9644\n",
      "Epoch 2/10 | Batch 940/2990 | Loss: 360.5487\n",
      "Epoch 2/10 | Batch 960/2990 | Loss: 825.2892\n",
      "Epoch 2/10 | Batch 980/2990 | Loss: 320.0099\n",
      "Epoch 2/10 | Batch 1000/2990 | Loss: 169.4364\n",
      "Epoch 2/10 | Batch 1020/2990 | Loss: 1605.6497\n",
      "Epoch 2/10 | Batch 1040/2990 | Loss: 761.6089\n",
      "Epoch 2/10 | Batch 1060/2990 | Loss: 378.1080\n",
      "Epoch 2/10 | Batch 1080/2990 | Loss: 397.2542\n",
      "Epoch 2/10 | Batch 1100/2990 | Loss: 428.6918\n",
      "Epoch 2/10 | Batch 1120/2990 | Loss: 1471.1877\n",
      "Epoch 2/10 | Batch 1140/2990 | Loss: 1488.0114\n",
      "Epoch 2/10 | Batch 1160/2990 | Loss: 932.7665\n",
      "Epoch 2/10 | Batch 1180/2990 | Loss: 767.3800\n",
      "Epoch 2/10 | Batch 1200/2990 | Loss: 401.6691\n",
      "Epoch 2/10 | Batch 1220/2990 | Loss: 353.5335\n",
      "Epoch 2/10 | Batch 1240/2990 | Loss: 346.6328\n",
      "Epoch 2/10 | Batch 1260/2990 | Loss: 796.7093\n",
      "Epoch 2/10 | Batch 1280/2990 | Loss: 2224.4302\n",
      "Epoch 2/10 | Batch 1300/2990 | Loss: 1229.2559\n",
      "Epoch 2/10 | Batch 1320/2990 | Loss: 875.0495\n",
      "Epoch 2/10 | Batch 1340/2990 | Loss: 528.2650\n",
      "Epoch 2/10 | Batch 1360/2990 | Loss: 439.2720\n",
      "Epoch 2/10 | Batch 1380/2990 | Loss: 756.8856\n",
      "Epoch 2/10 | Batch 1400/2990 | Loss: 661.1267\n",
      "Epoch 2/10 | Batch 1420/2990 | Loss: 397.5717\n",
      "Epoch 2/10 | Batch 1440/2990 | Loss: 658.2707\n",
      "Epoch 2/10 | Batch 1460/2990 | Loss: 915.7649\n",
      "Epoch 2/10 | Batch 1480/2990 | Loss: 473.0024\n",
      "Epoch 2/10 | Batch 1500/2990 | Loss: 544.1119\n",
      "Epoch 2/10 | Batch 1520/2990 | Loss: 1399.4678\n",
      "Epoch 2/10 | Batch 1540/2990 | Loss: 557.5400\n",
      "Epoch 2/10 | Batch 1560/2990 | Loss: 603.4731\n",
      "Epoch 2/10 | Batch 1580/2990 | Loss: 628.4315\n",
      "Epoch 2/10 | Batch 1600/2990 | Loss: 538.8029\n",
      "Epoch 2/10 | Batch 1620/2990 | Loss: 885.5503\n",
      "Epoch 2/10 | Batch 1640/2990 | Loss: 857.9751\n",
      "Epoch 2/10 | Batch 1660/2990 | Loss: 129.8356\n",
      "Epoch 2/10 | Batch 1680/2990 | Loss: 567.9391\n",
      "Epoch 2/10 | Batch 1700/2990 | Loss: 474.1134\n",
      "Epoch 2/10 | Batch 1720/2990 | Loss: 1112.2166\n",
      "Epoch 2/10 | Batch 1740/2990 | Loss: 405.8464\n",
      "Epoch 2/10 | Batch 1760/2990 | Loss: 620.4961\n",
      "Epoch 2/10 | Batch 1780/2990 | Loss: 722.2052\n",
      "Epoch 2/10 | Batch 1800/2990 | Loss: 411.5949\n",
      "Epoch 2/10 | Batch 1820/2990 | Loss: 474.4971\n",
      "Epoch 2/10 | Batch 1840/2990 | Loss: 709.1499\n",
      "Epoch 2/10 | Batch 1860/2990 | Loss: 1252.4952\n",
      "Epoch 2/10 | Batch 1880/2990 | Loss: 1280.2185\n",
      "Epoch 2/10 | Batch 1900/2990 | Loss: 1239.7471\n",
      "Epoch 2/10 | Batch 1920/2990 | Loss: 274.1884\n",
      "Epoch 2/10 | Batch 1940/2990 | Loss: 584.7429\n",
      "Epoch 2/10 | Batch 1960/2990 | Loss: 411.4875\n",
      "Epoch 2/10 | Batch 1980/2990 | Loss: 371.6175\n",
      "Epoch 2/10 | Batch 2000/2990 | Loss: 699.9271\n",
      "Epoch 2/10 | Batch 2020/2990 | Loss: 510.1700\n",
      "Epoch 2/10 | Batch 2040/2990 | Loss: 761.2943\n",
      "Epoch 2/10 | Batch 2060/2990 | Loss: 668.6172\n",
      "Epoch 2/10 | Batch 2080/2990 | Loss: 139.9571\n",
      "Epoch 2/10 | Batch 2100/2990 | Loss: 381.7334\n",
      "Epoch 2/10 | Batch 2120/2990 | Loss: 600.0019\n",
      "Epoch 2/10 | Batch 2140/2990 | Loss: 329.3187\n",
      "Epoch 2/10 | Batch 2160/2990 | Loss: 559.3484\n",
      "Epoch 2/10 | Batch 2180/2990 | Loss: 1298.1334\n",
      "Epoch 2/10 | Batch 2200/2990 | Loss: 279.2713\n",
      "Epoch 2/10 | Batch 2220/2990 | Loss: 280.6469\n",
      "Epoch 2/10 | Batch 2240/2990 | Loss: 253.9764\n",
      "Epoch 2/10 | Batch 2260/2990 | Loss: 287.3548\n",
      "Epoch 2/10 | Batch 2280/2990 | Loss: 551.0227\n",
      "Epoch 2/10 | Batch 2300/2990 | Loss: 419.3599\n",
      "Epoch 2/10 | Batch 2320/2990 | Loss: 410.1381\n",
      "Epoch 2/10 | Batch 2340/2990 | Loss: 218.1823\n",
      "Epoch 2/10 | Batch 2360/2990 | Loss: 1044.7344\n",
      "Epoch 2/10 | Batch 2380/2990 | Loss: 520.0121\n",
      "Epoch 2/10 | Batch 2400/2990 | Loss: 1102.0632\n",
      "Epoch 2/10 | Batch 2420/2990 | Loss: 178.3990\n",
      "Epoch 2/10 | Batch 2440/2990 | Loss: 212.4992\n",
      "Epoch 2/10 | Batch 2460/2990 | Loss: 534.3820\n",
      "Epoch 2/10 | Batch 2480/2990 | Loss: 866.4285\n",
      "Epoch 2/10 | Batch 2500/2990 | Loss: 580.5618\n",
      "Epoch 2/10 | Batch 2520/2990 | Loss: 490.3095\n",
      "Epoch 2/10 | Batch 2540/2990 | Loss: 489.6240\n",
      "Epoch 2/10 | Batch 2560/2990 | Loss: 342.4746\n",
      "Epoch 2/10 | Batch 2580/2990 | Loss: 443.8880\n",
      "Epoch 2/10 | Batch 2600/2990 | Loss: 412.8798\n",
      "Epoch 2/10 | Batch 2620/2990 | Loss: 528.4256\n",
      "Epoch 2/10 | Batch 2640/2990 | Loss: 364.4318\n",
      "Epoch 2/10 | Batch 2660/2990 | Loss: 956.3380\n",
      "Epoch 2/10 | Batch 2680/2990 | Loss: 496.4639\n",
      "Epoch 2/10 | Batch 2700/2990 | Loss: 471.4488\n",
      "Epoch 2/10 | Batch 2720/2990 | Loss: 352.0189\n",
      "Epoch 2/10 | Batch 2740/2990 | Loss: 373.3669\n",
      "Epoch 2/10 | Batch 2760/2990 | Loss: 441.0583\n",
      "Epoch 2/10 | Batch 2780/2990 | Loss: 393.8239\n",
      "Epoch 2/10 | Batch 2800/2990 | Loss: 225.8285\n",
      "Epoch 2/10 | Batch 2820/2990 | Loss: 550.7273\n",
      "Epoch 2/10 | Batch 2840/2990 | Loss: 226.9039\n",
      "Epoch 2/10 | Batch 2860/2990 | Loss: 511.3011\n",
      "Epoch 2/10 | Batch 2880/2990 | Loss: 306.0934\n",
      "Epoch 2/10 | Batch 2900/2990 | Loss: 655.6749\n",
      "Epoch 2/10 | Batch 2920/2990 | Loss: 211.8755\n",
      "Epoch 2/10 | Batch 2940/2990 | Loss: 795.0305\n",
      "Epoch 2/10 | Batch 2960/2990 | Loss: 463.4524\n",
      "Epoch 2/10 | Batch 2980/2990 | Loss: 384.9143\n",
      "✅ Epoch 2/10 | Train Loss: 631.8879 | Val Loss: 406.0784\n",
      "Epoch 3/10 | Batch 0/2990 | Loss: 311.8130\n",
      "Epoch 3/10 | Batch 20/2990 | Loss: 315.6473\n",
      "Epoch 3/10 | Batch 40/2990 | Loss: 603.1205\n",
      "Epoch 3/10 | Batch 60/2990 | Loss: 648.7621\n",
      "Epoch 3/10 | Batch 80/2990 | Loss: 218.3233\n",
      "Epoch 3/10 | Batch 100/2990 | Loss: 290.4733\n",
      "Epoch 3/10 | Batch 120/2990 | Loss: 579.4804\n",
      "Epoch 3/10 | Batch 140/2990 | Loss: 566.5391\n",
      "Epoch 3/10 | Batch 160/2990 | Loss: 526.3080\n",
      "Epoch 3/10 | Batch 180/2990 | Loss: 255.0441\n",
      "Epoch 3/10 | Batch 200/2990 | Loss: 639.2935\n",
      "Epoch 3/10 | Batch 220/2990 | Loss: 175.3761\n",
      "Epoch 3/10 | Batch 240/2990 | Loss: 441.5457\n",
      "Epoch 3/10 | Batch 260/2990 | Loss: 319.2109\n",
      "Epoch 3/10 | Batch 280/2990 | Loss: 488.2248\n",
      "Epoch 3/10 | Batch 300/2990 | Loss: 668.9802\n",
      "Epoch 3/10 | Batch 320/2990 | Loss: 707.9821\n",
      "Epoch 3/10 | Batch 340/2990 | Loss: 286.6337\n",
      "Epoch 3/10 | Batch 360/2990 | Loss: 313.2555\n",
      "Epoch 3/10 | Batch 380/2990 | Loss: 157.3182\n",
      "Epoch 3/10 | Batch 400/2990 | Loss: 368.2117\n",
      "Epoch 3/10 | Batch 420/2990 | Loss: 544.5791\n",
      "Epoch 3/10 | Batch 440/2990 | Loss: 600.8329\n",
      "Epoch 3/10 | Batch 460/2990 | Loss: 392.1641\n",
      "Epoch 3/10 | Batch 480/2990 | Loss: 1083.3143\n",
      "Epoch 3/10 | Batch 500/2990 | Loss: 258.3956\n",
      "Epoch 3/10 | Batch 520/2990 | Loss: 913.0187\n",
      "Epoch 3/10 | Batch 540/2990 | Loss: 280.1852\n",
      "Epoch 3/10 | Batch 560/2990 | Loss: 203.0806\n",
      "Epoch 3/10 | Batch 580/2990 | Loss: 291.0743\n",
      "Epoch 3/10 | Batch 600/2990 | Loss: 1204.3184\n",
      "Epoch 3/10 | Batch 620/2990 | Loss: 522.3682\n",
      "Epoch 3/10 | Batch 640/2990 | Loss: 223.7315\n",
      "Epoch 3/10 | Batch 660/2990 | Loss: 588.0381\n",
      "Epoch 3/10 | Batch 680/2990 | Loss: 358.5521\n",
      "Epoch 3/10 | Batch 700/2990 | Loss: 358.0751\n",
      "Epoch 3/10 | Batch 720/2990 | Loss: 299.1479\n",
      "Epoch 3/10 | Batch 740/2990 | Loss: 370.0396\n",
      "Epoch 3/10 | Batch 760/2990 | Loss: 203.8792\n",
      "Epoch 3/10 | Batch 780/2990 | Loss: 559.4355\n",
      "Epoch 3/10 | Batch 800/2990 | Loss: 278.0455\n",
      "Epoch 3/10 | Batch 820/2990 | Loss: 334.1550\n",
      "Epoch 3/10 | Batch 840/2990 | Loss: 256.0803\n",
      "Epoch 3/10 | Batch 860/2990 | Loss: 379.0917\n",
      "Epoch 3/10 | Batch 880/2990 | Loss: 259.6172\n",
      "Epoch 3/10 | Batch 900/2990 | Loss: 586.9774\n",
      "Epoch 3/10 | Batch 920/2990 | Loss: 505.5408\n",
      "Epoch 3/10 | Batch 940/2990 | Loss: 324.2012\n",
      "Epoch 3/10 | Batch 960/2990 | Loss: 515.2925\n",
      "Epoch 3/10 | Batch 980/2990 | Loss: 408.3320\n",
      "Epoch 3/10 | Batch 1000/2990 | Loss: 326.3999\n",
      "Epoch 3/10 | Batch 1020/2990 | Loss: 161.5790\n",
      "Epoch 3/10 | Batch 1040/2990 | Loss: 390.8693\n",
      "Epoch 3/10 | Batch 1060/2990 | Loss: 249.5838\n",
      "Epoch 3/10 | Batch 1080/2990 | Loss: 334.7206\n",
      "Epoch 3/10 | Batch 1100/2990 | Loss: 158.9783\n",
      "Epoch 3/10 | Batch 1120/2990 | Loss: 517.4290\n",
      "Epoch 3/10 | Batch 1140/2990 | Loss: 63.6801\n",
      "Epoch 3/10 | Batch 1160/2990 | Loss: 388.7159\n",
      "Epoch 3/10 | Batch 1180/2990 | Loss: 202.2912\n",
      "Epoch 3/10 | Batch 1200/2990 | Loss: 214.1918\n",
      "Epoch 3/10 | Batch 1220/2990 | Loss: 121.7023\n",
      "Epoch 3/10 | Batch 1240/2990 | Loss: 302.8205\n",
      "Epoch 3/10 | Batch 1260/2990 | Loss: 259.0032\n",
      "Epoch 3/10 | Batch 1280/2990 | Loss: 397.8522\n",
      "Epoch 3/10 | Batch 1300/2990 | Loss: 614.2381\n",
      "Epoch 3/10 | Batch 1320/2990 | Loss: 223.7485\n",
      "Epoch 3/10 | Batch 1340/2990 | Loss: 367.8203\n",
      "Epoch 3/10 | Batch 1360/2990 | Loss: 316.3094\n",
      "Epoch 3/10 | Batch 1380/2990 | Loss: 153.9453\n",
      "Epoch 3/10 | Batch 1400/2990 | Loss: 658.6287\n",
      "Epoch 3/10 | Batch 1420/2990 | Loss: 375.3185\n",
      "Epoch 3/10 | Batch 1440/2990 | Loss: 175.4402\n",
      "Epoch 3/10 | Batch 1460/2990 | Loss: 436.0715\n",
      "Epoch 3/10 | Batch 1480/2990 | Loss: 131.5472\n",
      "Epoch 3/10 | Batch 1500/2990 | Loss: 201.8221\n",
      "Epoch 3/10 | Batch 1520/2990 | Loss: 210.3286\n",
      "Epoch 3/10 | Batch 1540/2990 | Loss: 227.2923\n",
      "Epoch 3/10 | Batch 1560/2990 | Loss: 296.5302\n",
      "Epoch 3/10 | Batch 1580/2990 | Loss: 415.7777\n",
      "Epoch 3/10 | Batch 1600/2990 | Loss: 98.1872\n",
      "Epoch 3/10 | Batch 1620/2990 | Loss: 198.7008\n",
      "Epoch 3/10 | Batch 1640/2990 | Loss: 728.4816\n",
      "Epoch 3/10 | Batch 1660/2990 | Loss: 229.1452\n",
      "Epoch 3/10 | Batch 1680/2990 | Loss: 286.2747\n",
      "Epoch 3/10 | Batch 1700/2990 | Loss: 187.2850\n",
      "Epoch 3/10 | Batch 1720/2990 | Loss: 1213.3959\n",
      "Epoch 3/10 | Batch 1740/2990 | Loss: 379.6634\n",
      "Epoch 3/10 | Batch 1760/2990 | Loss: 297.9879\n",
      "Epoch 3/10 | Batch 1780/2990 | Loss: 500.9591\n",
      "Epoch 3/10 | Batch 1800/2990 | Loss: 280.0418\n",
      "Epoch 3/10 | Batch 1820/2990 | Loss: 130.0407\n",
      "Epoch 3/10 | Batch 1840/2990 | Loss: 312.0258\n",
      "Epoch 3/10 | Batch 1860/2990 | Loss: 330.7014\n",
      "Epoch 3/10 | Batch 1880/2990 | Loss: 402.0851\n",
      "Epoch 3/10 | Batch 1900/2990 | Loss: 442.2432\n",
      "Epoch 3/10 | Batch 1920/2990 | Loss: 254.7649\n",
      "Epoch 3/10 | Batch 1940/2990 | Loss: 159.5272\n",
      "Epoch 3/10 | Batch 1960/2990 | Loss: 332.5883\n",
      "Epoch 3/10 | Batch 1980/2990 | Loss: 196.0275\n",
      "Epoch 3/10 | Batch 2000/2990 | Loss: 215.5566\n",
      "Epoch 3/10 | Batch 2020/2990 | Loss: 152.6447\n",
      "Epoch 3/10 | Batch 2040/2990 | Loss: 253.4348\n",
      "Epoch 3/10 | Batch 2060/2990 | Loss: 358.0293\n",
      "Epoch 3/10 | Batch 2080/2990 | Loss: 367.3793\n",
      "Epoch 3/10 | Batch 2100/2990 | Loss: 194.9632\n",
      "Epoch 3/10 | Batch 2120/2990 | Loss: 133.1696\n",
      "Epoch 3/10 | Batch 2140/2990 | Loss: 295.7345\n",
      "Epoch 3/10 | Batch 2160/2990 | Loss: 297.1582\n",
      "Epoch 3/10 | Batch 2180/2990 | Loss: 314.5301\n",
      "Epoch 3/10 | Batch 2200/2990 | Loss: 160.5016\n",
      "Epoch 3/10 | Batch 2220/2990 | Loss: 312.6675\n",
      "Epoch 3/10 | Batch 2240/2990 | Loss: 216.3956\n",
      "Epoch 3/10 | Batch 2260/2990 | Loss: 399.7982\n",
      "Epoch 3/10 | Batch 2280/2990 | Loss: 172.9835\n",
      "Epoch 3/10 | Batch 2300/2990 | Loss: 239.6947\n",
      "Epoch 3/10 | Batch 2320/2990 | Loss: 313.3399\n",
      "Epoch 3/10 | Batch 2340/2990 | Loss: 584.2928\n",
      "Epoch 3/10 | Batch 2360/2990 | Loss: 317.3365\n",
      "Epoch 3/10 | Batch 2380/2990 | Loss: 285.1676\n",
      "Epoch 3/10 | Batch 2400/2990 | Loss: 241.8695\n",
      "Epoch 3/10 | Batch 2420/2990 | Loss: 302.2013\n",
      "Epoch 3/10 | Batch 2440/2990 | Loss: 175.5299\n",
      "Epoch 3/10 | Batch 2460/2990 | Loss: 156.8994\n",
      "Epoch 3/10 | Batch 2480/2990 | Loss: 194.9730\n",
      "Epoch 3/10 | Batch 2500/2990 | Loss: 241.0170\n",
      "Epoch 3/10 | Batch 2520/2990 | Loss: 366.0889\n",
      "Epoch 3/10 | Batch 2540/2990 | Loss: 252.1015\n",
      "Epoch 3/10 | Batch 2560/2990 | Loss: 254.9214\n",
      "Epoch 3/10 | Batch 2580/2990 | Loss: 471.5066\n",
      "Epoch 3/10 | Batch 2600/2990 | Loss: 442.8459\n",
      "Epoch 3/10 | Batch 2620/2990 | Loss: 321.6868\n",
      "Epoch 3/10 | Batch 2640/2990 | Loss: 423.2005\n",
      "Epoch 3/10 | Batch 2660/2990 | Loss: 809.2381\n",
      "Epoch 3/10 | Batch 2680/2990 | Loss: 154.9794\n",
      "Epoch 3/10 | Batch 2700/2990 | Loss: 305.0942\n",
      "Epoch 3/10 | Batch 2720/2990 | Loss: 400.7965\n",
      "Epoch 3/10 | Batch 2740/2990 | Loss: 241.3036\n",
      "Epoch 3/10 | Batch 2760/2990 | Loss: 285.3068\n",
      "Epoch 3/10 | Batch 2780/2990 | Loss: 228.2514\n",
      "Epoch 3/10 | Batch 2800/2990 | Loss: 498.5485\n",
      "Epoch 3/10 | Batch 2820/2990 | Loss: 185.6579\n",
      "Epoch 3/10 | Batch 2840/2990 | Loss: 260.3147\n",
      "Epoch 3/10 | Batch 2860/2990 | Loss: 436.1873\n",
      "Epoch 3/10 | Batch 2880/2990 | Loss: 288.1279\n",
      "Epoch 3/10 | Batch 2900/2990 | Loss: 274.9344\n",
      "Epoch 3/10 | Batch 2920/2990 | Loss: 569.5128\n",
      "Epoch 3/10 | Batch 2940/2990 | Loss: 720.0269\n",
      "Epoch 3/10 | Batch 2960/2990 | Loss: 201.8918\n",
      "Epoch 3/10 | Batch 2980/2990 | Loss: 378.3628\n",
      "✅ Epoch 3/10 | Train Loss: 378.2339 | Val Loss: 239.2534\n",
      "Epoch 4/10 | Batch 0/2990 | Loss: 277.6159\n",
      "Epoch 4/10 | Batch 20/2990 | Loss: 116.3435\n",
      "Epoch 4/10 | Batch 40/2990 | Loss: 409.2379\n",
      "Epoch 4/10 | Batch 60/2990 | Loss: 137.5795\n",
      "Epoch 4/10 | Batch 80/2990 | Loss: 114.0016\n",
      "Epoch 4/10 | Batch 100/2990 | Loss: 456.3049\n",
      "Epoch 4/10 | Batch 120/2990 | Loss: 185.5497\n",
      "Epoch 4/10 | Batch 140/2990 | Loss: 269.3374\n",
      "Epoch 4/10 | Batch 160/2990 | Loss: 382.4558\n",
      "Epoch 4/10 | Batch 180/2990 | Loss: 205.7201\n",
      "Epoch 4/10 | Batch 200/2990 | Loss: 587.4990\n",
      "Epoch 4/10 | Batch 220/2990 | Loss: 195.3114\n",
      "Epoch 4/10 | Batch 240/2990 | Loss: 185.3920\n",
      "Epoch 4/10 | Batch 260/2990 | Loss: 404.6196\n",
      "Epoch 4/10 | Batch 280/2990 | Loss: 366.4274\n",
      "Epoch 4/10 | Batch 300/2990 | Loss: 170.1758\n",
      "Epoch 4/10 | Batch 320/2990 | Loss: 173.2686\n",
      "Epoch 4/10 | Batch 340/2990 | Loss: 104.2252\n",
      "Epoch 4/10 | Batch 360/2990 | Loss: 327.6632\n",
      "Epoch 4/10 | Batch 380/2990 | Loss: 224.9033\n",
      "Epoch 4/10 | Batch 400/2990 | Loss: 291.2085\n",
      "Epoch 4/10 | Batch 420/2990 | Loss: 439.0309\n",
      "Epoch 4/10 | Batch 440/2990 | Loss: 277.9378\n",
      "Epoch 4/10 | Batch 460/2990 | Loss: 203.7388\n",
      "Epoch 4/10 | Batch 480/2990 | Loss: 188.2496\n",
      "Epoch 4/10 | Batch 500/2990 | Loss: 190.7191\n",
      "Epoch 4/10 | Batch 520/2990 | Loss: 122.0155\n",
      "Epoch 4/10 | Batch 540/2990 | Loss: 210.3033\n",
      "Epoch 4/10 | Batch 560/2990 | Loss: 387.3045\n",
      "Epoch 4/10 | Batch 580/2990 | Loss: 149.2478\n",
      "Epoch 4/10 | Batch 600/2990 | Loss: 135.4877\n",
      "Epoch 4/10 | Batch 620/2990 | Loss: 199.2766\n",
      "Epoch 4/10 | Batch 640/2990 | Loss: 397.5327\n",
      "Epoch 4/10 | Batch 660/2990 | Loss: 166.6959\n",
      "Epoch 4/10 | Batch 680/2990 | Loss: 324.1992\n",
      "Epoch 4/10 | Batch 700/2990 | Loss: 420.0002\n",
      "Epoch 4/10 | Batch 720/2990 | Loss: 274.5799\n",
      "Epoch 4/10 | Batch 740/2990 | Loss: 666.6903\n",
      "Epoch 4/10 | Batch 760/2990 | Loss: 67.9691\n",
      "Epoch 4/10 | Batch 780/2990 | Loss: 254.8473\n",
      "Epoch 4/10 | Batch 800/2990 | Loss: 252.4780\n",
      "Epoch 4/10 | Batch 820/2990 | Loss: 326.3519\n",
      "Epoch 4/10 | Batch 840/2990 | Loss: 297.2417\n",
      "Epoch 4/10 | Batch 860/2990 | Loss: 251.9641\n",
      "Epoch 4/10 | Batch 880/2990 | Loss: 342.0530\n",
      "Epoch 4/10 | Batch 900/2990 | Loss: 697.7430\n",
      "Epoch 4/10 | Batch 920/2990 | Loss: 195.7177\n",
      "Epoch 4/10 | Batch 940/2990 | Loss: 441.7412\n",
      "Epoch 4/10 | Batch 960/2990 | Loss: 202.8297\n",
      "Epoch 4/10 | Batch 980/2990 | Loss: 207.0703\n",
      "Epoch 4/10 | Batch 1000/2990 | Loss: 128.7222\n",
      "Epoch 4/10 | Batch 1020/2990 | Loss: 559.0291\n",
      "Epoch 4/10 | Batch 1040/2990 | Loss: 274.4332\n",
      "Epoch 4/10 | Batch 1060/2990 | Loss: 177.4353\n",
      "Epoch 4/10 | Batch 1080/2990 | Loss: 382.5384\n",
      "Epoch 4/10 | Batch 1100/2990 | Loss: 261.8134\n",
      "Epoch 4/10 | Batch 1120/2990 | Loss: 85.1831\n",
      "Epoch 4/10 | Batch 1140/2990 | Loss: 212.5766\n",
      "Epoch 4/10 | Batch 1160/2990 | Loss: 165.1254\n",
      "Epoch 4/10 | Batch 1180/2990 | Loss: 519.4900\n",
      "Epoch 4/10 | Batch 1200/2990 | Loss: 246.0788\n",
      "Epoch 4/10 | Batch 1220/2990 | Loss: 297.4910\n",
      "Epoch 4/10 | Batch 1240/2990 | Loss: 496.0267\n",
      "Epoch 4/10 | Batch 1260/2990 | Loss: 182.8142\n",
      "Epoch 4/10 | Batch 1280/2990 | Loss: 208.7182\n",
      "Epoch 4/10 | Batch 1300/2990 | Loss: 425.4980\n",
      "Epoch 4/10 | Batch 1320/2990 | Loss: 236.3051\n",
      "Epoch 4/10 | Batch 1340/2990 | Loss: 90.4862\n",
      "Epoch 4/10 | Batch 1360/2990 | Loss: 305.7845\n",
      "Epoch 4/10 | Batch 1380/2990 | Loss: 251.0293\n",
      "Epoch 4/10 | Batch 1400/2990 | Loss: 134.7712\n",
      "Epoch 4/10 | Batch 1420/2990 | Loss: 131.4611\n",
      "Epoch 4/10 | Batch 1440/2990 | Loss: 84.2512\n",
      "Epoch 4/10 | Batch 1460/2990 | Loss: 165.0493\n",
      "Epoch 4/10 | Batch 1480/2990 | Loss: 109.7461\n",
      "Epoch 4/10 | Batch 1500/2990 | Loss: 705.3070\n",
      "Epoch 4/10 | Batch 1520/2990 | Loss: 302.9359\n",
      "Epoch 4/10 | Batch 1540/2990 | Loss: 261.8908\n",
      "Epoch 4/10 | Batch 1560/2990 | Loss: 266.0865\n",
      "Epoch 4/10 | Batch 1580/2990 | Loss: 232.5354\n",
      "Epoch 4/10 | Batch 1600/2990 | Loss: 160.8656\n",
      "Epoch 4/10 | Batch 1620/2990 | Loss: 369.5638\n",
      "Epoch 4/10 | Batch 1640/2990 | Loss: 326.0979\n",
      "Epoch 4/10 | Batch 1660/2990 | Loss: 149.0341\n",
      "Epoch 4/10 | Batch 1680/2990 | Loss: 122.6899\n",
      "Epoch 4/10 | Batch 1700/2990 | Loss: 158.1362\n",
      "Epoch 4/10 | Batch 1720/2990 | Loss: 318.9771\n",
      "Epoch 4/10 | Batch 1740/2990 | Loss: 128.3124\n",
      "Epoch 4/10 | Batch 1760/2990 | Loss: 167.8842\n",
      "Epoch 4/10 | Batch 1780/2990 | Loss: 209.1248\n",
      "Epoch 4/10 | Batch 1800/2990 | Loss: 232.8806\n",
      "Epoch 4/10 | Batch 1820/2990 | Loss: 407.3026\n",
      "Epoch 4/10 | Batch 1840/2990 | Loss: 149.1926\n",
      "Epoch 4/10 | Batch 1860/2990 | Loss: 80.8974\n",
      "Epoch 4/10 | Batch 1880/2990 | Loss: 258.5121\n",
      "Epoch 4/10 | Batch 1900/2990 | Loss: 232.4166\n",
      "Epoch 4/10 | Batch 1920/2990 | Loss: 135.7109\n",
      "Epoch 4/10 | Batch 1940/2990 | Loss: 258.5165\n",
      "Epoch 4/10 | Batch 1960/2990 | Loss: 336.7867\n",
      "Epoch 4/10 | Batch 1980/2990 | Loss: 383.1347\n",
      "Epoch 4/10 | Batch 2000/2990 | Loss: 185.7570\n",
      "Epoch 4/10 | Batch 2020/2990 | Loss: 131.2161\n",
      "Epoch 4/10 | Batch 2040/2990 | Loss: 460.6420\n",
      "Epoch 4/10 | Batch 2060/2990 | Loss: 212.8977\n",
      "Epoch 4/10 | Batch 2080/2990 | Loss: 301.2076\n",
      "Epoch 4/10 | Batch 2100/2990 | Loss: 287.3059\n",
      "Epoch 4/10 | Batch 2120/2990 | Loss: 477.3796\n",
      "Epoch 4/10 | Batch 2140/2990 | Loss: 379.2582\n",
      "Epoch 4/10 | Batch 2160/2990 | Loss: 463.1027\n",
      "Epoch 4/10 | Batch 2180/2990 | Loss: 548.3345\n",
      "Epoch 4/10 | Batch 2200/2990 | Loss: 505.2450\n",
      "Epoch 4/10 | Batch 2220/2990 | Loss: 253.0316\n",
      "Epoch 4/10 | Batch 2240/2990 | Loss: 606.8618\n",
      "Epoch 4/10 | Batch 2260/2990 | Loss: 240.4724\n",
      "Epoch 4/10 | Batch 2280/2990 | Loss: 217.5622\n",
      "Epoch 4/10 | Batch 2300/2990 | Loss: 539.3930\n",
      "Epoch 4/10 | Batch 2320/2990 | Loss: 404.8425\n",
      "Epoch 4/10 | Batch 2340/2990 | Loss: 395.0636\n",
      "Epoch 4/10 | Batch 2360/2990 | Loss: 129.9876\n",
      "Epoch 4/10 | Batch 2380/2990 | Loss: 254.6494\n",
      "Epoch 4/10 | Batch 2400/2990 | Loss: 216.4103\n",
      "Epoch 4/10 | Batch 2420/2990 | Loss: 64.9118\n",
      "Epoch 4/10 | Batch 2440/2990 | Loss: 169.0587\n",
      "Epoch 4/10 | Batch 2460/2990 | Loss: 665.6548\n",
      "Epoch 4/10 | Batch 2480/2990 | Loss: 170.6620\n",
      "Epoch 4/10 | Batch 2500/2990 | Loss: 177.8657\n",
      "Epoch 4/10 | Batch 2520/2990 | Loss: 227.8572\n",
      "Epoch 4/10 | Batch 2540/2990 | Loss: 476.5726\n",
      "Epoch 4/10 | Batch 2560/2990 | Loss: 318.9129\n",
      "Epoch 4/10 | Batch 2580/2990 | Loss: 325.5630\n",
      "Epoch 4/10 | Batch 2600/2990 | Loss: 205.7248\n",
      "Epoch 4/10 | Batch 2620/2990 | Loss: 112.1280\n",
      "Epoch 4/10 | Batch 2640/2990 | Loss: 229.0215\n",
      "Epoch 4/10 | Batch 2660/2990 | Loss: 370.6069\n",
      "Epoch 4/10 | Batch 2680/2990 | Loss: 211.7789\n",
      "Epoch 4/10 | Batch 2700/2990 | Loss: 196.8427\n",
      "Epoch 4/10 | Batch 2720/2990 | Loss: 113.0845\n",
      "Epoch 4/10 | Batch 2740/2990 | Loss: 91.9380\n",
      "Epoch 4/10 | Batch 2760/2990 | Loss: 142.6297\n",
      "Epoch 4/10 | Batch 2780/2990 | Loss: 126.1800\n",
      "Epoch 4/10 | Batch 2800/2990 | Loss: 124.0445\n",
      "Epoch 4/10 | Batch 2820/2990 | Loss: 149.8058\n",
      "Epoch 4/10 | Batch 2840/2990 | Loss: 278.2678\n",
      "Epoch 4/10 | Batch 2860/2990 | Loss: 337.3958\n",
      "Epoch 4/10 | Batch 2880/2990 | Loss: 204.8717\n",
      "Epoch 4/10 | Batch 2900/2990 | Loss: 293.7732\n",
      "Epoch 4/10 | Batch 2920/2990 | Loss: 208.8607\n",
      "Epoch 4/10 | Batch 2940/2990 | Loss: 142.8848\n",
      "Epoch 4/10 | Batch 2960/2990 | Loss: 408.0602\n",
      "Epoch 4/10 | Batch 2980/2990 | Loss: 182.2574\n",
      "✅ Epoch 4/10 | Train Loss: 270.3732 | Val Loss: 182.3758\n",
      "Epoch 5/10 | Batch 0/2990 | Loss: 197.1972\n",
      "Epoch 5/10 | Batch 20/2990 | Loss: 100.0200\n",
      "Epoch 5/10 | Batch 40/2990 | Loss: 62.1488\n",
      "Epoch 5/10 | Batch 60/2990 | Loss: 190.5637\n",
      "Epoch 5/10 | Batch 80/2990 | Loss: 92.1041\n",
      "Epoch 5/10 | Batch 100/2990 | Loss: 100.3318\n",
      "Epoch 5/10 | Batch 120/2990 | Loss: 109.1667\n",
      "Epoch 5/10 | Batch 140/2990 | Loss: 44.0444\n",
      "Epoch 5/10 | Batch 160/2990 | Loss: 168.3584\n",
      "Epoch 5/10 | Batch 180/2990 | Loss: 289.8153\n",
      "Epoch 5/10 | Batch 200/2990 | Loss: 324.8557\n",
      "Epoch 5/10 | Batch 220/2990 | Loss: 180.5932\n",
      "Epoch 5/10 | Batch 240/2990 | Loss: 241.6384\n",
      "Epoch 5/10 | Batch 260/2990 | Loss: 320.9105\n",
      "Epoch 5/10 | Batch 280/2990 | Loss: 178.3326\n",
      "Epoch 5/10 | Batch 300/2990 | Loss: 337.2614\n",
      "Epoch 5/10 | Batch 320/2990 | Loss: 312.2714\n",
      "Epoch 5/10 | Batch 340/2990 | Loss: 297.3528\n",
      "Epoch 5/10 | Batch 360/2990 | Loss: 139.2954\n",
      "Epoch 5/10 | Batch 380/2990 | Loss: 261.0461\n",
      "Epoch 5/10 | Batch 400/2990 | Loss: 115.4802\n",
      "Epoch 5/10 | Batch 420/2990 | Loss: 97.6940\n",
      "Epoch 5/10 | Batch 440/2990 | Loss: 154.6822\n",
      "Epoch 5/10 | Batch 460/2990 | Loss: 197.4734\n",
      "Epoch 5/10 | Batch 480/2990 | Loss: 171.7017\n",
      "Epoch 5/10 | Batch 500/2990 | Loss: 91.3151\n",
      "Epoch 5/10 | Batch 520/2990 | Loss: 194.7703\n",
      "Epoch 5/10 | Batch 540/2990 | Loss: 152.8500\n",
      "Epoch 5/10 | Batch 560/2990 | Loss: 95.7869\n",
      "Epoch 5/10 | Batch 580/2990 | Loss: 253.4629\n",
      "Epoch 5/10 | Batch 600/2990 | Loss: 42.6339\n",
      "Epoch 5/10 | Batch 620/2990 | Loss: 77.6158\n",
      "Epoch 5/10 | Batch 640/2990 | Loss: 145.9329\n",
      "Epoch 5/10 | Batch 660/2990 | Loss: 161.7003\n",
      "Epoch 5/10 | Batch 680/2990 | Loss: 106.1139\n",
      "Epoch 5/10 | Batch 700/2990 | Loss: 241.4639\n",
      "Epoch 5/10 | Batch 720/2990 | Loss: 200.2475\n",
      "Epoch 5/10 | Batch 740/2990 | Loss: 109.0540\n",
      "Epoch 5/10 | Batch 760/2990 | Loss: 465.8814\n",
      "Epoch 5/10 | Batch 780/2990 | Loss: 123.6548\n",
      "Epoch 5/10 | Batch 800/2990 | Loss: 215.5526\n",
      "Epoch 5/10 | Batch 820/2990 | Loss: 186.0910\n",
      "Epoch 5/10 | Batch 840/2990 | Loss: 348.5033\n",
      "Epoch 5/10 | Batch 860/2990 | Loss: 416.4683\n",
      "Epoch 5/10 | Batch 880/2990 | Loss: 136.9798\n",
      "Epoch 5/10 | Batch 900/2990 | Loss: 132.2875\n",
      "Epoch 5/10 | Batch 920/2990 | Loss: 84.1448\n",
      "Epoch 5/10 | Batch 940/2990 | Loss: 149.7003\n",
      "Epoch 5/10 | Batch 960/2990 | Loss: 261.5226\n",
      "Epoch 5/10 | Batch 980/2990 | Loss: 92.4756\n",
      "Epoch 5/10 | Batch 1000/2990 | Loss: 177.4938\n",
      "Epoch 5/10 | Batch 1020/2990 | Loss: 105.5709\n",
      "Epoch 5/10 | Batch 1040/2990 | Loss: 112.5513\n",
      "Epoch 5/10 | Batch 1060/2990 | Loss: 168.3733\n",
      "Epoch 5/10 | Batch 1080/2990 | Loss: 140.3181\n",
      "Epoch 5/10 | Batch 1100/2990 | Loss: 186.9395\n",
      "Epoch 5/10 | Batch 1120/2990 | Loss: 181.4895\n",
      "Epoch 5/10 | Batch 1140/2990 | Loss: 166.7560\n",
      "Epoch 5/10 | Batch 1160/2990 | Loss: 175.2420\n",
      "Epoch 5/10 | Batch 1180/2990 | Loss: 95.9889\n",
      "Epoch 5/10 | Batch 1200/2990 | Loss: 141.9955\n",
      "Epoch 5/10 | Batch 1220/2990 | Loss: 270.5642\n",
      "Epoch 5/10 | Batch 1240/2990 | Loss: 127.5026\n",
      "Epoch 5/10 | Batch 1260/2990 | Loss: 79.4506\n",
      "Epoch 5/10 | Batch 1280/2990 | Loss: 82.1066\n",
      "Epoch 5/10 | Batch 1300/2990 | Loss: 274.1047\n",
      "Epoch 5/10 | Batch 1320/2990 | Loss: 150.0607\n",
      "Epoch 5/10 | Batch 1340/2990 | Loss: 305.8683\n",
      "Epoch 5/10 | Batch 1360/2990 | Loss: 200.1910\n",
      "Epoch 5/10 | Batch 1380/2990 | Loss: 110.1743\n",
      "Epoch 5/10 | Batch 1400/2990 | Loss: 118.6911\n",
      "Epoch 5/10 | Batch 1420/2990 | Loss: 234.7215\n",
      "Epoch 5/10 | Batch 1440/2990 | Loss: 256.1513\n",
      "Epoch 5/10 | Batch 1460/2990 | Loss: 319.5352\n",
      "Epoch 5/10 | Batch 1480/2990 | Loss: 230.9282\n",
      "Epoch 5/10 | Batch 1500/2990 | Loss: 146.2427\n",
      "Epoch 5/10 | Batch 1520/2990 | Loss: 167.7091\n",
      "Epoch 5/10 | Batch 1540/2990 | Loss: 149.7449\n",
      "Epoch 5/10 | Batch 1560/2990 | Loss: 180.5214\n",
      "Epoch 5/10 | Batch 1580/2990 | Loss: 126.8045\n",
      "Epoch 5/10 | Batch 1600/2990 | Loss: 232.3730\n",
      "Epoch 5/10 | Batch 1620/2990 | Loss: 198.8254\n",
      "Epoch 5/10 | Batch 1640/2990 | Loss: 94.9441\n",
      "Epoch 5/10 | Batch 1660/2990 | Loss: 96.0925\n",
      "Epoch 5/10 | Batch 1680/2990 | Loss: 213.4518\n",
      "Epoch 5/10 | Batch 1700/2990 | Loss: 67.5718\n",
      "Epoch 5/10 | Batch 1720/2990 | Loss: 329.2236\n",
      "Epoch 5/10 | Batch 1740/2990 | Loss: 504.6438\n",
      "Epoch 5/10 | Batch 1760/2990 | Loss: 160.0186\n",
      "Epoch 5/10 | Batch 1780/2990 | Loss: 515.9696\n",
      "Epoch 5/10 | Batch 1800/2990 | Loss: 184.5480\n",
      "Epoch 5/10 | Batch 1820/2990 | Loss: 469.8826\n",
      "Epoch 5/10 | Batch 1840/2990 | Loss: 46.3089\n",
      "Epoch 5/10 | Batch 1860/2990 | Loss: 178.8116\n",
      "Epoch 5/10 | Batch 1880/2990 | Loss: 218.1682\n",
      "Epoch 5/10 | Batch 1900/2990 | Loss: 195.8386\n",
      "Epoch 5/10 | Batch 1920/2990 | Loss: 91.8797\n",
      "Epoch 5/10 | Batch 1940/2990 | Loss: 169.7578\n",
      "Epoch 5/10 | Batch 1960/2990 | Loss: 122.6300\n",
      "Epoch 5/10 | Batch 1980/2990 | Loss: 102.7940\n",
      "Epoch 5/10 | Batch 2000/2990 | Loss: 219.0505\n",
      "Epoch 5/10 | Batch 2020/2990 | Loss: 121.1321\n",
      "Epoch 5/10 | Batch 2040/2990 | Loss: 57.9047\n",
      "Epoch 5/10 | Batch 2060/2990 | Loss: 207.8639\n",
      "Epoch 5/10 | Batch 2080/2990 | Loss: 558.5101\n",
      "Epoch 5/10 | Batch 2100/2990 | Loss: 144.1310\n",
      "Epoch 5/10 | Batch 2120/2990 | Loss: 84.9600\n",
      "Epoch 5/10 | Batch 2140/2990 | Loss: 75.9856\n",
      "Epoch 5/10 | Batch 2160/2990 | Loss: 201.9939\n",
      "Epoch 5/10 | Batch 2180/2990 | Loss: 101.9648\n",
      "Epoch 5/10 | Batch 2200/2990 | Loss: 596.8124\n",
      "Epoch 5/10 | Batch 2220/2990 | Loss: 222.6042\n",
      "Epoch 5/10 | Batch 2240/2990 | Loss: 229.4951\n",
      "Epoch 5/10 | Batch 2260/2990 | Loss: 229.4780\n",
      "Epoch 5/10 | Batch 2280/2990 | Loss: 346.7185\n",
      "Epoch 5/10 | Batch 2300/2990 | Loss: 142.6902\n",
      "Epoch 5/10 | Batch 2320/2990 | Loss: 143.4590\n",
      "Epoch 5/10 | Batch 2340/2990 | Loss: 240.8668\n",
      "Epoch 5/10 | Batch 2360/2990 | Loss: 4261.3989\n",
      "Epoch 5/10 | Batch 2380/2990 | Loss: 157.8177\n",
      "Epoch 5/10 | Batch 2400/2990 | Loss: 169.6282\n",
      "Epoch 5/10 | Batch 2420/2990 | Loss: 353.0487\n",
      "Epoch 5/10 | Batch 2440/2990 | Loss: 150.3522\n",
      "Epoch 5/10 | Batch 2460/2990 | Loss: 133.7226\n",
      "Epoch 5/10 | Batch 2480/2990 | Loss: 113.1779\n",
      "Epoch 5/10 | Batch 2500/2990 | Loss: 466.4373\n",
      "Epoch 5/10 | Batch 2520/2990 | Loss: 305.2564\n",
      "Epoch 5/10 | Batch 2540/2990 | Loss: 355.8484\n",
      "Epoch 5/10 | Batch 2560/2990 | Loss: 298.1725\n",
      "Epoch 5/10 | Batch 2580/2990 | Loss: 441.1754\n",
      "Epoch 5/10 | Batch 2600/2990 | Loss: 238.8870\n",
      "Epoch 5/10 | Batch 2620/2990 | Loss: 328.0834\n",
      "Epoch 5/10 | Batch 2640/2990 | Loss: 222.4624\n",
      "Epoch 5/10 | Batch 2660/2990 | Loss: 791.2426\n",
      "Epoch 5/10 | Batch 2680/2990 | Loss: 151.5312\n",
      "Epoch 5/10 | Batch 2700/2990 | Loss: 138.3004\n",
      "Epoch 5/10 | Batch 2720/2990 | Loss: 99.3110\n",
      "Epoch 5/10 | Batch 2740/2990 | Loss: 228.2724\n",
      "Epoch 5/10 | Batch 2760/2990 | Loss: 278.7235\n",
      "Epoch 5/10 | Batch 2780/2990 | Loss: 85.0651\n",
      "Epoch 5/10 | Batch 2800/2990 | Loss: 123.9506\n",
      "Epoch 5/10 | Batch 2820/2990 | Loss: 351.2924\n",
      "Epoch 5/10 | Batch 2840/2990 | Loss: 301.3099\n",
      "Epoch 5/10 | Batch 2860/2990 | Loss: 265.9274\n",
      "Epoch 5/10 | Batch 2880/2990 | Loss: 138.7224\n",
      "Epoch 5/10 | Batch 2900/2990 | Loss: 363.2404\n",
      "Epoch 5/10 | Batch 2920/2990 | Loss: 118.2185\n",
      "Epoch 5/10 | Batch 2940/2990 | Loss: 256.7629\n",
      "Epoch 5/10 | Batch 2960/2990 | Loss: 152.7415\n",
      "Epoch 5/10 | Batch 2980/2990 | Loss: 408.9344\n",
      "✅ Epoch 5/10 | Train Loss: 221.3689 | Val Loss: 198.9466\n",
      "Epoch 6/10 | Batch 0/2990 | Loss: 128.6098\n",
      "Epoch 6/10 | Batch 20/2990 | Loss: 118.8270\n",
      "Epoch 6/10 | Batch 40/2990 | Loss: 235.8269\n",
      "Epoch 6/10 | Batch 60/2990 | Loss: 122.0010\n",
      "Epoch 6/10 | Batch 80/2990 | Loss: 197.4443\n",
      "Epoch 6/10 | Batch 100/2990 | Loss: 176.0887\n",
      "Epoch 6/10 | Batch 120/2990 | Loss: 115.1874\n",
      "Epoch 6/10 | Batch 140/2990 | Loss: 209.0440\n",
      "Epoch 6/10 | Batch 160/2990 | Loss: 125.9136\n",
      "Epoch 6/10 | Batch 180/2990 | Loss: 439.5110\n",
      "Epoch 6/10 | Batch 200/2990 | Loss: 107.0546\n",
      "Epoch 6/10 | Batch 220/2990 | Loss: 176.2024\n",
      "Epoch 6/10 | Batch 240/2990 | Loss: 128.2294\n",
      "Epoch 6/10 | Batch 260/2990 | Loss: 92.0571\n",
      "Epoch 6/10 | Batch 280/2990 | Loss: 105.0126\n",
      "Epoch 6/10 | Batch 300/2990 | Loss: 70.4507\n",
      "Epoch 6/10 | Batch 320/2990 | Loss: 198.8294\n",
      "Epoch 6/10 | Batch 340/2990 | Loss: 327.6010\n",
      "Epoch 6/10 | Batch 360/2990 | Loss: 370.4810\n",
      "Epoch 6/10 | Batch 380/2990 | Loss: 174.5518\n",
      "Epoch 6/10 | Batch 400/2990 | Loss: 133.9354\n",
      "Epoch 6/10 | Batch 420/2990 | Loss: 129.5258\n",
      "Epoch 6/10 | Batch 440/2990 | Loss: 205.3195\n",
      "Epoch 6/10 | Batch 460/2990 | Loss: 98.4306\n",
      "Epoch 6/10 | Batch 480/2990 | Loss: 203.7692\n",
      "Epoch 6/10 | Batch 500/2990 | Loss: 154.8004\n",
      "Epoch 6/10 | Batch 520/2990 | Loss: 171.2801\n",
      "Epoch 6/10 | Batch 540/2990 | Loss: 32.0514\n",
      "Epoch 6/10 | Batch 560/2990 | Loss: 97.8323\n",
      "Epoch 6/10 | Batch 580/2990 | Loss: 101.4611\n",
      "Epoch 6/10 | Batch 600/2990 | Loss: 97.3661\n",
      "Epoch 6/10 | Batch 620/2990 | Loss: 107.4652\n",
      "Epoch 6/10 | Batch 640/2990 | Loss: 38.5084\n",
      "Epoch 6/10 | Batch 660/2990 | Loss: 198.7325\n",
      "Epoch 6/10 | Batch 680/2990 | Loss: 128.8059\n",
      "Epoch 6/10 | Batch 700/2990 | Loss: 188.5080\n",
      "Epoch 6/10 | Batch 720/2990 | Loss: 134.8676\n",
      "Epoch 6/10 | Batch 740/2990 | Loss: 133.7648\n",
      "Epoch 6/10 | Batch 760/2990 | Loss: 361.3235\n",
      "Epoch 6/10 | Batch 780/2990 | Loss: 130.4311\n",
      "Epoch 6/10 | Batch 800/2990 | Loss: 775.8362\n",
      "Epoch 6/10 | Batch 820/2990 | Loss: 140.9402\n",
      "Epoch 6/10 | Batch 840/2990 | Loss: 226.8853\n",
      "Epoch 6/10 | Batch 860/2990 | Loss: 209.8835\n",
      "Epoch 6/10 | Batch 880/2990 | Loss: 135.9037\n",
      "Epoch 6/10 | Batch 900/2990 | Loss: 94.7439\n",
      "Epoch 6/10 | Batch 920/2990 | Loss: 129.2200\n",
      "Epoch 6/10 | Batch 940/2990 | Loss: 159.0449\n",
      "Epoch 6/10 | Batch 960/2990 | Loss: 72.4294\n",
      "Epoch 6/10 | Batch 980/2990 | Loss: 229.3988\n",
      "Epoch 6/10 | Batch 1000/2990 | Loss: 110.5816\n",
      "Epoch 6/10 | Batch 1020/2990 | Loss: 170.1365\n",
      "Epoch 6/10 | Batch 1040/2990 | Loss: 62.9938\n",
      "Epoch 6/10 | Batch 1060/2990 | Loss: 192.3756\n",
      "Epoch 6/10 | Batch 1080/2990 | Loss: 1125.6074\n",
      "Epoch 6/10 | Batch 1100/2990 | Loss: 63.3950\n",
      "Epoch 6/10 | Batch 1120/2990 | Loss: 595.2220\n",
      "Epoch 6/10 | Batch 1140/2990 | Loss: 94.8723\n",
      "Epoch 6/10 | Batch 1160/2990 | Loss: 223.7564\n",
      "Epoch 6/10 | Batch 1180/2990 | Loss: 160.0859\n",
      "Epoch 6/10 | Batch 1200/2990 | Loss: 107.3414\n",
      "Epoch 6/10 | Batch 1220/2990 | Loss: 172.2822\n",
      "Epoch 6/10 | Batch 1240/2990 | Loss: 100.2532\n",
      "Epoch 6/10 | Batch 1260/2990 | Loss: 229.8738\n",
      "Epoch 6/10 | Batch 1280/2990 | Loss: 116.4014\n",
      "Epoch 6/10 | Batch 1300/2990 | Loss: 257.6063\n",
      "Epoch 6/10 | Batch 1320/2990 | Loss: 338.6445\n",
      "Epoch 6/10 | Batch 1340/2990 | Loss: 126.9063\n",
      "Epoch 6/10 | Batch 1360/2990 | Loss: 48.1720\n",
      "Epoch 6/10 | Batch 1380/2990 | Loss: 121.7456\n",
      "Epoch 6/10 | Batch 1400/2990 | Loss: 253.6733\n",
      "Epoch 6/10 | Batch 1420/2990 | Loss: 40.7388\n",
      "Epoch 6/10 | Batch 1440/2990 | Loss: 80.1115\n",
      "Epoch 6/10 | Batch 1460/2990 | Loss: 207.1961\n",
      "Epoch 6/10 | Batch 1480/2990 | Loss: 237.9743\n",
      "Epoch 6/10 | Batch 1500/2990 | Loss: 184.9045\n",
      "Epoch 6/10 | Batch 1520/2990 | Loss: 58.1517\n",
      "Epoch 6/10 | Batch 1540/2990 | Loss: 211.2374\n",
      "Epoch 6/10 | Batch 1560/2990 | Loss: 112.7271\n",
      "Epoch 6/10 | Batch 1580/2990 | Loss: 112.1854\n",
      "Epoch 6/10 | Batch 1600/2990 | Loss: 146.9101\n",
      "Epoch 6/10 | Batch 1620/2990 | Loss: 69.6472\n",
      "Epoch 6/10 | Batch 1640/2990 | Loss: 362.0710\n",
      "Epoch 6/10 | Batch 1660/2990 | Loss: 233.6436\n",
      "Epoch 6/10 | Batch 1680/2990 | Loss: 193.5322\n",
      "Epoch 6/10 | Batch 1700/2990 | Loss: 170.3999\n",
      "Epoch 6/10 | Batch 1720/2990 | Loss: 276.9046\n",
      "Epoch 6/10 | Batch 1740/2990 | Loss: 277.7329\n",
      "Epoch 6/10 | Batch 1760/2990 | Loss: 71.1005\n",
      "Epoch 6/10 | Batch 1780/2990 | Loss: 272.1834\n",
      "Epoch 6/10 | Batch 1800/2990 | Loss: 344.5975\n",
      "Epoch 6/10 | Batch 1820/2990 | Loss: 209.1496\n",
      "Epoch 6/10 | Batch 1840/2990 | Loss: 100.3186\n",
      "Epoch 6/10 | Batch 1860/2990 | Loss: 199.4248\n",
      "Epoch 6/10 | Batch 1880/2990 | Loss: 155.1692\n",
      "Epoch 6/10 | Batch 1900/2990 | Loss: 104.2732\n",
      "Epoch 6/10 | Batch 1920/2990 | Loss: 159.7750\n",
      "Epoch 6/10 | Batch 1940/2990 | Loss: 223.4987\n",
      "Epoch 6/10 | Batch 1960/2990 | Loss: 103.2815\n",
      "Epoch 6/10 | Batch 1980/2990 | Loss: 89.4642\n",
      "Epoch 6/10 | Batch 2000/2990 | Loss: 96.3525\n",
      "Epoch 6/10 | Batch 2020/2990 | Loss: 123.3484\n",
      "Epoch 6/10 | Batch 2040/2990 | Loss: 141.9658\n",
      "Epoch 6/10 | Batch 2060/2990 | Loss: 241.0722\n",
      "Epoch 6/10 | Batch 2080/2990 | Loss: 120.7535\n",
      "Epoch 6/10 | Batch 2100/2990 | Loss: 148.0724\n",
      "Epoch 6/10 | Batch 2120/2990 | Loss: 86.2757\n",
      "Epoch 6/10 | Batch 2140/2990 | Loss: 173.5137\n",
      "Epoch 6/10 | Batch 2160/2990 | Loss: 96.1045\n",
      "Epoch 6/10 | Batch 2180/2990 | Loss: 174.2720\n",
      "Epoch 6/10 | Batch 2200/2990 | Loss: 86.2332\n",
      "Epoch 6/10 | Batch 2220/2990 | Loss: 94.3181\n",
      "Epoch 6/10 | Batch 2240/2990 | Loss: 120.1560\n",
      "Epoch 6/10 | Batch 2260/2990 | Loss: 85.2495\n",
      "Epoch 6/10 | Batch 2280/2990 | Loss: 79.4165\n",
      "Epoch 6/10 | Batch 2300/2990 | Loss: 195.9568\n",
      "Epoch 6/10 | Batch 2320/2990 | Loss: 119.6133\n",
      "Epoch 6/10 | Batch 2340/2990 | Loss: 119.1343\n",
      "Epoch 6/10 | Batch 2360/2990 | Loss: 98.3615\n",
      "Epoch 6/10 | Batch 2380/2990 | Loss: 150.0129\n",
      "Epoch 6/10 | Batch 2400/2990 | Loss: 108.2900\n",
      "Epoch 6/10 | Batch 2420/2990 | Loss: 155.2264\n",
      "Epoch 6/10 | Batch 2440/2990 | Loss: 118.4180\n",
      "Epoch 6/10 | Batch 2460/2990 | Loss: 186.3988\n",
      "Epoch 6/10 | Batch 2480/2990 | Loss: 251.0408\n",
      "Epoch 6/10 | Batch 2500/2990 | Loss: 281.2994\n",
      "Epoch 6/10 | Batch 2520/2990 | Loss: 75.0148\n",
      "Epoch 6/10 | Batch 2540/2990 | Loss: 202.4139\n",
      "Epoch 6/10 | Batch 2560/2990 | Loss: 147.2114\n",
      "Epoch 6/10 | Batch 2580/2990 | Loss: 200.3713\n",
      "Epoch 6/10 | Batch 2600/2990 | Loss: 175.7049\n",
      "Epoch 6/10 | Batch 2620/2990 | Loss: 308.2658\n",
      "Epoch 6/10 | Batch 2640/2990 | Loss: 30.2409\n",
      "Epoch 6/10 | Batch 2660/2990 | Loss: 185.3456\n",
      "Epoch 6/10 | Batch 2680/2990 | Loss: 277.4114\n",
      "Epoch 6/10 | Batch 2700/2990 | Loss: 147.0500\n",
      "Epoch 6/10 | Batch 2720/2990 | Loss: 201.1320\n",
      "Epoch 6/10 | Batch 2740/2990 | Loss: 44.7233\n",
      "Epoch 6/10 | Batch 2760/2990 | Loss: 109.2623\n",
      "Epoch 6/10 | Batch 2780/2990 | Loss: 134.6982\n",
      "Epoch 6/10 | Batch 2800/2990 | Loss: 142.6018\n",
      "Epoch 6/10 | Batch 2820/2990 | Loss: 75.3929\n",
      "Epoch 6/10 | Batch 2840/2990 | Loss: 160.0394\n",
      "Epoch 6/10 | Batch 2860/2990 | Loss: 117.9947\n",
      "Epoch 6/10 | Batch 2880/2990 | Loss: 180.0272\n",
      "Epoch 6/10 | Batch 2900/2990 | Loss: 94.5213\n",
      "Epoch 6/10 | Batch 2920/2990 | Loss: 152.1076\n",
      "Epoch 6/10 | Batch 2940/2990 | Loss: 96.7601\n",
      "Epoch 6/10 | Batch 2960/2990 | Loss: 151.8585\n",
      "Epoch 6/10 | Batch 2980/2990 | Loss: 97.2099\n",
      "✅ Epoch 6/10 | Train Loss: 166.3174 | Val Loss: 141.2293\n",
      "Epoch 7/10 | Batch 0/2990 | Loss: 41.7569\n",
      "Epoch 7/10 | Batch 20/2990 | Loss: 103.1324\n",
      "Epoch 7/10 | Batch 40/2990 | Loss: 86.4535\n",
      "Epoch 7/10 | Batch 60/2990 | Loss: 147.5300\n",
      "Epoch 7/10 | Batch 80/2990 | Loss: 169.7431\n",
      "Epoch 7/10 | Batch 100/2990 | Loss: 114.4008\n",
      "Epoch 7/10 | Batch 120/2990 | Loss: 68.8219\n",
      "Epoch 7/10 | Batch 140/2990 | Loss: 160.7711\n",
      "Epoch 7/10 | Batch 160/2990 | Loss: 207.6023\n",
      "Epoch 7/10 | Batch 180/2990 | Loss: 78.1668\n",
      "Epoch 7/10 | Batch 200/2990 | Loss: 273.3345\n",
      "Epoch 7/10 | Batch 220/2990 | Loss: 127.6354\n",
      "Epoch 7/10 | Batch 240/2990 | Loss: 65.5831\n",
      "Epoch 7/10 | Batch 260/2990 | Loss: 165.9372\n",
      "Epoch 7/10 | Batch 280/2990 | Loss: 231.9223\n",
      "Epoch 7/10 | Batch 300/2990 | Loss: 80.7063\n",
      "Epoch 7/10 | Batch 320/2990 | Loss: 138.5318\n",
      "Epoch 7/10 | Batch 340/2990 | Loss: 109.9193\n",
      "Epoch 7/10 | Batch 360/2990 | Loss: 165.8526\n",
      "Epoch 7/10 | Batch 380/2990 | Loss: 167.7343\n",
      "Epoch 7/10 | Batch 400/2990 | Loss: 265.0954\n",
      "Epoch 7/10 | Batch 420/2990 | Loss: 74.4116\n",
      "Epoch 7/10 | Batch 440/2990 | Loss: 94.7990\n",
      "Epoch 7/10 | Batch 460/2990 | Loss: 91.1285\n",
      "Epoch 7/10 | Batch 480/2990 | Loss: 121.0113\n",
      "Epoch 7/10 | Batch 500/2990 | Loss: 84.8989\n",
      "Epoch 7/10 | Batch 520/2990 | Loss: 97.4333\n",
      "Epoch 7/10 | Batch 540/2990 | Loss: 173.5923\n",
      "Epoch 7/10 | Batch 560/2990 | Loss: 121.2135\n",
      "Epoch 7/10 | Batch 580/2990 | Loss: 109.8063\n",
      "Epoch 7/10 | Batch 600/2990 | Loss: 75.9183\n",
      "Epoch 7/10 | Batch 620/2990 | Loss: 51.3567\n",
      "Epoch 7/10 | Batch 640/2990 | Loss: 137.0692\n",
      "Epoch 7/10 | Batch 660/2990 | Loss: 315.0859\n",
      "Epoch 7/10 | Batch 680/2990 | Loss: 281.7526\n",
      "Epoch 7/10 | Batch 700/2990 | Loss: 128.6266\n",
      "Epoch 7/10 | Batch 720/2990 | Loss: 147.5268\n",
      "Epoch 7/10 | Batch 740/2990 | Loss: 27.4851\n",
      "Epoch 7/10 | Batch 760/2990 | Loss: 88.9689\n",
      "Epoch 7/10 | Batch 780/2990 | Loss: 107.2566\n",
      "Epoch 7/10 | Batch 800/2990 | Loss: 109.6155\n",
      "Epoch 7/10 | Batch 820/2990 | Loss: 102.3167\n",
      "Epoch 7/10 | Batch 840/2990 | Loss: 83.4319\n",
      "Epoch 7/10 | Batch 860/2990 | Loss: 121.1532\n",
      "Epoch 7/10 | Batch 880/2990 | Loss: 120.2524\n",
      "Epoch 7/10 | Batch 900/2990 | Loss: 121.9730\n",
      "Epoch 7/10 | Batch 920/2990 | Loss: 161.6637\n",
      "Epoch 7/10 | Batch 940/2990 | Loss: 126.9136\n",
      "Epoch 7/10 | Batch 960/2990 | Loss: 86.9255\n",
      "Epoch 7/10 | Batch 980/2990 | Loss: 142.2572\n",
      "Epoch 7/10 | Batch 1000/2990 | Loss: 72.6013\n",
      "Epoch 7/10 | Batch 1020/2990 | Loss: 86.5704\n",
      "Epoch 7/10 | Batch 1040/2990 | Loss: 229.3365\n",
      "Epoch 7/10 | Batch 1060/2990 | Loss: 253.0856\n",
      "Epoch 7/10 | Batch 1080/2990 | Loss: 334.0889\n",
      "Epoch 7/10 | Batch 1100/2990 | Loss: 146.9468\n",
      "Epoch 7/10 | Batch 1120/2990 | Loss: 157.4472\n",
      "Epoch 7/10 | Batch 1140/2990 | Loss: 187.6266\n",
      "Epoch 7/10 | Batch 1160/2990 | Loss: 242.0005\n",
      "Epoch 7/10 | Batch 1180/2990 | Loss: 129.1507\n",
      "Epoch 7/10 | Batch 1200/2990 | Loss: 434.4954\n",
      "Epoch 7/10 | Batch 1220/2990 | Loss: 232.2794\n",
      "Epoch 7/10 | Batch 1240/2990 | Loss: 104.2728\n",
      "Epoch 7/10 | Batch 1260/2990 | Loss: 59.8921\n",
      "Epoch 7/10 | Batch 1280/2990 | Loss: 201.3214\n",
      "Epoch 7/10 | Batch 1300/2990 | Loss: 99.4838\n",
      "Epoch 7/10 | Batch 1320/2990 | Loss: 88.4711\n",
      "Epoch 7/10 | Batch 1340/2990 | Loss: 127.1318\n",
      "Epoch 7/10 | Batch 1360/2990 | Loss: 96.5840\n",
      "Epoch 7/10 | Batch 1380/2990 | Loss: 82.2561\n",
      "Epoch 7/10 | Batch 1400/2990 | Loss: 72.4143\n",
      "Epoch 7/10 | Batch 1420/2990 | Loss: 46.7238\n",
      "Epoch 7/10 | Batch 1440/2990 | Loss: 212.7048\n",
      "Epoch 7/10 | Batch 1460/2990 | Loss: 107.3305\n",
      "Epoch 7/10 | Batch 1480/2990 | Loss: 198.1096\n",
      "Epoch 7/10 | Batch 1500/2990 | Loss: 301.4029\n",
      "Epoch 7/10 | Batch 1520/2990 | Loss: 69.3394\n",
      "Epoch 7/10 | Batch 1540/2990 | Loss: 275.0054\n",
      "Epoch 7/10 | Batch 1560/2990 | Loss: 177.2995\n",
      "Epoch 7/10 | Batch 1580/2990 | Loss: 157.2224\n",
      "Epoch 7/10 | Batch 1600/2990 | Loss: 138.6587\n",
      "Epoch 7/10 | Batch 1620/2990 | Loss: 122.5190\n",
      "Epoch 7/10 | Batch 1640/2990 | Loss: 112.0236\n",
      "Epoch 7/10 | Batch 1660/2990 | Loss: 302.5921\n",
      "Epoch 7/10 | Batch 1680/2990 | Loss: 102.9332\n",
      "Epoch 7/10 | Batch 1700/2990 | Loss: 89.7386\n",
      "Epoch 7/10 | Batch 1720/2990 | Loss: 66.2048\n",
      "Epoch 7/10 | Batch 1740/2990 | Loss: 79.1645\n",
      "Epoch 7/10 | Batch 1760/2990 | Loss: 130.2587\n",
      "Epoch 7/10 | Batch 1780/2990 | Loss: 74.8331\n",
      "Epoch 7/10 | Batch 1800/2990 | Loss: 136.7375\n",
      "Epoch 7/10 | Batch 1820/2990 | Loss: 99.9924\n",
      "Epoch 7/10 | Batch 1840/2990 | Loss: 192.2727\n",
      "Epoch 7/10 | Batch 1860/2990 | Loss: 52.4277\n",
      "Epoch 7/10 | Batch 1880/2990 | Loss: 59.5996\n",
      "Epoch 7/10 | Batch 1900/2990 | Loss: 75.8867\n",
      "Epoch 7/10 | Batch 1920/2990 | Loss: 91.8570\n",
      "Epoch 7/10 | Batch 1940/2990 | Loss: 98.6598\n",
      "Epoch 7/10 | Batch 1960/2990 | Loss: 142.1390\n",
      "Epoch 7/10 | Batch 1980/2990 | Loss: 262.3525\n",
      "Epoch 7/10 | Batch 2000/2990 | Loss: 163.8118\n",
      "Epoch 7/10 | Batch 2020/2990 | Loss: 68.5185\n",
      "Epoch 7/10 | Batch 2040/2990 | Loss: 64.4330\n",
      "Epoch 7/10 | Batch 2060/2990 | Loss: 139.9850\n",
      "Epoch 7/10 | Batch 2080/2990 | Loss: 113.2260\n",
      "Epoch 7/10 | Batch 2100/2990 | Loss: 97.7904\n",
      "Epoch 7/10 | Batch 2120/2990 | Loss: 98.4110\n",
      "Epoch 7/10 | Batch 2140/2990 | Loss: 75.4741\n",
      "Epoch 7/10 | Batch 2160/2990 | Loss: 194.2143\n",
      "Epoch 7/10 | Batch 2180/2990 | Loss: 137.6782\n",
      "Epoch 7/10 | Batch 2200/2990 | Loss: 219.1974\n",
      "Epoch 7/10 | Batch 2220/2990 | Loss: 168.4092\n",
      "Epoch 7/10 | Batch 2240/2990 | Loss: 85.2090\n",
      "Epoch 7/10 | Batch 2260/2990 | Loss: 274.8167\n",
      "Epoch 7/10 | Batch 2280/2990 | Loss: 155.5208\n",
      "Epoch 7/10 | Batch 2300/2990 | Loss: 271.2427\n",
      "Epoch 7/10 | Batch 2320/2990 | Loss: 65.9757\n",
      "Epoch 7/10 | Batch 2340/2990 | Loss: 124.5454\n",
      "Epoch 7/10 | Batch 2360/2990 | Loss: 69.7266\n",
      "Epoch 7/10 | Batch 2380/2990 | Loss: 133.4029\n",
      "Epoch 7/10 | Batch 2400/2990 | Loss: 54.6065\n",
      "Epoch 7/10 | Batch 2420/2990 | Loss: 144.0410\n",
      "Epoch 7/10 | Batch 2440/2990 | Loss: 74.5067\n",
      "Epoch 7/10 | Batch 2460/2990 | Loss: 152.8000\n",
      "Epoch 7/10 | Batch 2480/2990 | Loss: 105.7056\n",
      "Epoch 7/10 | Batch 2500/2990 | Loss: 158.5117\n",
      "Epoch 7/10 | Batch 2520/2990 | Loss: 95.0387\n",
      "Epoch 7/10 | Batch 2540/2990 | Loss: 118.7701\n",
      "Epoch 7/10 | Batch 2560/2990 | Loss: 179.3278\n",
      "Epoch 7/10 | Batch 2580/2990 | Loss: 29.6719\n",
      "Epoch 7/10 | Batch 2600/2990 | Loss: 121.5463\n",
      "Epoch 7/10 | Batch 2620/2990 | Loss: 51.8079\n",
      "Epoch 7/10 | Batch 2640/2990 | Loss: 74.3512\n",
      "Epoch 7/10 | Batch 2660/2990 | Loss: 66.3669\n",
      "Epoch 7/10 | Batch 2680/2990 | Loss: 109.4348\n",
      "Epoch 7/10 | Batch 2700/2990 | Loss: 218.4646\n",
      "Epoch 7/10 | Batch 2720/2990 | Loss: 73.4401\n",
      "Epoch 7/10 | Batch 2740/2990 | Loss: 136.7209\n",
      "Epoch 7/10 | Batch 2760/2990 | Loss: 50.1187\n",
      "Epoch 7/10 | Batch 2780/2990 | Loss: 92.0854\n",
      "Epoch 7/10 | Batch 2800/2990 | Loss: 59.5794\n",
      "Epoch 7/10 | Batch 2820/2990 | Loss: 82.5872\n",
      "Epoch 7/10 | Batch 2840/2990 | Loss: 82.6138\n",
      "Epoch 7/10 | Batch 2860/2990 | Loss: 75.1338\n",
      "Epoch 7/10 | Batch 2880/2990 | Loss: 58.1935\n",
      "Epoch 7/10 | Batch 2900/2990 | Loss: 65.3691\n",
      "Epoch 7/10 | Batch 2920/2990 | Loss: 192.3364\n",
      "Epoch 7/10 | Batch 2940/2990 | Loss: 88.5926\n",
      "Epoch 7/10 | Batch 2960/2990 | Loss: 52.9842\n",
      "Epoch 7/10 | Batch 2980/2990 | Loss: 264.5442\n",
      "✅ Epoch 7/10 | Train Loss: 140.1139 | Val Loss: 113.8758\n",
      "Epoch 8/10 | Batch 0/2990 | Loss: 154.2229\n",
      "Epoch 8/10 | Batch 20/2990 | Loss: 36.7371\n",
      "Epoch 8/10 | Batch 40/2990 | Loss: 107.8575\n",
      "Epoch 8/10 | Batch 60/2990 | Loss: 64.8700\n",
      "Epoch 8/10 | Batch 80/2990 | Loss: 103.1795\n",
      "Epoch 8/10 | Batch 100/2990 | Loss: 115.6353\n",
      "Epoch 8/10 | Batch 120/2990 | Loss: 35.9004\n",
      "Epoch 8/10 | Batch 140/2990 | Loss: 41.2149\n",
      "Epoch 8/10 | Batch 160/2990 | Loss: 78.7016\n",
      "Epoch 8/10 | Batch 180/2990 | Loss: 83.5285\n",
      "Epoch 8/10 | Batch 200/2990 | Loss: 82.9056\n",
      "Epoch 8/10 | Batch 220/2990 | Loss: 63.0158\n",
      "Epoch 8/10 | Batch 240/2990 | Loss: 79.3393\n",
      "Epoch 8/10 | Batch 260/2990 | Loss: 52.5016\n",
      "Epoch 8/10 | Batch 280/2990 | Loss: 72.8939\n",
      "Epoch 8/10 | Batch 300/2990 | Loss: 190.8337\n",
      "Epoch 8/10 | Batch 320/2990 | Loss: 69.2728\n",
      "Epoch 8/10 | Batch 340/2990 | Loss: 133.8385\n",
      "Epoch 8/10 | Batch 360/2990 | Loss: 67.4513\n",
      "Epoch 8/10 | Batch 380/2990 | Loss: 108.8992\n",
      "Epoch 8/10 | Batch 400/2990 | Loss: 198.9921\n",
      "Epoch 8/10 | Batch 420/2990 | Loss: 97.5149\n",
      "Epoch 8/10 | Batch 440/2990 | Loss: 132.6465\n",
      "Epoch 8/10 | Batch 460/2990 | Loss: 197.7796\n",
      "Epoch 8/10 | Batch 480/2990 | Loss: 94.2403\n",
      "Epoch 8/10 | Batch 500/2990 | Loss: 49.3627\n",
      "Epoch 8/10 | Batch 520/2990 | Loss: 126.9787\n",
      "Epoch 8/10 | Batch 540/2990 | Loss: 198.2026\n",
      "Epoch 8/10 | Batch 560/2990 | Loss: 236.2298\n",
      "Epoch 8/10 | Batch 580/2990 | Loss: 252.9619\n",
      "Epoch 8/10 | Batch 600/2990 | Loss: 88.2596\n",
      "Epoch 8/10 | Batch 620/2990 | Loss: 85.7973\n",
      "Epoch 8/10 | Batch 640/2990 | Loss: 46.5750\n",
      "Epoch 8/10 | Batch 660/2990 | Loss: 266.2845\n",
      "Epoch 8/10 | Batch 680/2990 | Loss: 87.3139\n",
      "Epoch 8/10 | Batch 700/2990 | Loss: 87.7594\n",
      "Epoch 8/10 | Batch 720/2990 | Loss: 84.9884\n",
      "Epoch 8/10 | Batch 740/2990 | Loss: 51.3093\n",
      "Epoch 8/10 | Batch 760/2990 | Loss: 78.5357\n",
      "Epoch 8/10 | Batch 780/2990 | Loss: 81.0735\n",
      "Epoch 8/10 | Batch 800/2990 | Loss: 104.9405\n",
      "Epoch 8/10 | Batch 820/2990 | Loss: 88.7833\n",
      "Epoch 8/10 | Batch 840/2990 | Loss: 143.8960\n",
      "Epoch 8/10 | Batch 860/2990 | Loss: 80.2095\n",
      "Epoch 8/10 | Batch 880/2990 | Loss: 182.5553\n",
      "Epoch 8/10 | Batch 900/2990 | Loss: 82.2689\n",
      "Epoch 8/10 | Batch 920/2990 | Loss: 191.5660\n",
      "Epoch 8/10 | Batch 940/2990 | Loss: 158.4297\n",
      "Epoch 8/10 | Batch 960/2990 | Loss: 100.5679\n",
      "Epoch 8/10 | Batch 980/2990 | Loss: 83.7244\n",
      "Epoch 8/10 | Batch 1000/2990 | Loss: 175.3502\n",
      "Epoch 8/10 | Batch 1020/2990 | Loss: 89.0766\n",
      "Epoch 8/10 | Batch 1040/2990 | Loss: 74.8990\n",
      "Epoch 8/10 | Batch 1060/2990 | Loss: 121.7762\n",
      "Epoch 8/10 | Batch 1080/2990 | Loss: 146.2238\n",
      "Epoch 8/10 | Batch 1100/2990 | Loss: 136.3752\n",
      "Epoch 8/10 | Batch 1120/2990 | Loss: 160.6021\n",
      "Epoch 8/10 | Batch 1140/2990 | Loss: 182.9826\n",
      "Epoch 8/10 | Batch 1160/2990 | Loss: 83.5681\n",
      "Epoch 8/10 | Batch 1180/2990 | Loss: 127.2639\n",
      "Epoch 8/10 | Batch 1200/2990 | Loss: 169.8739\n",
      "Epoch 8/10 | Batch 1220/2990 | Loss: 151.7874\n",
      "Epoch 8/10 | Batch 1240/2990 | Loss: 95.0169\n",
      "Epoch 8/10 | Batch 1260/2990 | Loss: 139.8587\n",
      "Epoch 8/10 | Batch 1280/2990 | Loss: 147.9663\n",
      "Epoch 8/10 | Batch 1300/2990 | Loss: 182.9250\n",
      "Epoch 8/10 | Batch 1320/2990 | Loss: 100.4124\n",
      "Epoch 8/10 | Batch 1340/2990 | Loss: 225.9909\n",
      "Epoch 8/10 | Batch 1360/2990 | Loss: 90.4236\n",
      "Epoch 8/10 | Batch 1380/2990 | Loss: 133.8939\n",
      "Epoch 8/10 | Batch 1400/2990 | Loss: 135.8000\n",
      "Epoch 8/10 | Batch 1420/2990 | Loss: 136.2304\n",
      "Epoch 8/10 | Batch 1440/2990 | Loss: 76.7854\n",
      "Epoch 8/10 | Batch 1460/2990 | Loss: 110.3340\n",
      "Epoch 8/10 | Batch 1480/2990 | Loss: 55.9626\n",
      "Epoch 8/10 | Batch 1500/2990 | Loss: 100.3820\n",
      "Epoch 8/10 | Batch 1520/2990 | Loss: 148.5036\n",
      "Epoch 8/10 | Batch 1540/2990 | Loss: 378.1828\n",
      "Epoch 8/10 | Batch 1560/2990 | Loss: 263.4537\n",
      "Epoch 8/10 | Batch 1580/2990 | Loss: 91.4913\n",
      "Epoch 8/10 | Batch 1600/2990 | Loss: 76.4585\n",
      "Epoch 8/10 | Batch 1620/2990 | Loss: 99.5133\n",
      "Epoch 8/10 | Batch 1640/2990 | Loss: 36.0688\n",
      "Epoch 8/10 | Batch 1660/2990 | Loss: 98.2749\n",
      "Epoch 8/10 | Batch 1680/2990 | Loss: 96.1889\n",
      "Epoch 8/10 | Batch 1700/2990 | Loss: 90.9052\n",
      "Epoch 8/10 | Batch 1720/2990 | Loss: 58.9079\n",
      "Epoch 8/10 | Batch 1740/2990 | Loss: 87.5281\n",
      "Epoch 8/10 | Batch 1760/2990 | Loss: 90.1189\n",
      "Epoch 8/10 | Batch 1780/2990 | Loss: 61.0947\n",
      "Epoch 8/10 | Batch 1800/2990 | Loss: 137.9876\n",
      "Epoch 8/10 | Batch 1820/2990 | Loss: 47.0565\n",
      "Epoch 8/10 | Batch 1840/2990 | Loss: 78.0471\n",
      "Epoch 8/10 | Batch 1860/2990 | Loss: 96.1116\n",
      "Epoch 8/10 | Batch 1880/2990 | Loss: 80.0046\n",
      "Epoch 8/10 | Batch 1900/2990 | Loss: 175.6256\n",
      "Epoch 8/10 | Batch 1920/2990 | Loss: 32.0218\n",
      "Epoch 8/10 | Batch 1940/2990 | Loss: 143.0052\n",
      "Epoch 8/10 | Batch 1960/2990 | Loss: 190.1712\n",
      "Epoch 8/10 | Batch 1980/2990 | Loss: 48.5084\n",
      "Epoch 8/10 | Batch 2000/2990 | Loss: 71.7158\n",
      "Epoch 8/10 | Batch 2020/2990 | Loss: 106.0930\n",
      "Epoch 8/10 | Batch 2040/2990 | Loss: 50.2610\n",
      "Epoch 8/10 | Batch 2060/2990 | Loss: 85.7113\n",
      "Epoch 8/10 | Batch 2080/2990 | Loss: 153.7219\n",
      "Epoch 8/10 | Batch 2100/2990 | Loss: 121.5390\n",
      "Epoch 8/10 | Batch 2120/2990 | Loss: 89.3311\n",
      "Epoch 8/10 | Batch 2140/2990 | Loss: 98.3165\n",
      "Epoch 8/10 | Batch 2160/2990 | Loss: 106.0504\n",
      "Epoch 8/10 | Batch 2180/2990 | Loss: 90.6479\n",
      "Epoch 8/10 | Batch 2200/2990 | Loss: 50.9070\n",
      "Epoch 8/10 | Batch 2220/2990 | Loss: 59.5207\n",
      "Epoch 8/10 | Batch 2240/2990 | Loss: 67.7443\n",
      "Epoch 8/10 | Batch 2260/2990 | Loss: 130.7131\n",
      "Epoch 8/10 | Batch 2280/2990 | Loss: 80.7521\n",
      "Epoch 8/10 | Batch 2300/2990 | Loss: 64.6019\n",
      "Epoch 8/10 | Batch 2320/2990 | Loss: 143.8854\n",
      "Epoch 8/10 | Batch 2340/2990 | Loss: 159.1922\n",
      "Epoch 8/10 | Batch 2360/2990 | Loss: 147.8541\n",
      "Epoch 8/10 | Batch 2380/2990 | Loss: 80.4578\n",
      "Epoch 8/10 | Batch 2400/2990 | Loss: 87.8697\n",
      "Epoch 8/10 | Batch 2420/2990 | Loss: 57.9993\n",
      "Epoch 8/10 | Batch 2440/2990 | Loss: 61.5858\n",
      "Epoch 8/10 | Batch 2460/2990 | Loss: 230.2553\n",
      "Epoch 8/10 | Batch 2480/2990 | Loss: 201.1993\n",
      "Epoch 8/10 | Batch 2500/2990 | Loss: 100.7574\n",
      "Epoch 8/10 | Batch 2520/2990 | Loss: 126.4165\n",
      "Epoch 8/10 | Batch 2540/2990 | Loss: 31.8154\n",
      "Epoch 8/10 | Batch 2560/2990 | Loss: 88.5262\n",
      "Epoch 8/10 | Batch 2580/2990 | Loss: 162.5287\n",
      "Epoch 8/10 | Batch 2600/2990 | Loss: 59.2168\n",
      "Epoch 8/10 | Batch 2620/2990 | Loss: 192.6270\n",
      "Epoch 8/10 | Batch 2640/2990 | Loss: 76.8252\n",
      "Epoch 8/10 | Batch 2660/2990 | Loss: 125.9495\n",
      "Epoch 8/10 | Batch 2680/2990 | Loss: 90.6290\n",
      "Epoch 8/10 | Batch 2700/2990 | Loss: 130.0341\n",
      "Epoch 8/10 | Batch 2720/2990 | Loss: 79.4088\n",
      "Epoch 8/10 | Batch 2740/2990 | Loss: 165.5269\n",
      "Epoch 8/10 | Batch 2760/2990 | Loss: 74.3939\n",
      "Epoch 8/10 | Batch 2780/2990 | Loss: 44.1489\n",
      "Epoch 8/10 | Batch 2800/2990 | Loss: 24.7740\n",
      "Epoch 8/10 | Batch 2820/2990 | Loss: 119.0422\n",
      "Epoch 8/10 | Batch 2840/2990 | Loss: 161.3690\n",
      "Epoch 8/10 | Batch 2860/2990 | Loss: 175.6030\n",
      "Epoch 8/10 | Batch 2880/2990 | Loss: 54.7256\n",
      "Epoch 8/10 | Batch 2900/2990 | Loss: 126.8782\n",
      "Epoch 8/10 | Batch 2920/2990 | Loss: 40.2196\n",
      "Epoch 8/10 | Batch 2940/2990 | Loss: 72.3322\n",
      "Epoch 8/10 | Batch 2960/2990 | Loss: 151.4030\n",
      "Epoch 8/10 | Batch 2980/2990 | Loss: 72.1577\n",
      "✅ Epoch 8/10 | Train Loss: 114.9007 | Val Loss: 168.7195\n",
      "Epoch 9/10 | Batch 0/2990 | Loss: 250.5637\n",
      "Epoch 9/10 | Batch 20/2990 | Loss: 44.1086\n",
      "Epoch 9/10 | Batch 40/2990 | Loss: 147.3191\n",
      "Epoch 9/10 | Batch 60/2990 | Loss: 67.3897\n",
      "Epoch 9/10 | Batch 80/2990 | Loss: 99.4085\n",
      "Epoch 9/10 | Batch 100/2990 | Loss: 55.2516\n",
      "Epoch 9/10 | Batch 120/2990 | Loss: 100.5146\n",
      "Epoch 9/10 | Batch 140/2990 | Loss: 163.2758\n",
      "Epoch 9/10 | Batch 160/2990 | Loss: 322.4578\n",
      "Epoch 9/10 | Batch 180/2990 | Loss: 118.9226\n",
      "Epoch 9/10 | Batch 200/2990 | Loss: 60.5252\n",
      "Epoch 9/10 | Batch 220/2990 | Loss: 150.4772\n",
      "Epoch 9/10 | Batch 240/2990 | Loss: 41.5078\n",
      "Epoch 9/10 | Batch 260/2990 | Loss: 186.0043\n",
      "Epoch 9/10 | Batch 280/2990 | Loss: 117.9353\n",
      "Epoch 9/10 | Batch 300/2990 | Loss: 116.3782\n",
      "Epoch 9/10 | Batch 320/2990 | Loss: 60.2654\n",
      "Epoch 9/10 | Batch 340/2990 | Loss: 61.1880\n",
      "Epoch 9/10 | Batch 360/2990 | Loss: 89.1017\n",
      "Epoch 9/10 | Batch 380/2990 | Loss: 37.0422\n",
      "Epoch 9/10 | Batch 400/2990 | Loss: 101.4739\n",
      "Epoch 9/10 | Batch 420/2990 | Loss: 33.3249\n",
      "Epoch 9/10 | Batch 440/2990 | Loss: 80.9321\n",
      "Epoch 9/10 | Batch 460/2990 | Loss: 85.0938\n",
      "Epoch 9/10 | Batch 480/2990 | Loss: 138.4900\n",
      "Epoch 9/10 | Batch 500/2990 | Loss: 111.9347\n",
      "Epoch 9/10 | Batch 520/2990 | Loss: 104.3693\n",
      "Epoch 9/10 | Batch 540/2990 | Loss: 73.6931\n",
      "Epoch 9/10 | Batch 560/2990 | Loss: 40.5360\n",
      "Epoch 9/10 | Batch 580/2990 | Loss: 45.3048\n",
      "Epoch 9/10 | Batch 600/2990 | Loss: 182.2090\n",
      "Epoch 9/10 | Batch 620/2990 | Loss: 56.6860\n",
      "Epoch 9/10 | Batch 640/2990 | Loss: 98.0375\n",
      "Epoch 9/10 | Batch 660/2990 | Loss: 52.0717\n",
      "Epoch 9/10 | Batch 680/2990 | Loss: 212.6296\n",
      "Epoch 9/10 | Batch 700/2990 | Loss: 24.3673\n",
      "Epoch 9/10 | Batch 720/2990 | Loss: 69.9542\n",
      "Epoch 9/10 | Batch 740/2990 | Loss: 54.6464\n",
      "Epoch 9/10 | Batch 760/2990 | Loss: 114.2301\n",
      "Epoch 9/10 | Batch 780/2990 | Loss: 108.9632\n",
      "Epoch 9/10 | Batch 800/2990 | Loss: 71.2701\n",
      "Epoch 9/10 | Batch 820/2990 | Loss: 43.2160\n",
      "Epoch 9/10 | Batch 840/2990 | Loss: 123.5356\n",
      "Epoch 9/10 | Batch 860/2990 | Loss: 63.9610\n",
      "Epoch 9/10 | Batch 880/2990 | Loss: 216.4627\n",
      "Epoch 9/10 | Batch 900/2990 | Loss: 92.1835\n",
      "Epoch 9/10 | Batch 920/2990 | Loss: 132.7024\n",
      "Epoch 9/10 | Batch 940/2990 | Loss: 36.0171\n",
      "Epoch 9/10 | Batch 960/2990 | Loss: 82.1997\n",
      "Epoch 9/10 | Batch 980/2990 | Loss: 62.9648\n",
      "Epoch 9/10 | Batch 1000/2990 | Loss: 128.0464\n",
      "Epoch 9/10 | Batch 1020/2990 | Loss: 110.7688\n",
      "Epoch 9/10 | Batch 1040/2990 | Loss: 180.1948\n",
      "Epoch 9/10 | Batch 1060/2990 | Loss: 102.3147\n",
      "Epoch 9/10 | Batch 1080/2990 | Loss: 75.2951\n",
      "Epoch 9/10 | Batch 1100/2990 | Loss: 96.6071\n",
      "Epoch 9/10 | Batch 1120/2990 | Loss: 59.7613\n",
      "Epoch 9/10 | Batch 1140/2990 | Loss: 101.0118\n",
      "Epoch 9/10 | Batch 1160/2990 | Loss: 115.4158\n",
      "Epoch 9/10 | Batch 1180/2990 | Loss: 71.1466\n",
      "Epoch 9/10 | Batch 1200/2990 | Loss: 57.8358\n",
      "Epoch 9/10 | Batch 1220/2990 | Loss: 72.3444\n",
      "Epoch 9/10 | Batch 1240/2990 | Loss: 46.5137\n",
      "Epoch 9/10 | Batch 1260/2990 | Loss: 99.0268\n",
      "Epoch 9/10 | Batch 1280/2990 | Loss: 50.4164\n",
      "Epoch 9/10 | Batch 1300/2990 | Loss: 79.6463\n",
      "Epoch 9/10 | Batch 1320/2990 | Loss: 120.0060\n",
      "Epoch 9/10 | Batch 1340/2990 | Loss: 120.8728\n",
      "Epoch 9/10 | Batch 1360/2990 | Loss: 101.3381\n",
      "Epoch 9/10 | Batch 1380/2990 | Loss: 70.6208\n",
      "Epoch 9/10 | Batch 1400/2990 | Loss: 89.3653\n",
      "Epoch 9/10 | Batch 1420/2990 | Loss: 82.0288\n",
      "Epoch 9/10 | Batch 1440/2990 | Loss: 38.5183\n",
      "Epoch 9/10 | Batch 1460/2990 | Loss: 112.8363\n",
      "Epoch 9/10 | Batch 1480/2990 | Loss: 203.2515\n",
      "Epoch 9/10 | Batch 1500/2990 | Loss: 64.7915\n",
      "Epoch 9/10 | Batch 1520/2990 | Loss: 95.8695\n",
      "Epoch 9/10 | Batch 1540/2990 | Loss: 206.2675\n",
      "Epoch 9/10 | Batch 1560/2990 | Loss: 204.3841\n",
      "Epoch 9/10 | Batch 1580/2990 | Loss: 104.1099\n",
      "Epoch 9/10 | Batch 1600/2990 | Loss: 31.8326\n",
      "Epoch 9/10 | Batch 1620/2990 | Loss: 56.8355\n",
      "Epoch 9/10 | Batch 1640/2990 | Loss: 105.0554\n",
      "Epoch 9/10 | Batch 1660/2990 | Loss: 48.6718\n",
      "Epoch 9/10 | Batch 1680/2990 | Loss: 55.2655\n",
      "Epoch 9/10 | Batch 1700/2990 | Loss: 92.5098\n",
      "Epoch 9/10 | Batch 1720/2990 | Loss: 89.6995\n",
      "Epoch 9/10 | Batch 1740/2990 | Loss: 109.0537\n",
      "Epoch 9/10 | Batch 1760/2990 | Loss: 40.2860\n",
      "Epoch 9/10 | Batch 1780/2990 | Loss: 129.3312\n",
      "Epoch 9/10 | Batch 1800/2990 | Loss: 112.4124\n",
      "Epoch 9/10 | Batch 1820/2990 | Loss: 85.1689\n",
      "Epoch 9/10 | Batch 1840/2990 | Loss: 30.0285\n",
      "Epoch 9/10 | Batch 1860/2990 | Loss: 157.9027\n",
      "Epoch 9/10 | Batch 1880/2990 | Loss: 103.9105\n",
      "Epoch 9/10 | Batch 1900/2990 | Loss: 77.4807\n",
      "Epoch 9/10 | Batch 1920/2990 | Loss: 81.6035\n",
      "Epoch 9/10 | Batch 1940/2990 | Loss: 62.4092\n",
      "Epoch 9/10 | Batch 1960/2990 | Loss: 116.2057\n",
      "Epoch 9/10 | Batch 1980/2990 | Loss: 42.5859\n",
      "Epoch 9/10 | Batch 2000/2990 | Loss: 40.5094\n",
      "Epoch 9/10 | Batch 2020/2990 | Loss: 74.9491\n",
      "Epoch 9/10 | Batch 2040/2990 | Loss: 114.0216\n",
      "Epoch 9/10 | Batch 2060/2990 | Loss: 43.7970\n",
      "Epoch 9/10 | Batch 2080/2990 | Loss: 58.2992\n",
      "Epoch 9/10 | Batch 2100/2990 | Loss: 74.7488\n",
      "Epoch 9/10 | Batch 2120/2990 | Loss: 29.7577\n",
      "Epoch 9/10 | Batch 2140/2990 | Loss: 233.5204\n",
      "Epoch 9/10 | Batch 2160/2990 | Loss: 154.6556\n",
      "Epoch 9/10 | Batch 2180/2990 | Loss: 230.7809\n",
      "Epoch 9/10 | Batch 2200/2990 | Loss: 58.7695\n",
      "Epoch 9/10 | Batch 2220/2990 | Loss: 189.6858\n",
      "Epoch 9/10 | Batch 2240/2990 | Loss: 79.3144\n",
      "Epoch 9/10 | Batch 2260/2990 | Loss: 87.0863\n",
      "Epoch 9/10 | Batch 2280/2990 | Loss: 255.2425\n",
      "Epoch 9/10 | Batch 2300/2990 | Loss: 88.9480\n",
      "Epoch 9/10 | Batch 2320/2990 | Loss: 178.2201\n",
      "Epoch 9/10 | Batch 2340/2990 | Loss: 166.4562\n",
      "Epoch 9/10 | Batch 2360/2990 | Loss: 144.3442\n",
      "Epoch 9/10 | Batch 2380/2990 | Loss: 123.1556\n",
      "Epoch 9/10 | Batch 2400/2990 | Loss: 221.7062\n",
      "Epoch 9/10 | Batch 2420/2990 | Loss: 878.3318\n",
      "Epoch 9/10 | Batch 2440/2990 | Loss: 153.3417\n",
      "Epoch 9/10 | Batch 2460/2990 | Loss: 46.6456\n",
      "Epoch 9/10 | Batch 2480/2990 | Loss: 84.8738\n",
      "Epoch 9/10 | Batch 2500/2990 | Loss: 181.7188\n",
      "Epoch 9/10 | Batch 2520/2990 | Loss: 35.4805\n",
      "Epoch 9/10 | Batch 2540/2990 | Loss: 87.6984\n",
      "Epoch 9/10 | Batch 2560/2990 | Loss: 101.6440\n",
      "Epoch 9/10 | Batch 2580/2990 | Loss: 61.0926\n",
      "Epoch 9/10 | Batch 2600/2990 | Loss: 46.3085\n",
      "Epoch 9/10 | Batch 2620/2990 | Loss: 146.5161\n",
      "Epoch 9/10 | Batch 2640/2990 | Loss: 161.9455\n",
      "Epoch 9/10 | Batch 2660/2990 | Loss: 70.5057\n",
      "Epoch 9/10 | Batch 2680/2990 | Loss: 100.7875\n",
      "Epoch 9/10 | Batch 2700/2990 | Loss: 64.3467\n",
      "Epoch 9/10 | Batch 2720/2990 | Loss: 123.9494\n",
      "Epoch 9/10 | Batch 2740/2990 | Loss: 77.9369\n",
      "Epoch 9/10 | Batch 2760/2990 | Loss: 107.1069\n",
      "Epoch 9/10 | Batch 2780/2990 | Loss: 71.3375\n",
      "Epoch 9/10 | Batch 2800/2990 | Loss: 61.5882\n",
      "Epoch 9/10 | Batch 2820/2990 | Loss: 83.6404\n",
      "Epoch 9/10 | Batch 2840/2990 | Loss: 95.9704\n",
      "Epoch 9/10 | Batch 2860/2990 | Loss: 130.9483\n",
      "Epoch 9/10 | Batch 2880/2990 | Loss: 154.4001\n",
      "Epoch 9/10 | Batch 2900/2990 | Loss: 173.2144\n",
      "Epoch 9/10 | Batch 2920/2990 | Loss: 38.3388\n",
      "Epoch 9/10 | Batch 2940/2990 | Loss: 80.1676\n",
      "Epoch 9/10 | Batch 2960/2990 | Loss: 149.9004\n",
      "Epoch 9/10 | Batch 2980/2990 | Loss: 144.8302\n",
      "✅ Epoch 9/10 | Train Loss: 106.0422 | Val Loss: 106.9179\n",
      "Epoch 10/10 | Batch 0/2990 | Loss: 90.4238\n",
      "Epoch 10/10 | Batch 20/2990 | Loss: 48.2309\n",
      "Epoch 10/10 | Batch 40/2990 | Loss: 44.5815\n",
      "Epoch 10/10 | Batch 60/2990 | Loss: 156.3540\n",
      "Epoch 10/10 | Batch 80/2990 | Loss: 139.8547\n",
      "Epoch 10/10 | Batch 100/2990 | Loss: 90.3425\n",
      "Epoch 10/10 | Batch 120/2990 | Loss: 135.1388\n",
      "Epoch 10/10 | Batch 140/2990 | Loss: 48.0922\n",
      "Epoch 10/10 | Batch 160/2990 | Loss: 78.6388\n",
      "Epoch 10/10 | Batch 180/2990 | Loss: 44.2801\n",
      "Epoch 10/10 | Batch 200/2990 | Loss: 76.1028\n",
      "Epoch 10/10 | Batch 220/2990 | Loss: 74.9973\n",
      "Epoch 10/10 | Batch 240/2990 | Loss: 56.1740\n",
      "Epoch 10/10 | Batch 260/2990 | Loss: 57.2546\n",
      "Epoch 10/10 | Batch 280/2990 | Loss: 50.4827\n",
      "Epoch 10/10 | Batch 300/2990 | Loss: 157.6995\n",
      "Epoch 10/10 | Batch 320/2990 | Loss: 91.3421\n",
      "Epoch 10/10 | Batch 340/2990 | Loss: 85.0574\n",
      "Epoch 10/10 | Batch 360/2990 | Loss: 59.5451\n",
      "Epoch 10/10 | Batch 380/2990 | Loss: 123.3758\n",
      "Epoch 10/10 | Batch 400/2990 | Loss: 35.0086\n",
      "Epoch 10/10 | Batch 420/2990 | Loss: 69.5524\n",
      "Epoch 10/10 | Batch 440/2990 | Loss: 297.4310\n",
      "Epoch 10/10 | Batch 460/2990 | Loss: 90.3491\n",
      "Epoch 10/10 | Batch 480/2990 | Loss: 90.9489\n",
      "Epoch 10/10 | Batch 500/2990 | Loss: 60.7555\n",
      "Epoch 10/10 | Batch 520/2990 | Loss: 153.5020\n",
      "Epoch 10/10 | Batch 540/2990 | Loss: 55.6840\n",
      "Epoch 10/10 | Batch 560/2990 | Loss: 61.1034\n",
      "Epoch 10/10 | Batch 580/2990 | Loss: 94.6066\n",
      "Epoch 10/10 | Batch 600/2990 | Loss: 46.1504\n",
      "Epoch 10/10 | Batch 620/2990 | Loss: 106.9551\n",
      "Epoch 10/10 | Batch 640/2990 | Loss: 84.7643\n",
      "Epoch 10/10 | Batch 660/2990 | Loss: 59.2903\n",
      "Epoch 10/10 | Batch 680/2990 | Loss: 67.5812\n",
      "Epoch 10/10 | Batch 700/2990 | Loss: 96.3414\n",
      "Epoch 10/10 | Batch 720/2990 | Loss: 145.2841\n",
      "Epoch 10/10 | Batch 740/2990 | Loss: 110.0452\n",
      "Epoch 10/10 | Batch 760/2990 | Loss: 58.2112\n",
      "Epoch 10/10 | Batch 780/2990 | Loss: 60.7164\n",
      "Epoch 10/10 | Batch 800/2990 | Loss: 53.0729\n",
      "Epoch 10/10 | Batch 820/2990 | Loss: 144.6749\n",
      "Epoch 10/10 | Batch 840/2990 | Loss: 232.8579\n",
      "Epoch 10/10 | Batch 860/2990 | Loss: 41.8996\n",
      "Epoch 10/10 | Batch 880/2990 | Loss: 65.1879\n",
      "Epoch 10/10 | Batch 900/2990 | Loss: 89.7929\n",
      "Epoch 10/10 | Batch 920/2990 | Loss: 48.1043\n",
      "Epoch 10/10 | Batch 940/2990 | Loss: 63.1579\n",
      "Epoch 10/10 | Batch 960/2990 | Loss: 43.6537\n",
      "Epoch 10/10 | Batch 980/2990 | Loss: 63.1617\n",
      "Epoch 10/10 | Batch 1000/2990 | Loss: 77.6790\n",
      "Epoch 10/10 | Batch 1020/2990 | Loss: 96.6255\n",
      "Epoch 10/10 | Batch 1040/2990 | Loss: 105.5686\n",
      "Epoch 10/10 | Batch 1060/2990 | Loss: 38.5664\n",
      "Epoch 10/10 | Batch 1080/2990 | Loss: 27.5679\n",
      "Epoch 10/10 | Batch 1100/2990 | Loss: 54.6172\n",
      "Epoch 10/10 | Batch 1120/2990 | Loss: 43.2364\n",
      "Epoch 10/10 | Batch 1140/2990 | Loss: 60.0034\n",
      "Epoch 10/10 | Batch 1160/2990 | Loss: 84.6962\n",
      "Epoch 10/10 | Batch 1180/2990 | Loss: 56.0070\n",
      "Epoch 10/10 | Batch 1200/2990 | Loss: 79.5285\n",
      "Epoch 10/10 | Batch 1220/2990 | Loss: 68.5953\n",
      "Epoch 10/10 | Batch 1240/2990 | Loss: 98.3775\n",
      "Epoch 10/10 | Batch 1260/2990 | Loss: 53.7786\n",
      "Epoch 10/10 | Batch 1280/2990 | Loss: 163.1348\n",
      "Epoch 10/10 | Batch 1300/2990 | Loss: 76.8363\n",
      "Epoch 10/10 | Batch 1320/2990 | Loss: 68.7908\n",
      "Epoch 10/10 | Batch 1340/2990 | Loss: 97.4151\n",
      "Epoch 10/10 | Batch 1360/2990 | Loss: 74.6540\n",
      "Epoch 10/10 | Batch 1380/2990 | Loss: 76.3179\n",
      "Epoch 10/10 | Batch 1400/2990 | Loss: 149.7504\n",
      "Epoch 10/10 | Batch 1420/2990 | Loss: 58.6205\n",
      "Epoch 10/10 | Batch 1440/2990 | Loss: 37.3771\n",
      "Epoch 10/10 | Batch 1460/2990 | Loss: 92.4342\n",
      "Epoch 10/10 | Batch 1480/2990 | Loss: 76.7357\n",
      "Epoch 10/10 | Batch 1500/2990 | Loss: 22.0892\n",
      "Epoch 10/10 | Batch 1520/2990 | Loss: 174.0700\n",
      "Epoch 10/10 | Batch 1540/2990 | Loss: 49.9531\n",
      "Epoch 10/10 | Batch 1560/2990 | Loss: 55.1298\n",
      "Epoch 10/10 | Batch 1580/2990 | Loss: 56.2867\n",
      "Epoch 10/10 | Batch 1600/2990 | Loss: 137.0313\n",
      "Epoch 10/10 | Batch 1620/2990 | Loss: 62.9633\n",
      "Epoch 10/10 | Batch 1640/2990 | Loss: 40.6371\n",
      "Epoch 10/10 | Batch 1660/2990 | Loss: 30.9021\n",
      "Epoch 10/10 | Batch 1680/2990 | Loss: 48.0266\n",
      "Epoch 10/10 | Batch 1700/2990 | Loss: 74.2428\n",
      "Epoch 10/10 | Batch 1720/2990 | Loss: 70.5526\n",
      "Epoch 10/10 | Batch 1740/2990 | Loss: 48.1109\n",
      "Epoch 10/10 | Batch 1760/2990 | Loss: 52.3597\n",
      "Epoch 10/10 | Batch 1780/2990 | Loss: 95.0199\n",
      "Epoch 10/10 | Batch 1800/2990 | Loss: 55.2626\n",
      "Epoch 10/10 | Batch 1820/2990 | Loss: 91.2553\n",
      "Epoch 10/10 | Batch 1840/2990 | Loss: 120.4495\n",
      "Epoch 10/10 | Batch 1860/2990 | Loss: 47.2452\n",
      "Epoch 10/10 | Batch 1880/2990 | Loss: 45.2209\n",
      "Epoch 10/10 | Batch 1900/2990 | Loss: 62.5562\n",
      "Epoch 10/10 | Batch 1920/2990 | Loss: 103.7660\n",
      "Epoch 10/10 | Batch 1940/2990 | Loss: 34.0827\n",
      "Epoch 10/10 | Batch 1960/2990 | Loss: 46.6895\n",
      "Epoch 10/10 | Batch 1980/2990 | Loss: 77.1182\n",
      "Epoch 10/10 | Batch 2000/2990 | Loss: 99.7037\n",
      "Epoch 10/10 | Batch 2020/2990 | Loss: 153.5807\n",
      "Epoch 10/10 | Batch 2040/2990 | Loss: 263.2432\n",
      "Epoch 10/10 | Batch 2060/2990 | Loss: 94.9916\n",
      "Epoch 10/10 | Batch 2080/2990 | Loss: 70.6999\n",
      "Epoch 10/10 | Batch 2100/2990 | Loss: 128.0109\n",
      "Epoch 10/10 | Batch 2120/2990 | Loss: 110.3887\n",
      "Epoch 10/10 | Batch 2140/2990 | Loss: 146.2320\n",
      "Epoch 10/10 | Batch 2160/2990 | Loss: 117.6476\n",
      "Epoch 10/10 | Batch 2180/2990 | Loss: 86.7988\n",
      "Epoch 10/10 | Batch 2200/2990 | Loss: 101.4232\n",
      "Epoch 10/10 | Batch 2220/2990 | Loss: 57.4633\n",
      "Epoch 10/10 | Batch 2240/2990 | Loss: 117.1904\n",
      "Epoch 10/10 | Batch 2260/2990 | Loss: 44.2065\n",
      "Epoch 10/10 | Batch 2280/2990 | Loss: 116.2827\n",
      "Epoch 10/10 | Batch 2300/2990 | Loss: 68.2718\n",
      "Epoch 10/10 | Batch 2320/2990 | Loss: 62.0926\n",
      "Epoch 10/10 | Batch 2340/2990 | Loss: 79.7389\n",
      "Epoch 10/10 | Batch 2360/2990 | Loss: 28.7848\n",
      "Epoch 10/10 | Batch 2380/2990 | Loss: 121.7692\n",
      "Epoch 10/10 | Batch 2400/2990 | Loss: 1068.9760\n",
      "Epoch 10/10 | Batch 2420/2990 | Loss: 71.6671\n",
      "Epoch 10/10 | Batch 2440/2990 | Loss: 168.3960\n",
      "Epoch 10/10 | Batch 2460/2990 | Loss: 87.7988\n",
      "Epoch 10/10 | Batch 2480/2990 | Loss: 74.1582\n",
      "Epoch 10/10 | Batch 2500/2990 | Loss: 60.3850\n",
      "Epoch 10/10 | Batch 2520/2990 | Loss: 76.8755\n",
      "Epoch 10/10 | Batch 2540/2990 | Loss: 100.2412\n",
      "Epoch 10/10 | Batch 2560/2990 | Loss: 107.3968\n",
      "Epoch 10/10 | Batch 2580/2990 | Loss: 115.8550\n",
      "Epoch 10/10 | Batch 2600/2990 | Loss: 38.6009\n",
      "Epoch 10/10 | Batch 2620/2990 | Loss: 61.1846\n",
      "Epoch 10/10 | Batch 2640/2990 | Loss: 73.9471\n",
      "Epoch 10/10 | Batch 2660/2990 | Loss: 81.9763\n",
      "Epoch 10/10 | Batch 2680/2990 | Loss: 124.7917\n",
      "Epoch 10/10 | Batch 2700/2990 | Loss: 105.9666\n",
      "Epoch 10/10 | Batch 2720/2990 | Loss: 65.7449\n",
      "Epoch 10/10 | Batch 2740/2990 | Loss: 123.7072\n",
      "Epoch 10/10 | Batch 2760/2990 | Loss: 126.4617\n",
      "Epoch 10/10 | Batch 2780/2990 | Loss: 70.8621\n",
      "Epoch 10/10 | Batch 2800/2990 | Loss: 44.9379\n",
      "Epoch 10/10 | Batch 2820/2990 | Loss: 10.9111\n",
      "Epoch 10/10 | Batch 2840/2990 | Loss: 49.7086\n",
      "Epoch 10/10 | Batch 2860/2990 | Loss: 229.2659\n",
      "Epoch 10/10 | Batch 2880/2990 | Loss: 113.9006\n",
      "Epoch 10/10 | Batch 2900/2990 | Loss: 92.9619\n",
      "Epoch 10/10 | Batch 2920/2990 | Loss: 108.6898\n",
      "Epoch 10/10 | Batch 2940/2990 | Loss: 213.0220\n",
      "Epoch 10/10 | Batch 2960/2990 | Loss: 87.2541\n",
      "Epoch 10/10 | Batch 2980/2990 | Loss: 32.8409\n",
      "✅ Epoch 10/10 | Train Loss: 85.8659 | Val Loss: 90.4951\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
