{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))    \n",
    "from SwinV2Model import SwinV2WheatModel\n",
    "from dataLoaderFunc import loadSplitData, createLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device='cuda'):\n",
    "    best_val_loss = float('inf')\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch = rgb_batch.to(device)\n",
    "            dsm_batch = dsm_batch.to(device)\n",
    "            label_batch = label_batch.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "            \n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # üîç Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch = rgb_batch.to(device)\n",
    "                dsm_batch = dsm_batch.to(device)\n",
    "                label_batch = label_batch.to(device).float().unsqueeze(1)\n",
    "\n",
    "                outputs = model(rgb_batch, dsm_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"üìò Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ‚úÖ Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_swinv2_model.pt\")\n",
    "            print(\"‚úÖ Best model saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚úÖ Initialize Model, Loss Function, and Optimizer\n",
    "\n",
    "# ‚úÖ Universal device selection (works on both Mac M2 & Windows with RTX 4060)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ‚úÖ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ‚úÖ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ‚úÖ Default to CPU if no GPU is available\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "model = SwinV2WheatModel()\n",
    "criterion = nn.MSELoss()  # or nn.L1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5, verbose=True)\n",
    "# ‚úÖ Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([16, 1, 1])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 0/2990 | Loss: 129557.3594\n",
      "Epoch 1/10 | Batch 20/2990 | Loss: 11112.9150\n",
      "Epoch 1/10 | Batch 40/2990 | Loss: 5883.5342\n",
      "Epoch 1/10 | Batch 60/2990 | Loss: 7552.8281\n",
      "Epoch 1/10 | Batch 80/2990 | Loss: 7195.6333\n",
      "Epoch 1/10 | Batch 100/2990 | Loss: 10774.9424\n",
      "Epoch 1/10 | Batch 120/2990 | Loss: 11910.5000\n",
      "Epoch 1/10 | Batch 140/2990 | Loss: 10771.8857\n",
      "Epoch 1/10 | Batch 160/2990 | Loss: 9445.3906\n",
      "Epoch 1/10 | Batch 180/2990 | Loss: 10448.7129\n",
      "Epoch 1/10 | Batch 200/2990 | Loss: 7728.2778\n",
      "Epoch 1/10 | Batch 220/2990 | Loss: 3962.3398\n",
      "Epoch 1/10 | Batch 240/2990 | Loss: 3939.9558\n",
      "Epoch 1/10 | Batch 260/2990 | Loss: 6206.5386\n",
      "Epoch 1/10 | Batch 280/2990 | Loss: 9219.5371\n",
      "Epoch 1/10 | Batch 300/2990 | Loss: 8489.2598\n",
      "Epoch 1/10 | Batch 320/2990 | Loss: 9159.8467\n",
      "Epoch 1/10 | Batch 340/2990 | Loss: 5852.9507\n",
      "Epoch 1/10 | Batch 360/2990 | Loss: 11035.4609\n",
      "Epoch 1/10 | Batch 380/2990 | Loss: 11080.6660\n",
      "Epoch 1/10 | Batch 400/2990 | Loss: 7120.9297\n",
      "Epoch 1/10 | Batch 420/2990 | Loss: 15010.6465\n",
      "Epoch 1/10 | Batch 440/2990 | Loss: 10075.5508\n",
      "Epoch 1/10 | Batch 460/2990 | Loss: 7480.5278\n",
      "Epoch 1/10 | Batch 480/2990 | Loss: 12053.7393\n",
      "Epoch 1/10 | Batch 500/2990 | Loss: 8708.1494\n",
      "Epoch 1/10 | Batch 520/2990 | Loss: 9994.0742\n",
      "Epoch 1/10 | Batch 540/2990 | Loss: 7558.6885\n",
      "Epoch 1/10 | Batch 560/2990 | Loss: 11749.4531\n",
      "Epoch 1/10 | Batch 580/2990 | Loss: 11241.9238\n",
      "Epoch 1/10 | Batch 600/2990 | Loss: 11088.7861\n",
      "Epoch 1/10 | Batch 620/2990 | Loss: 7632.7642\n",
      "Epoch 1/10 | Batch 640/2990 | Loss: 5598.8320\n",
      "Epoch 1/10 | Batch 660/2990 | Loss: 13961.0859\n",
      "Epoch 1/10 | Batch 680/2990 | Loss: 17638.6055\n",
      "Epoch 1/10 | Batch 700/2990 | Loss: 8797.7266\n",
      "Epoch 1/10 | Batch 720/2990 | Loss: 7973.9551\n",
      "Epoch 1/10 | Batch 740/2990 | Loss: 13634.6875\n",
      "Epoch 1/10 | Batch 760/2990 | Loss: 9807.2715\n",
      "Epoch 1/10 | Batch 780/2990 | Loss: 6823.2158\n",
      "Epoch 1/10 | Batch 800/2990 | Loss: 13260.6855\n",
      "Epoch 1/10 | Batch 820/2990 | Loss: 9587.2881\n",
      "Epoch 1/10 | Batch 840/2990 | Loss: 9373.4102\n",
      "Epoch 1/10 | Batch 860/2990 | Loss: 7655.3569\n",
      "Epoch 1/10 | Batch 880/2990 | Loss: 5952.1885\n",
      "Epoch 1/10 | Batch 900/2990 | Loss: 3992.5156\n",
      "Epoch 1/10 | Batch 920/2990 | Loss: 13065.1426\n",
      "Epoch 1/10 | Batch 940/2990 | Loss: 11458.7021\n",
      "Epoch 1/10 | Batch 960/2990 | Loss: 15326.5342\n",
      "Epoch 1/10 | Batch 980/2990 | Loss: 5413.3130\n",
      "Epoch 1/10 | Batch 1000/2990 | Loss: 8417.6709\n",
      "Epoch 1/10 | Batch 1020/2990 | Loss: 10492.3926\n",
      "Epoch 1/10 | Batch 1040/2990 | Loss: 15189.3281\n",
      "Epoch 1/10 | Batch 1060/2990 | Loss: 11369.9043\n",
      "Epoch 1/10 | Batch 1080/2990 | Loss: 8479.7197\n",
      "Epoch 1/10 | Batch 1100/2990 | Loss: 9984.6426\n",
      "Epoch 1/10 | Batch 1120/2990 | Loss: 3700.9390\n",
      "Epoch 1/10 | Batch 1140/2990 | Loss: 3896.4199\n",
      "Epoch 1/10 | Batch 1160/2990 | Loss: 11954.6055\n",
      "Epoch 1/10 | Batch 1180/2990 | Loss: 11789.9248\n",
      "Epoch 1/10 | Batch 1200/2990 | Loss: 12811.6504\n",
      "Epoch 1/10 | Batch 1220/2990 | Loss: 7922.4697\n",
      "Epoch 1/10 | Batch 1240/2990 | Loss: 8248.0312\n",
      "Epoch 1/10 | Batch 1260/2990 | Loss: 10518.8301\n",
      "Epoch 1/10 | Batch 1280/2990 | Loss: 15052.3018\n",
      "Epoch 1/10 | Batch 1300/2990 | Loss: 19945.3066\n",
      "Epoch 1/10 | Batch 1320/2990 | Loss: 6650.4775\n",
      "Epoch 1/10 | Batch 1340/2990 | Loss: 10081.2012\n",
      "Epoch 1/10 | Batch 1360/2990 | Loss: 5236.6963\n",
      "Epoch 1/10 | Batch 1380/2990 | Loss: 11196.2744\n",
      "Epoch 1/10 | Batch 1400/2990 | Loss: 5394.1484\n",
      "Epoch 1/10 | Batch 1420/2990 | Loss: 6594.8267\n",
      "Epoch 1/10 | Batch 1440/2990 | Loss: 8157.2451\n",
      "Epoch 1/10 | Batch 1460/2990 | Loss: 5511.4395\n",
      "Epoch 1/10 | Batch 1480/2990 | Loss: 5731.8896\n",
      "Epoch 1/10 | Batch 1500/2990 | Loss: 6284.2388\n",
      "Epoch 1/10 | Batch 1520/2990 | Loss: 4575.2100\n",
      "Epoch 1/10 | Batch 1540/2990 | Loss: 5997.9600\n",
      "Epoch 1/10 | Batch 1560/2990 | Loss: 12310.9062\n",
      "Epoch 1/10 | Batch 1580/2990 | Loss: 9951.5732\n",
      "Epoch 1/10 | Batch 1600/2990 | Loss: 9362.8799\n",
      "Epoch 1/10 | Batch 1620/2990 | Loss: 6032.0737\n",
      "Epoch 1/10 | Batch 1640/2990 | Loss: 9421.1992\n",
      "Epoch 1/10 | Batch 1660/2990 | Loss: 12973.3213\n",
      "Epoch 1/10 | Batch 1680/2990 | Loss: 8780.3418\n",
      "Epoch 1/10 | Batch 1700/2990 | Loss: 10508.0156\n",
      "Epoch 1/10 | Batch 1720/2990 | Loss: 7079.7041\n",
      "Epoch 1/10 | Batch 1740/2990 | Loss: 9692.7197\n",
      "Epoch 1/10 | Batch 1760/2990 | Loss: 12446.2607\n",
      "Epoch 1/10 | Batch 1780/2990 | Loss: 13338.7031\n",
      "Epoch 1/10 | Batch 1800/2990 | Loss: 7278.2012\n",
      "Epoch 1/10 | Batch 1820/2990 | Loss: 7160.1055\n",
      "Epoch 1/10 | Batch 1840/2990 | Loss: 9665.3467\n",
      "Epoch 1/10 | Batch 1860/2990 | Loss: 2642.0737\n",
      "Epoch 1/10 | Batch 1880/2990 | Loss: 9975.0225\n",
      "Epoch 1/10 | Batch 1900/2990 | Loss: 11737.1660\n",
      "Epoch 1/10 | Batch 1920/2990 | Loss: 8122.4536\n",
      "Epoch 1/10 | Batch 1940/2990 | Loss: 4717.4946\n",
      "Epoch 1/10 | Batch 1960/2990 | Loss: 7062.2500\n",
      "Epoch 1/10 | Batch 1980/2990 | Loss: 8734.0596\n",
      "Epoch 1/10 | Batch 2000/2990 | Loss: 7286.2422\n",
      "Epoch 1/10 | Batch 2020/2990 | Loss: 15505.6553\n",
      "Epoch 1/10 | Batch 2040/2990 | Loss: 10506.6973\n",
      "Epoch 1/10 | Batch 2060/2990 | Loss: 12638.4277\n",
      "Epoch 1/10 | Batch 2080/2990 | Loss: 6117.0830\n",
      "Epoch 1/10 | Batch 2100/2990 | Loss: 7980.0439\n",
      "Epoch 1/10 | Batch 2120/2990 | Loss: 4804.4985\n",
      "Epoch 1/10 | Batch 2140/2990 | Loss: 12469.0850\n",
      "Epoch 1/10 | Batch 2160/2990 | Loss: 8913.7012\n",
      "Epoch 1/10 | Batch 2180/2990 | Loss: 6224.9976\n",
      "Epoch 1/10 | Batch 2200/2990 | Loss: 17060.7773\n",
      "Epoch 1/10 | Batch 2220/2990 | Loss: 10283.6152\n",
      "Epoch 1/10 | Batch 2240/2990 | Loss: 6790.2588\n",
      "Epoch 1/10 | Batch 2260/2990 | Loss: 7030.4946\n",
      "Epoch 1/10 | Batch 2280/2990 | Loss: 8898.8877\n",
      "Epoch 1/10 | Batch 2300/2990 | Loss: 10115.5430\n",
      "Epoch 1/10 | Batch 2320/2990 | Loss: 10587.3643\n",
      "Epoch 1/10 | Batch 2340/2990 | Loss: 11306.3496\n",
      "Epoch 1/10 | Batch 2360/2990 | Loss: 2777.9375\n",
      "Epoch 1/10 | Batch 2380/2990 | Loss: 14318.8926\n",
      "Epoch 1/10 | Batch 2400/2990 | Loss: 9828.5205\n",
      "Epoch 1/10 | Batch 2420/2990 | Loss: 5217.4683\n",
      "Epoch 1/10 | Batch 2440/2990 | Loss: 9615.0420\n",
      "Epoch 1/10 | Batch 2460/2990 | Loss: 10281.1992\n",
      "Epoch 1/10 | Batch 2480/2990 | Loss: 12413.4619\n",
      "Epoch 1/10 | Batch 2500/2990 | Loss: 4655.2334\n",
      "Epoch 1/10 | Batch 2520/2990 | Loss: 8868.7109\n",
      "Epoch 1/10 | Batch 2540/2990 | Loss: 14532.0547\n",
      "Epoch 1/10 | Batch 2560/2990 | Loss: 7703.4297\n",
      "Epoch 1/10 | Batch 2580/2990 | Loss: 13194.8242\n",
      "Epoch 1/10 | Batch 2600/2990 | Loss: 10716.9268\n",
      "Epoch 1/10 | Batch 2620/2990 | Loss: 13136.7070\n",
      "Epoch 1/10 | Batch 2640/2990 | Loss: 3575.7590\n",
      "Epoch 1/10 | Batch 2660/2990 | Loss: 14262.2139\n",
      "Epoch 1/10 | Batch 2680/2990 | Loss: 13791.5215\n",
      "Epoch 1/10 | Batch 2700/2990 | Loss: 8392.7480\n",
      "Epoch 1/10 | Batch 2720/2990 | Loss: 12367.1768\n",
      "Epoch 1/10 | Batch 2740/2990 | Loss: 11375.7031\n",
      "Epoch 1/10 | Batch 2760/2990 | Loss: 5946.8027\n",
      "Epoch 1/10 | Batch 2780/2990 | Loss: 14414.4355\n",
      "Epoch 1/10 | Batch 2800/2990 | Loss: 17150.2793\n",
      "Epoch 1/10 | Batch 2820/2990 | Loss: 7176.6973\n",
      "Epoch 1/10 | Batch 2840/2990 | Loss: 12101.2832\n",
      "Epoch 1/10 | Batch 2860/2990 | Loss: 8569.7666\n",
      "Epoch 1/10 | Batch 2880/2990 | Loss: 9613.8584\n",
      "Epoch 1/10 | Batch 2900/2990 | Loss: 14005.7891\n",
      "Epoch 1/10 | Batch 2920/2990 | Loss: 6951.2573\n",
      "Epoch 1/10 | Batch 2940/2990 | Loss: 14155.9238\n",
      "Epoch 1/10 | Batch 2960/2990 | Loss: 10263.2520\n",
      "Epoch 1/10 | Batch 2980/2990 | Loss: 7894.0176\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.step() missing 1 required positional argument: 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device)\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx % \u001b[32m20\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     22\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m avg_train_loss = running_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# üîç Validation\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.step() missing 1 required positional argument: 'metrics'"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
