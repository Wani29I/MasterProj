{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTModel\n",
    "\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from VisionTransformerModel import ViTWheatModel\n",
    "from dataLoaderFunc import loadSplitData, createLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Compute Validation Loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                outputs = model(rgb_batch, dsm_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save Best Model\n",
    "        torch.save(model.state_dict(), \"vit_wheat_model.pth\")\n",
    "\n",
    "# ✅ Test the model on validation set\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb_batch, dsm_batch, label_batch in test_loader:\n",
    "            rgb_batch, dsm_batch = rgb_batch.to(device), dsm_batch.to(device)\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            actuals.extend(label_batch.cpu().numpy().flatten())\n",
    "\n",
    "    return predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ✅ Initialize Model, Loss Function, and Optimizer\n",
    "# ✅ Universal device selection (works on both Mac M2 & Windows with RTX 4060)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ✅ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ✅ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ✅ Default to CPU if no GPU is available\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "model = ViTWheatModel().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 0/2990 | Loss: 101725.0469\n",
      "Epoch 1/10 | Batch 100/2990 | Loss: 24245.3613\n",
      "Epoch 1/10 | Batch 200/2990 | Loss: 5811.1489\n",
      "Epoch 1/10 | Batch 300/2990 | Loss: 6865.1572\n",
      "Epoch 1/10 | Batch 400/2990 | Loss: 7162.7280\n",
      "Epoch 1/10 | Batch 500/2990 | Loss: 6818.8828\n",
      "Epoch 1/10 | Batch 600/2990 | Loss: 4650.8857\n",
      "Epoch 1/10 | Batch 700/2990 | Loss: 8009.5762\n",
      "Epoch 1/10 | Batch 800/2990 | Loss: 14069.7695\n",
      "Epoch 1/10 | Batch 900/2990 | Loss: 8990.9277\n",
      "Epoch 1/10 | Batch 1000/2990 | Loss: 9682.3545\n",
      "Epoch 1/10 | Batch 1100/2990 | Loss: 11566.8027\n",
      "Epoch 1/10 | Batch 1200/2990 | Loss: 7959.4253\n",
      "Epoch 1/10 | Batch 1300/2990 | Loss: 4316.6172\n",
      "Epoch 1/10 | Batch 1400/2990 | Loss: 21406.9707\n",
      "Epoch 1/10 | Batch 1500/2990 | Loss: 12259.5908\n",
      "Epoch 1/10 | Batch 1600/2990 | Loss: 18633.9980\n",
      "Epoch 1/10 | Batch 1700/2990 | Loss: 8475.2109\n",
      "Epoch 1/10 | Batch 1800/2990 | Loss: 8076.8135\n",
      "Epoch 1/10 | Batch 1900/2990 | Loss: 8333.1670\n",
      "Epoch 1/10 | Batch 2000/2990 | Loss: 10083.2979\n",
      "Epoch 1/10 | Batch 2100/2990 | Loss: 4494.8418\n",
      "Epoch 1/10 | Batch 2200/2990 | Loss: 2824.4666\n",
      "Epoch 1/10 | Batch 2300/2990 | Loss: 4894.4966\n",
      "Epoch 1/10 | Batch 2400/2990 | Loss: 7388.3164\n",
      "Epoch 1/10 | Batch 2500/2990 | Loss: 9576.3086\n",
      "Epoch 1/10 | Batch 2600/2990 | Loss: 9422.7793\n",
      "Epoch 1/10 | Batch 2700/2990 | Loss: 10317.0439\n",
      "Epoch 1/10 | Batch 2800/2990 | Loss: 6873.1885\n",
      "Epoch 1/10 | Batch 2900/2990 | Loss: 7832.3174\n",
      "✅ Epoch 1/10 | Train Loss: 10032.3862 | Val Loss: 9968.4297\n",
      "Epoch 2/10 | Batch 0/2990 | Loss: 9694.7578\n",
      "Epoch 2/10 | Batch 100/2990 | Loss: 10604.2129\n",
      "Epoch 2/10 | Batch 200/2990 | Loss: 12285.1523\n",
      "Epoch 2/10 | Batch 300/2990 | Loss: 3997.5464\n",
      "Epoch 2/10 | Batch 400/2990 | Loss: 5388.8286\n",
      "Epoch 2/10 | Batch 500/2990 | Loss: 13548.3691\n",
      "Epoch 2/10 | Batch 600/2990 | Loss: 10447.1309\n",
      "Epoch 2/10 | Batch 700/2990 | Loss: 3848.1968\n",
      "Epoch 2/10 | Batch 800/2990 | Loss: 10082.8809\n",
      "Epoch 2/10 | Batch 900/2990 | Loss: 15484.0947\n",
      "Epoch 2/10 | Batch 1000/2990 | Loss: 6518.0942\n",
      "Epoch 2/10 | Batch 1100/2990 | Loss: 7062.4458\n",
      "Epoch 2/10 | Batch 1200/2990 | Loss: 11124.0117\n",
      "Epoch 2/10 | Batch 1300/2990 | Loss: 6867.9375\n",
      "Epoch 2/10 | Batch 1400/2990 | Loss: 12136.3359\n",
      "Epoch 2/10 | Batch 1500/2990 | Loss: 10163.1699\n",
      "Epoch 2/10 | Batch 1600/2990 | Loss: 3016.7166\n",
      "Epoch 2/10 | Batch 1700/2990 | Loss: 7842.8252\n",
      "Epoch 2/10 | Batch 1800/2990 | Loss: 6888.7114\n",
      "Epoch 2/10 | Batch 1900/2990 | Loss: 8248.4863\n",
      "Epoch 2/10 | Batch 2000/2990 | Loss: 8206.5430\n",
      "Epoch 2/10 | Batch 2100/2990 | Loss: 7575.9478\n",
      "Epoch 2/10 | Batch 2200/2990 | Loss: 15309.4775\n",
      "Epoch 2/10 | Batch 2300/2990 | Loss: 14958.3252\n",
      "Epoch 2/10 | Batch 2400/2990 | Loss: 9177.2012\n",
      "Epoch 2/10 | Batch 2500/2990 | Loss: 8046.4883\n",
      "Epoch 2/10 | Batch 2600/2990 | Loss: 15714.3174\n",
      "Epoch 2/10 | Batch 2700/2990 | Loss: 9800.0049\n",
      "Epoch 2/10 | Batch 2800/2990 | Loss: 10710.7803\n",
      "Epoch 2/10 | Batch 2900/2990 | Loss: 4389.2344\n",
      "✅ Epoch 2/10 | Train Loss: 9123.3314 | Val Loss: 9387.6104\n",
      "Epoch 3/10 | Batch 0/2990 | Loss: 14024.0547\n",
      "Epoch 3/10 | Batch 100/2990 | Loss: 7446.6343\n",
      "Epoch 3/10 | Batch 200/2990 | Loss: 17525.9062\n",
      "Epoch 3/10 | Batch 300/2990 | Loss: 7640.7158\n",
      "Epoch 3/10 | Batch 400/2990 | Loss: 9229.0898\n",
      "Epoch 3/10 | Batch 500/2990 | Loss: 6442.3223\n",
      "Epoch 3/10 | Batch 600/2990 | Loss: 17988.7637\n",
      "Epoch 3/10 | Batch 700/2990 | Loss: 5905.6816\n",
      "Epoch 3/10 | Batch 800/2990 | Loss: 7885.0376\n",
      "Epoch 3/10 | Batch 900/2990 | Loss: 14121.0342\n",
      "Epoch 3/10 | Batch 1000/2990 | Loss: 6292.0444\n",
      "Epoch 3/10 | Batch 1100/2990 | Loss: 11493.9336\n",
      "Epoch 3/10 | Batch 1200/2990 | Loss: 6047.2905\n",
      "Epoch 3/10 | Batch 1300/2990 | Loss: 7448.1157\n",
      "Epoch 3/10 | Batch 1400/2990 | Loss: 10306.2246\n",
      "Epoch 3/10 | Batch 1500/2990 | Loss: 11200.3262\n",
      "Epoch 3/10 | Batch 1600/2990 | Loss: 4031.0808\n",
      "Epoch 3/10 | Batch 1700/2990 | Loss: 11663.2188\n",
      "Epoch 3/10 | Batch 1800/2990 | Loss: 15349.3594\n",
      "Epoch 3/10 | Batch 1900/2990 | Loss: 8321.9492\n",
      "Epoch 3/10 | Batch 2000/2990 | Loss: 7068.7231\n",
      "Epoch 3/10 | Batch 2100/2990 | Loss: 8688.8145\n",
      "Epoch 3/10 | Batch 2200/2990 | Loss: 7546.5840\n",
      "Epoch 3/10 | Batch 2300/2990 | Loss: 7456.1689\n",
      "Epoch 3/10 | Batch 2400/2990 | Loss: 10292.4961\n",
      "Epoch 3/10 | Batch 2500/2990 | Loss: 8386.4453\n",
      "Epoch 3/10 | Batch 2600/2990 | Loss: 6776.1475\n",
      "Epoch 3/10 | Batch 2700/2990 | Loss: 11411.5098\n",
      "Epoch 3/10 | Batch 2800/2990 | Loss: 9120.1270\n",
      "Epoch 3/10 | Batch 2900/2990 | Loss: 3199.6138\n",
      "✅ Epoch 3/10 | Train Loss: 8564.7848 | Val Loss: 7561.7058\n",
      "Epoch 4/10 | Batch 0/2990 | Loss: 15742.1738\n",
      "Epoch 4/10 | Batch 100/2990 | Loss: 6949.6704\n",
      "Epoch 4/10 | Batch 200/2990 | Loss: 7643.6182\n",
      "Epoch 4/10 | Batch 300/2990 | Loss: 4672.1025\n",
      "Epoch 4/10 | Batch 400/2990 | Loss: 3836.9753\n",
      "Epoch 4/10 | Batch 500/2990 | Loss: 4626.0405\n",
      "Epoch 4/10 | Batch 600/2990 | Loss: 1817.5887\n",
      "Epoch 4/10 | Batch 700/2990 | Loss: 6882.6372\n",
      "Epoch 4/10 | Batch 800/2990 | Loss: 6390.3203\n",
      "Epoch 4/10 | Batch 900/2990 | Loss: 4292.8193\n",
      "Epoch 4/10 | Batch 1000/2990 | Loss: 7439.6792\n",
      "Epoch 4/10 | Batch 1100/2990 | Loss: 6015.5811\n",
      "Epoch 4/10 | Batch 1200/2990 | Loss: 7132.7041\n",
      "Epoch 4/10 | Batch 1300/2990 | Loss: 3564.2595\n",
      "Epoch 4/10 | Batch 1400/2990 | Loss: 2866.6997\n",
      "Epoch 4/10 | Batch 1500/2990 | Loss: 5381.0312\n",
      "Epoch 4/10 | Batch 1600/2990 | Loss: 4604.0620\n",
      "Epoch 4/10 | Batch 1700/2990 | Loss: 4555.6709\n",
      "Epoch 4/10 | Batch 1800/2990 | Loss: 4123.6016\n",
      "Epoch 4/10 | Batch 1900/2990 | Loss: 5204.9331\n",
      "Epoch 4/10 | Batch 2000/2990 | Loss: 3846.9211\n",
      "Epoch 4/10 | Batch 2100/2990 | Loss: 3778.4839\n",
      "Epoch 4/10 | Batch 2200/2990 | Loss: 2657.3184\n",
      "Epoch 4/10 | Batch 2300/2990 | Loss: 2545.5085\n",
      "Epoch 4/10 | Batch 2400/2990 | Loss: 8394.2070\n",
      "Epoch 4/10 | Batch 2500/2990 | Loss: 10198.9160\n",
      "Epoch 4/10 | Batch 2600/2990 | Loss: 7023.9375\n",
      "Epoch 4/10 | Batch 2700/2990 | Loss: 6773.9053\n",
      "Epoch 4/10 | Batch 2800/2990 | Loss: 3022.3262\n",
      "Epoch 4/10 | Batch 2900/2990 | Loss: 4924.2529\n",
      "✅ Epoch 4/10 | Train Loss: 5779.3350 | Val Loss: 5679.1582\n",
      "Epoch 5/10 | Batch 0/2990 | Loss: 7226.3394\n",
      "Epoch 5/10 | Batch 100/2990 | Loss: 3850.3555\n",
      "Epoch 5/10 | Batch 200/2990 | Loss: 8180.7656\n",
      "Epoch 5/10 | Batch 300/2990 | Loss: 3987.7070\n",
      "Epoch 5/10 | Batch 400/2990 | Loss: 9161.7676\n",
      "Epoch 5/10 | Batch 500/2990 | Loss: 7363.4111\n",
      "Epoch 5/10 | Batch 600/2990 | Loss: 7869.9673\n",
      "Epoch 5/10 | Batch 700/2990 | Loss: 4278.2837\n",
      "Epoch 5/10 | Batch 800/2990 | Loss: 2837.4819\n",
      "Epoch 5/10 | Batch 900/2990 | Loss: 4719.2266\n",
      "Epoch 5/10 | Batch 1000/2990 | Loss: 3081.3208\n",
      "Epoch 5/10 | Batch 1100/2990 | Loss: 1594.3918\n",
      "Epoch 5/10 | Batch 1200/2990 | Loss: 5786.3306\n",
      "Epoch 5/10 | Batch 1300/2990 | Loss: 9374.0781\n",
      "Epoch 5/10 | Batch 1400/2990 | Loss: 5345.7041\n",
      "Epoch 5/10 | Batch 1500/2990 | Loss: 5379.2661\n",
      "Epoch 5/10 | Batch 1600/2990 | Loss: 6456.0620\n",
      "Epoch 5/10 | Batch 1700/2990 | Loss: 5862.4355\n",
      "Epoch 5/10 | Batch 1800/2990 | Loss: 4856.2563\n",
      "Epoch 5/10 | Batch 1900/2990 | Loss: 5478.1445\n",
      "Epoch 5/10 | Batch 2000/2990 | Loss: 3707.9248\n",
      "Epoch 5/10 | Batch 2100/2990 | Loss: 5226.8848\n",
      "Epoch 5/10 | Batch 2200/2990 | Loss: 7039.6108\n",
      "Epoch 5/10 | Batch 2300/2990 | Loss: 7710.4854\n",
      "Epoch 5/10 | Batch 2400/2990 | Loss: 6670.7847\n",
      "Epoch 5/10 | Batch 2500/2990 | Loss: 5086.2568\n",
      "Epoch 5/10 | Batch 2600/2990 | Loss: 2804.6382\n",
      "Epoch 5/10 | Batch 2700/2990 | Loss: 1936.3159\n",
      "Epoch 5/10 | Batch 2800/2990 | Loss: 3755.3533\n",
      "Epoch 5/10 | Batch 2900/2990 | Loss: 5797.8608\n",
      "✅ Epoch 5/10 | Train Loss: 4707.8385 | Val Loss: 3841.2415\n",
      "Epoch 6/10 | Batch 0/2990 | Loss: 3577.2612\n",
      "Epoch 6/10 | Batch 100/2990 | Loss: 5057.6606\n",
      "Epoch 6/10 | Batch 200/2990 | Loss: 3091.4863\n",
      "Epoch 6/10 | Batch 300/2990 | Loss: 7613.4204\n",
      "Epoch 6/10 | Batch 400/2990 | Loss: 1696.0093\n",
      "Epoch 6/10 | Batch 500/2990 | Loss: 2637.6658\n",
      "Epoch 6/10 | Batch 600/2990 | Loss: 2308.7847\n",
      "Epoch 6/10 | Batch 700/2990 | Loss: 2107.8418\n",
      "Epoch 6/10 | Batch 800/2990 | Loss: 3487.1826\n",
      "Epoch 6/10 | Batch 900/2990 | Loss: 3471.5723\n",
      "Epoch 6/10 | Batch 1000/2990 | Loss: 3414.7603\n",
      "Epoch 6/10 | Batch 1100/2990 | Loss: 4194.9629\n",
      "Epoch 6/10 | Batch 1200/2990 | Loss: 1519.4861\n",
      "Epoch 6/10 | Batch 1300/2990 | Loss: 2962.2832\n",
      "Epoch 6/10 | Batch 1400/2990 | Loss: 4233.3428\n",
      "Epoch 6/10 | Batch 1500/2990 | Loss: 2937.1973\n",
      "Epoch 6/10 | Batch 1600/2990 | Loss: 1637.1598\n",
      "Epoch 6/10 | Batch 1700/2990 | Loss: 1481.4268\n",
      "Epoch 6/10 | Batch 1800/2990 | Loss: 3333.5034\n",
      "Epoch 6/10 | Batch 1900/2990 | Loss: 3612.5679\n",
      "Epoch 6/10 | Batch 2000/2990 | Loss: 2401.0894\n",
      "Epoch 6/10 | Batch 2100/2990 | Loss: 3286.0525\n",
      "Epoch 6/10 | Batch 2200/2990 | Loss: 2569.0762\n",
      "Epoch 6/10 | Batch 2300/2990 | Loss: 2534.0752\n",
      "Epoch 6/10 | Batch 2400/2990 | Loss: 2248.9819\n",
      "Epoch 6/10 | Batch 2500/2990 | Loss: 3321.1562\n",
      "Epoch 6/10 | Batch 2600/2990 | Loss: 1765.9724\n",
      "Epoch 6/10 | Batch 2700/2990 | Loss: 2819.0417\n",
      "Epoch 6/10 | Batch 2800/2990 | Loss: 2440.5232\n",
      "Epoch 6/10 | Batch 2900/2990 | Loss: 1248.8732\n",
      "✅ Epoch 6/10 | Train Loss: 3343.7989 | Val Loss: 3308.9787\n",
      "Epoch 7/10 | Batch 0/2990 | Loss: 1904.8096\n",
      "Epoch 7/10 | Batch 100/2990 | Loss: 2235.1780\n",
      "Epoch 7/10 | Batch 200/2990 | Loss: 3088.1582\n",
      "Epoch 7/10 | Batch 300/2990 | Loss: 2345.6772\n",
      "Epoch 7/10 | Batch 400/2990 | Loss: 1949.2697\n",
      "Epoch 7/10 | Batch 500/2990 | Loss: 2182.0869\n",
      "Epoch 7/10 | Batch 600/2990 | Loss: 2953.1016\n",
      "Epoch 7/10 | Batch 700/2990 | Loss: 3388.7402\n",
      "Epoch 7/10 | Batch 800/2990 | Loss: 1922.8392\n",
      "Epoch 7/10 | Batch 900/2990 | Loss: 3573.9661\n",
      "Epoch 7/10 | Batch 1000/2990 | Loss: 2244.4094\n",
      "Epoch 7/10 | Batch 1100/2990 | Loss: 3182.6165\n",
      "Epoch 7/10 | Batch 1200/2990 | Loss: 3260.5420\n",
      "Epoch 7/10 | Batch 1300/2990 | Loss: 3036.9287\n",
      "Epoch 7/10 | Batch 1400/2990 | Loss: 2314.6479\n",
      "Epoch 7/10 | Batch 1500/2990 | Loss: 2206.8894\n",
      "Epoch 7/10 | Batch 1600/2990 | Loss: 1716.4362\n",
      "Epoch 7/10 | Batch 1700/2990 | Loss: 7006.7754\n",
      "Epoch 7/10 | Batch 1800/2990 | Loss: 2447.0247\n",
      "Epoch 7/10 | Batch 1900/2990 | Loss: 2351.4263\n",
      "Epoch 7/10 | Batch 2000/2990 | Loss: 2151.7925\n",
      "Epoch 7/10 | Batch 2100/2990 | Loss: 1052.9373\n",
      "Epoch 7/10 | Batch 2200/2990 | Loss: 2579.4893\n",
      "Epoch 7/10 | Batch 2300/2990 | Loss: 1706.1311\n",
      "Epoch 7/10 | Batch 2400/2990 | Loss: 3647.5002\n",
      "Epoch 7/10 | Batch 2500/2990 | Loss: 2242.8088\n",
      "Epoch 7/10 | Batch 2600/2990 | Loss: 2214.2205\n",
      "Epoch 7/10 | Batch 2700/2990 | Loss: 1302.0083\n",
      "Epoch 7/10 | Batch 2800/2990 | Loss: 2331.8792\n",
      "Epoch 7/10 | Batch 2900/2990 | Loss: 875.4030\n",
      "✅ Epoch 7/10 | Train Loss: 2689.6679 | Val Loss: 2860.5299\n",
      "Epoch 8/10 | Batch 0/2990 | Loss: 1705.9961\n",
      "Epoch 8/10 | Batch 100/2990 | Loss: 2473.0391\n",
      "Epoch 8/10 | Batch 200/2990 | Loss: 5273.1392\n",
      "Epoch 8/10 | Batch 300/2990 | Loss: 1494.0088\n",
      "Epoch 8/10 | Batch 400/2990 | Loss: 1493.4666\n",
      "Epoch 8/10 | Batch 500/2990 | Loss: 3530.8896\n",
      "Epoch 8/10 | Batch 600/2990 | Loss: 2002.3850\n",
      "Epoch 8/10 | Batch 700/2990 | Loss: 1371.5237\n",
      "Epoch 8/10 | Batch 800/2990 | Loss: 1051.4895\n",
      "Epoch 8/10 | Batch 900/2990 | Loss: 3268.3789\n",
      "Epoch 8/10 | Batch 1000/2990 | Loss: 3451.7275\n",
      "Epoch 8/10 | Batch 1100/2990 | Loss: 2586.6831\n",
      "Epoch 8/10 | Batch 1200/2990 | Loss: 1375.3031\n",
      "Epoch 8/10 | Batch 1300/2990 | Loss: 1982.9445\n",
      "Epoch 8/10 | Batch 1400/2990 | Loss: 3446.2261\n",
      "Epoch 8/10 | Batch 1500/2990 | Loss: 1515.8440\n",
      "Epoch 8/10 | Batch 1600/2990 | Loss: 3483.8667\n",
      "Epoch 8/10 | Batch 1700/2990 | Loss: 3986.1958\n",
      "Epoch 8/10 | Batch 1800/2990 | Loss: 1428.2640\n",
      "Epoch 8/10 | Batch 1900/2990 | Loss: 1983.5985\n",
      "Epoch 8/10 | Batch 2000/2990 | Loss: 1217.6570\n",
      "Epoch 8/10 | Batch 2100/2990 | Loss: 2139.4634\n",
      "Epoch 8/10 | Batch 2200/2990 | Loss: 1763.0613\n",
      "Epoch 8/10 | Batch 2300/2990 | Loss: 2877.6257\n",
      "Epoch 8/10 | Batch 2400/2990 | Loss: 1618.4813\n",
      "Epoch 8/10 | Batch 2500/2990 | Loss: 3211.3804\n",
      "Epoch 8/10 | Batch 2600/2990 | Loss: 2280.6851\n",
      "Epoch 8/10 | Batch 2700/2990 | Loss: 1546.1536\n",
      "Epoch 8/10 | Batch 2800/2990 | Loss: 1478.2006\n",
      "Epoch 8/10 | Batch 2900/2990 | Loss: 1073.1757\n",
      "✅ Epoch 8/10 | Train Loss: 2325.7999 | Val Loss: 2157.9520\n",
      "Epoch 9/10 | Batch 0/2990 | Loss: 611.4830\n",
      "Epoch 9/10 | Batch 100/2990 | Loss: 1092.6826\n",
      "Epoch 9/10 | Batch 200/2990 | Loss: 1246.8110\n",
      "Epoch 9/10 | Batch 300/2990 | Loss: 727.0605\n",
      "Epoch 9/10 | Batch 400/2990 | Loss: 1687.4888\n",
      "Epoch 9/10 | Batch 500/2990 | Loss: 2726.3110\n",
      "Epoch 9/10 | Batch 600/2990 | Loss: 2255.8503\n",
      "Epoch 9/10 | Batch 700/2990 | Loss: 1769.0530\n",
      "Epoch 9/10 | Batch 800/2990 | Loss: 1447.7817\n",
      "Epoch 9/10 | Batch 900/2990 | Loss: 860.5573\n",
      "Epoch 9/10 | Batch 1000/2990 | Loss: 2515.8164\n",
      "Epoch 9/10 | Batch 1100/2990 | Loss: 1415.5743\n",
      "Epoch 9/10 | Batch 1200/2990 | Loss: 1885.1145\n",
      "Epoch 9/10 | Batch 1300/2990 | Loss: 2110.7568\n",
      "Epoch 9/10 | Batch 1400/2990 | Loss: 1819.2595\n",
      "Epoch 9/10 | Batch 1500/2990 | Loss: 2232.1572\n",
      "Epoch 9/10 | Batch 1600/2990 | Loss: 1589.4713\n",
      "Epoch 9/10 | Batch 1700/2990 | Loss: 1176.4868\n",
      "Epoch 9/10 | Batch 1800/2990 | Loss: 686.0659\n",
      "Epoch 9/10 | Batch 1900/2990 | Loss: 1728.7321\n",
      "Epoch 9/10 | Batch 2000/2990 | Loss: 2573.3506\n",
      "Epoch 9/10 | Batch 2100/2990 | Loss: 1750.3804\n",
      "Epoch 9/10 | Batch 2200/2990 | Loss: 2048.6968\n",
      "Epoch 9/10 | Batch 2300/2990 | Loss: 3814.1289\n",
      "Epoch 9/10 | Batch 2400/2990 | Loss: 1398.2063\n",
      "Epoch 9/10 | Batch 2500/2990 | Loss: 875.3448\n",
      "Epoch 9/10 | Batch 2600/2990 | Loss: 2088.1714\n",
      "Epoch 9/10 | Batch 2700/2990 | Loss: 730.8967\n",
      "Epoch 9/10 | Batch 2800/2990 | Loss: 1192.0851\n",
      "Epoch 9/10 | Batch 2900/2990 | Loss: 2949.9604\n",
      "✅ Epoch 9/10 | Train Loss: 1874.6638 | Val Loss: 1897.5240\n",
      "Epoch 10/10 | Batch 0/2990 | Loss: 794.4120\n",
      "Epoch 10/10 | Batch 100/2990 | Loss: 669.4946\n",
      "Epoch 10/10 | Batch 200/2990 | Loss: 2602.7366\n",
      "Epoch 10/10 | Batch 300/2990 | Loss: 1879.7000\n",
      "Epoch 10/10 | Batch 400/2990 | Loss: 1072.1458\n",
      "Epoch 10/10 | Batch 500/2990 | Loss: 1085.4534\n",
      "Epoch 10/10 | Batch 600/2990 | Loss: 2519.5913\n",
      "Epoch 10/10 | Batch 700/2990 | Loss: 480.4374\n",
      "Epoch 10/10 | Batch 800/2990 | Loss: 1094.1820\n",
      "Epoch 10/10 | Batch 900/2990 | Loss: 1858.5596\n",
      "Epoch 10/10 | Batch 1000/2990 | Loss: 1941.3779\n",
      "Epoch 10/10 | Batch 1100/2990 | Loss: 3049.2251\n",
      "Epoch 10/10 | Batch 1200/2990 | Loss: 1046.3752\n",
      "Epoch 10/10 | Batch 1300/2990 | Loss: 1236.4238\n",
      "Epoch 10/10 | Batch 1400/2990 | Loss: 2458.1692\n",
      "Epoch 10/10 | Batch 1500/2990 | Loss: 973.8893\n",
      "Epoch 10/10 | Batch 1600/2990 | Loss: 537.9318\n",
      "Epoch 10/10 | Batch 1700/2990 | Loss: 900.0546\n",
      "Epoch 10/10 | Batch 1800/2990 | Loss: 1309.0083\n",
      "Epoch 10/10 | Batch 1900/2990 | Loss: 2434.9453\n",
      "Epoch 10/10 | Batch 2000/2990 | Loss: 2441.8110\n",
      "Epoch 10/10 | Batch 2100/2990 | Loss: 1539.9598\n",
      "Epoch 10/10 | Batch 2200/2990 | Loss: 3074.0955\n",
      "Epoch 10/10 | Batch 2300/2990 | Loss: 794.5746\n",
      "Epoch 10/10 | Batch 2400/2990 | Loss: 2370.4343\n",
      "Epoch 10/10 | Batch 2500/2990 | Loss: 1425.9591\n",
      "Epoch 10/10 | Batch 2600/2990 | Loss: 1041.7516\n",
      "Epoch 10/10 | Batch 2700/2990 | Loss: 1149.8969\n",
      "Epoch 10/10 | Batch 2800/2990 | Loss: 2487.2002\n",
      "Epoch 10/10 | Batch 2900/2990 | Loss: 1982.9181\n",
      "✅ Epoch 10/10 | Train Loss: 1522.9702 | Val Loss: 1894.0531\n"
     ]
    }
   ],
   "source": [
    "# ✅ Start Training\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 316.97, Actual: 368.00\n",
      "Predicted: 551.94, Actual: 552.00\n",
      "Predicted: 313.22, Actual: 454.00\n",
      "Predicted: 374.20, Actual: 281.00\n",
      "Predicted: 443.07, Actual: 472.00\n",
      "Predicted: 362.94, Actual: 425.00\n",
      "Predicted: 271.00, Actual: 295.00\n",
      "Predicted: 392.60, Actual: 430.00\n",
      "Predicted: 391.84, Actual: 392.00\n",
      "Predicted: 145.89, Actual: 129.00\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ✅ Load the saved model\n",
    "model.load_state_dict(torch.load(\"vit_wheat_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Run test\n",
    "preds, actuals = test_model(model, test_loader)\n",
    "\n",
    "# ✅ Print sample predictions\n",
    "for p, a in zip(preds[:10], actuals[:10]):\n",
    "    print(f\"Predicted: {p:.2f}, Actual: {a:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
