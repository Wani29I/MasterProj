{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from efficientNetModel import EfficientNetWheatModel\n",
    "from dataLoaderFunc import loadSplitData, createLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Compute Validation Loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                outputs = model(rgb_batch, dsm_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save Best Model\n",
    "        torch.save(model.state_dict(), \"efficientnet_wheat_model.pth\")\n",
    "\n",
    "# ✅ Test the model on validation set\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb_batch, dsm_batch, label_batch in test_loader:\n",
    "            rgb_batch, dsm_batch = rgb_batch.to(device), dsm_batch.to(device)\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            actuals.extend(label_batch.cpu().numpy().flatten())\n",
    "\n",
    "    return predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Initialize Model, Loss Function, and Optimizer\n",
    "\n",
    "# ✅ Universal device selection (works on both Mac M2 & Windows with RTX 4060)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ✅ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ✅ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ✅ Default to CPU if no GPU is available\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "model = EfficientNetWheatModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower learning rate for EfficientNet\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "# ✅ Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 0/2990 | Loss: 138205.1250\n",
      "Epoch 1/10 | Batch 20/2990 | Loss: 72352.3047\n",
      "Epoch 1/10 | Batch 40/2990 | Loss: 13863.5352\n",
      "Epoch 1/10 | Batch 60/2990 | Loss: 6886.5967\n",
      "Epoch 1/10 | Batch 80/2990 | Loss: 4209.9443\n",
      "Epoch 1/10 | Batch 100/2990 | Loss: 3245.1499\n",
      "Epoch 1/10 | Batch 120/2990 | Loss: 2727.2837\n",
      "Epoch 1/10 | Batch 140/2990 | Loss: 3461.9041\n",
      "Epoch 1/10 | Batch 160/2990 | Loss: 4402.2686\n",
      "Epoch 1/10 | Batch 180/2990 | Loss: 7920.2261\n",
      "Epoch 1/10 | Batch 200/2990 | Loss: 7893.5557\n",
      "Epoch 1/10 | Batch 220/2990 | Loss: 7634.8281\n",
      "Epoch 1/10 | Batch 240/2990 | Loss: 4557.2275\n",
      "Epoch 1/10 | Batch 260/2990 | Loss: 7128.0522\n",
      "Epoch 1/10 | Batch 280/2990 | Loss: 4141.8965\n",
      "Epoch 1/10 | Batch 300/2990 | Loss: 2541.5493\n",
      "Epoch 1/10 | Batch 320/2990 | Loss: 6190.6250\n",
      "Epoch 1/10 | Batch 340/2990 | Loss: 5646.0049\n",
      "Epoch 1/10 | Batch 360/2990 | Loss: 5274.7295\n",
      "Epoch 1/10 | Batch 380/2990 | Loss: 4305.2114\n",
      "Epoch 1/10 | Batch 400/2990 | Loss: 3916.4575\n",
      "Epoch 1/10 | Batch 420/2990 | Loss: 8828.9961\n",
      "Epoch 1/10 | Batch 440/2990 | Loss: 2479.9995\n",
      "Epoch 1/10 | Batch 460/2990 | Loss: 3525.4077\n",
      "Epoch 1/10 | Batch 480/2990 | Loss: 6800.7280\n",
      "Epoch 1/10 | Batch 500/2990 | Loss: 3010.3484\n",
      "Epoch 1/10 | Batch 520/2990 | Loss: 3321.5449\n",
      "Epoch 1/10 | Batch 540/2990 | Loss: 3372.1738\n",
      "Epoch 1/10 | Batch 560/2990 | Loss: 3053.6831\n",
      "Epoch 1/10 | Batch 580/2990 | Loss: 3226.0073\n",
      "Epoch 1/10 | Batch 600/2990 | Loss: 2461.0515\n",
      "Epoch 1/10 | Batch 620/2990 | Loss: 3467.6082\n",
      "Epoch 1/10 | Batch 640/2990 | Loss: 4478.6362\n",
      "Epoch 1/10 | Batch 660/2990 | Loss: 2843.9639\n",
      "Epoch 1/10 | Batch 680/2990 | Loss: 3627.4524\n",
      "Epoch 1/10 | Batch 700/2990 | Loss: 5163.7778\n",
      "Epoch 1/10 | Batch 720/2990 | Loss: 3568.1016\n",
      "Epoch 1/10 | Batch 740/2990 | Loss: 6361.2192\n",
      "Epoch 1/10 | Batch 760/2990 | Loss: 2791.8003\n",
      "Epoch 1/10 | Batch 780/2990 | Loss: 4416.1147\n",
      "Epoch 1/10 | Batch 800/2990 | Loss: 2263.4617\n",
      "Epoch 1/10 | Batch 820/2990 | Loss: 4978.9893\n",
      "Epoch 1/10 | Batch 840/2990 | Loss: 6750.5894\n",
      "Epoch 1/10 | Batch 860/2990 | Loss: 2872.9238\n",
      "Epoch 1/10 | Batch 880/2990 | Loss: 1506.1481\n",
      "Epoch 1/10 | Batch 900/2990 | Loss: 1639.1239\n",
      "Epoch 1/10 | Batch 920/2990 | Loss: 5012.0117\n",
      "Epoch 1/10 | Batch 940/2990 | Loss: 1902.8444\n",
      "Epoch 1/10 | Batch 960/2990 | Loss: 2599.2473\n",
      "Epoch 1/10 | Batch 980/2990 | Loss: 3871.9309\n",
      "Epoch 1/10 | Batch 1000/2990 | Loss: 2710.6907\n",
      "Epoch 1/10 | Batch 1020/2990 | Loss: 3732.5269\n",
      "Epoch 1/10 | Batch 1040/2990 | Loss: 2806.7783\n",
      "Epoch 1/10 | Batch 1060/2990 | Loss: 1579.2396\n",
      "Epoch 1/10 | Batch 1080/2990 | Loss: 5353.3120\n",
      "Epoch 1/10 | Batch 1100/2990 | Loss: 5831.0693\n",
      "Epoch 1/10 | Batch 1120/2990 | Loss: 4435.0723\n",
      "Epoch 1/10 | Batch 1140/2990 | Loss: 1953.9734\n",
      "Epoch 1/10 | Batch 1160/2990 | Loss: 1149.3064\n",
      "Epoch 1/10 | Batch 1180/2990 | Loss: 3467.3591\n",
      "Epoch 1/10 | Batch 1200/2990 | Loss: 2377.8086\n",
      "Epoch 1/10 | Batch 1220/2990 | Loss: 5322.2842\n",
      "Epoch 1/10 | Batch 1240/2990 | Loss: 861.7837\n",
      "Epoch 1/10 | Batch 1260/2990 | Loss: 2304.4788\n",
      "Epoch 1/10 | Batch 1280/2990 | Loss: 2040.3508\n",
      "Epoch 1/10 | Batch 1300/2990 | Loss: 2976.1885\n",
      "Epoch 1/10 | Batch 1320/2990 | Loss: 1605.9181\n",
      "Epoch 1/10 | Batch 1340/2990 | Loss: 3642.0420\n",
      "Epoch 1/10 | Batch 1360/2990 | Loss: 2081.0947\n",
      "Epoch 1/10 | Batch 1380/2990 | Loss: 3278.1082\n",
      "Epoch 1/10 | Batch 1400/2990 | Loss: 3185.1230\n",
      "Epoch 1/10 | Batch 1420/2990 | Loss: 5155.4453\n",
      "Epoch 1/10 | Batch 1440/2990 | Loss: 3513.0962\n",
      "Epoch 1/10 | Batch 1460/2990 | Loss: 2197.7810\n",
      "Epoch 1/10 | Batch 1480/2990 | Loss: 3726.6016\n",
      "Epoch 1/10 | Batch 1500/2990 | Loss: 1761.9819\n",
      "Epoch 1/10 | Batch 1520/2990 | Loss: 3550.3774\n",
      "Epoch 1/10 | Batch 1540/2990 | Loss: 1814.7100\n",
      "Epoch 1/10 | Batch 1560/2990 | Loss: 1332.1235\n",
      "Epoch 1/10 | Batch 1580/2990 | Loss: 2961.3618\n",
      "Epoch 1/10 | Batch 1600/2990 | Loss: 1876.4438\n",
      "Epoch 1/10 | Batch 1620/2990 | Loss: 1335.5681\n",
      "Epoch 1/10 | Batch 1640/2990 | Loss: 4020.6748\n",
      "Epoch 1/10 | Batch 1660/2990 | Loss: 1175.6276\n",
      "Epoch 1/10 | Batch 1680/2990 | Loss: 2673.2666\n",
      "Epoch 1/10 | Batch 1700/2990 | Loss: 1228.7064\n",
      "Epoch 1/10 | Batch 1720/2990 | Loss: 1924.3126\n",
      "Epoch 1/10 | Batch 1740/2990 | Loss: 745.7426\n",
      "Epoch 1/10 | Batch 1760/2990 | Loss: 4130.0278\n",
      "Epoch 1/10 | Batch 1780/2990 | Loss: 1571.2244\n",
      "Epoch 1/10 | Batch 1800/2990 | Loss: 1065.2029\n",
      "Epoch 1/10 | Batch 1820/2990 | Loss: 5214.3066\n",
      "Epoch 1/10 | Batch 1840/2990 | Loss: 1678.3879\n",
      "Epoch 1/10 | Batch 1860/2990 | Loss: 1791.4148\n",
      "Epoch 1/10 | Batch 1880/2990 | Loss: 2397.2451\n",
      "Epoch 1/10 | Batch 1900/2990 | Loss: 2511.6130\n",
      "Epoch 1/10 | Batch 1920/2990 | Loss: 1798.6738\n",
      "Epoch 1/10 | Batch 1940/2990 | Loss: 1791.8341\n",
      "Epoch 1/10 | Batch 1960/2990 | Loss: 1947.8629\n",
      "Epoch 1/10 | Batch 1980/2990 | Loss: 1307.2882\n",
      "Epoch 1/10 | Batch 2000/2990 | Loss: 2298.3594\n",
      "Epoch 1/10 | Batch 2020/2990 | Loss: 1871.9423\n",
      "Epoch 1/10 | Batch 2040/2990 | Loss: 2545.1104\n",
      "Epoch 1/10 | Batch 2060/2990 | Loss: 2553.6433\n",
      "Epoch 1/10 | Batch 2080/2990 | Loss: 1778.3785\n",
      "Epoch 1/10 | Batch 2100/2990 | Loss: 2070.6729\n",
      "Epoch 1/10 | Batch 2120/2990 | Loss: 3339.7026\n",
      "Epoch 1/10 | Batch 2140/2990 | Loss: 686.5634\n",
      "Epoch 1/10 | Batch 2160/2990 | Loss: 877.4906\n",
      "Epoch 1/10 | Batch 2180/2990 | Loss: 3369.0037\n",
      "Epoch 1/10 | Batch 2200/2990 | Loss: 1987.6189\n",
      "Epoch 1/10 | Batch 2220/2990 | Loss: 1361.0186\n",
      "Epoch 1/10 | Batch 2240/2990 | Loss: 1472.7000\n",
      "Epoch 1/10 | Batch 2260/2990 | Loss: 1795.7830\n",
      "Epoch 1/10 | Batch 2280/2990 | Loss: 1671.9581\n",
      "Epoch 1/10 | Batch 2300/2990 | Loss: 1102.8574\n",
      "Epoch 1/10 | Batch 2320/2990 | Loss: 1880.0464\n",
      "Epoch 1/10 | Batch 2340/2990 | Loss: 2446.9863\n",
      "Epoch 1/10 | Batch 2360/2990 | Loss: 1498.1533\n",
      "Epoch 1/10 | Batch 2380/2990 | Loss: 2612.0620\n",
      "Epoch 1/10 | Batch 2400/2990 | Loss: 2376.2688\n",
      "Epoch 1/10 | Batch 2420/2990 | Loss: 2047.8230\n",
      "Epoch 1/10 | Batch 2440/2990 | Loss: 1236.3099\n",
      "Epoch 1/10 | Batch 2460/2990 | Loss: 1131.3068\n",
      "Epoch 1/10 | Batch 2480/2990 | Loss: 2263.9355\n",
      "Epoch 1/10 | Batch 2500/2990 | Loss: 1076.1936\n",
      "Epoch 1/10 | Batch 2520/2990 | Loss: 516.5650\n",
      "Epoch 1/10 | Batch 2540/2990 | Loss: 1371.6617\n",
      "Epoch 1/10 | Batch 2560/2990 | Loss: 2186.4688\n",
      "Epoch 1/10 | Batch 2580/2990 | Loss: 1278.3750\n",
      "Epoch 1/10 | Batch 2600/2990 | Loss: 1421.1718\n",
      "Epoch 1/10 | Batch 2620/2990 | Loss: 1462.0649\n",
      "Epoch 1/10 | Batch 2640/2990 | Loss: 1512.8843\n",
      "Epoch 1/10 | Batch 2660/2990 | Loss: 712.9261\n",
      "Epoch 1/10 | Batch 2680/2990 | Loss: 1219.6584\n",
      "Epoch 1/10 | Batch 2700/2990 | Loss: 1602.7336\n",
      "Epoch 1/10 | Batch 2720/2990 | Loss: 1523.3386\n",
      "Epoch 1/10 | Batch 2740/2990 | Loss: 1591.8271\n",
      "Epoch 1/10 | Batch 2760/2990 | Loss: 1004.3230\n",
      "Epoch 1/10 | Batch 2780/2990 | Loss: 1461.2626\n",
      "Epoch 1/10 | Batch 2800/2990 | Loss: 2038.0569\n",
      "Epoch 1/10 | Batch 2820/2990 | Loss: 1489.2246\n",
      "Epoch 1/10 | Batch 2840/2990 | Loss: 912.0740\n",
      "Epoch 1/10 | Batch 2860/2990 | Loss: 2056.5852\n",
      "Epoch 1/10 | Batch 2880/2990 | Loss: 1681.1973\n",
      "Epoch 1/10 | Batch 2900/2990 | Loss: 799.7169\n",
      "Epoch 1/10 | Batch 2920/2990 | Loss: 579.1233\n",
      "Epoch 1/10 | Batch 2940/2990 | Loss: 1710.9065\n",
      "Epoch 1/10 | Batch 2960/2990 | Loss: 1345.5211\n",
      "Epoch 1/10 | Batch 2980/2990 | Loss: 1471.9703\n",
      "✅ Epoch 1/10 | Train Loss: 3896.3416 | Val Loss: 1198.8036\n",
      "Epoch 2/10 | Batch 0/2990 | Loss: 1163.7068\n",
      "Epoch 2/10 | Batch 20/2990 | Loss: 2131.8750\n",
      "Epoch 2/10 | Batch 40/2990 | Loss: 1135.8097\n",
      "Epoch 2/10 | Batch 60/2990 | Loss: 1274.9915\n",
      "Epoch 2/10 | Batch 80/2990 | Loss: 601.5099\n",
      "Epoch 2/10 | Batch 100/2990 | Loss: 1038.2690\n",
      "Epoch 2/10 | Batch 120/2990 | Loss: 893.0950\n",
      "Epoch 2/10 | Batch 140/2990 | Loss: 1216.0269\n",
      "Epoch 2/10 | Batch 160/2990 | Loss: 968.7291\n",
      "Epoch 2/10 | Batch 180/2990 | Loss: 830.8159\n",
      "Epoch 2/10 | Batch 200/2990 | Loss: 1143.4814\n",
      "Epoch 2/10 | Batch 220/2990 | Loss: 1899.1051\n",
      "Epoch 2/10 | Batch 240/2990 | Loss: 1885.2292\n",
      "Epoch 2/10 | Batch 260/2990 | Loss: 1230.9963\n",
      "Epoch 2/10 | Batch 280/2990 | Loss: 2282.1184\n",
      "Epoch 2/10 | Batch 300/2990 | Loss: 1281.4326\n",
      "Epoch 2/10 | Batch 320/2990 | Loss: 2461.0125\n",
      "Epoch 2/10 | Batch 340/2990 | Loss: 2091.5857\n",
      "Epoch 2/10 | Batch 360/2990 | Loss: 790.8955\n",
      "Epoch 2/10 | Batch 380/2990 | Loss: 475.6743\n",
      "Epoch 2/10 | Batch 400/2990 | Loss: 987.0442\n",
      "Epoch 2/10 | Batch 420/2990 | Loss: 1103.7906\n",
      "Epoch 2/10 | Batch 440/2990 | Loss: 1135.0679\n",
      "Epoch 2/10 | Batch 460/2990 | Loss: 1252.1526\n",
      "Epoch 2/10 | Batch 480/2990 | Loss: 1411.8718\n",
      "Epoch 2/10 | Batch 500/2990 | Loss: 2709.0176\n",
      "Epoch 2/10 | Batch 520/2990 | Loss: 1692.6466\n",
      "Epoch 2/10 | Batch 540/2990 | Loss: 729.9598\n",
      "Epoch 2/10 | Batch 560/2990 | Loss: 649.6702\n",
      "Epoch 2/10 | Batch 580/2990 | Loss: 732.8921\n",
      "Epoch 2/10 | Batch 600/2990 | Loss: 708.8948\n",
      "Epoch 2/10 | Batch 620/2990 | Loss: 1011.5305\n",
      "Epoch 2/10 | Batch 640/2990 | Loss: 1173.4930\n",
      "Epoch 2/10 | Batch 660/2990 | Loss: 1632.5933\n",
      "Epoch 2/10 | Batch 680/2990 | Loss: 634.5199\n",
      "Epoch 2/10 | Batch 700/2990 | Loss: 2191.0229\n",
      "Epoch 2/10 | Batch 720/2990 | Loss: 879.5293\n",
      "Epoch 2/10 | Batch 740/2990 | Loss: 2080.4805\n",
      "Epoch 2/10 | Batch 760/2990 | Loss: 1414.6648\n",
      "Epoch 2/10 | Batch 780/2990 | Loss: 485.8577\n",
      "Epoch 2/10 | Batch 800/2990 | Loss: 1797.2206\n",
      "Epoch 2/10 | Batch 820/2990 | Loss: 706.0868\n",
      "Epoch 2/10 | Batch 840/2990 | Loss: 735.4648\n",
      "Epoch 2/10 | Batch 860/2990 | Loss: 286.4355\n",
      "Epoch 2/10 | Batch 880/2990 | Loss: 1106.4126\n",
      "Epoch 2/10 | Batch 900/2990 | Loss: 638.3514\n",
      "Epoch 2/10 | Batch 920/2990 | Loss: 1577.7126\n",
      "Epoch 2/10 | Batch 940/2990 | Loss: 571.0016\n",
      "Epoch 2/10 | Batch 960/2990 | Loss: 1892.6008\n",
      "Epoch 2/10 | Batch 980/2990 | Loss: 668.8481\n",
      "Epoch 2/10 | Batch 1000/2990 | Loss: 1546.9202\n",
      "Epoch 2/10 | Batch 1020/2990 | Loss: 737.9951\n",
      "Epoch 2/10 | Batch 1040/2990 | Loss: 692.1740\n",
      "Epoch 2/10 | Batch 1060/2990 | Loss: 629.0193\n",
      "Epoch 2/10 | Batch 1080/2990 | Loss: 939.0547\n",
      "Epoch 2/10 | Batch 1100/2990 | Loss: 656.4764\n",
      "Epoch 2/10 | Batch 1120/2990 | Loss: 957.8591\n",
      "Epoch 2/10 | Batch 1140/2990 | Loss: 499.1629\n",
      "Epoch 2/10 | Batch 1160/2990 | Loss: 872.0044\n",
      "Epoch 2/10 | Batch 1180/2990 | Loss: 953.9444\n",
      "Epoch 2/10 | Batch 1200/2990 | Loss: 906.3757\n",
      "Epoch 2/10 | Batch 1220/2990 | Loss: 742.3704\n",
      "Epoch 2/10 | Batch 1240/2990 | Loss: 789.1649\n",
      "Epoch 2/10 | Batch 1260/2990 | Loss: 996.2668\n",
      "Epoch 2/10 | Batch 1280/2990 | Loss: 854.4594\n",
      "Epoch 2/10 | Batch 1300/2990 | Loss: 680.6182\n",
      "Epoch 2/10 | Batch 1320/2990 | Loss: 857.8541\n",
      "Epoch 2/10 | Batch 1340/2990 | Loss: 1020.6872\n",
      "Epoch 2/10 | Batch 1360/2990 | Loss: 774.6723\n",
      "Epoch 2/10 | Batch 1380/2990 | Loss: 786.9990\n",
      "Epoch 2/10 | Batch 1400/2990 | Loss: 778.0553\n",
      "Epoch 2/10 | Batch 1420/2990 | Loss: 2499.5061\n",
      "Epoch 2/10 | Batch 1440/2990 | Loss: 2000.9221\n",
      "Epoch 2/10 | Batch 1460/2990 | Loss: 885.3383\n",
      "Epoch 2/10 | Batch 1480/2990 | Loss: 504.8970\n",
      "Epoch 2/10 | Batch 1500/2990 | Loss: 488.7809\n",
      "Epoch 2/10 | Batch 1520/2990 | Loss: 679.6679\n",
      "Epoch 2/10 | Batch 1540/2990 | Loss: 467.4179\n",
      "Epoch 2/10 | Batch 1560/2990 | Loss: 1870.4719\n",
      "Epoch 2/10 | Batch 1580/2990 | Loss: 1154.8623\n",
      "Epoch 2/10 | Batch 1600/2990 | Loss: 1261.1470\n",
      "Epoch 2/10 | Batch 1620/2990 | Loss: 874.0799\n",
      "Epoch 2/10 | Batch 1640/2990 | Loss: 1431.3262\n",
      "Epoch 2/10 | Batch 1660/2990 | Loss: 2007.9633\n",
      "Epoch 2/10 | Batch 1680/2990 | Loss: 472.9593\n",
      "Epoch 2/10 | Batch 1700/2990 | Loss: 1020.5273\n",
      "Epoch 2/10 | Batch 1720/2990 | Loss: 3895.8330\n",
      "Epoch 2/10 | Batch 1740/2990 | Loss: 1468.5583\n",
      "Epoch 2/10 | Batch 1760/2990 | Loss: 1376.0283\n",
      "Epoch 2/10 | Batch 1780/2990 | Loss: 790.3267\n",
      "Epoch 2/10 | Batch 1800/2990 | Loss: 555.5164\n",
      "Epoch 2/10 | Batch 1820/2990 | Loss: 970.6612\n",
      "Epoch 2/10 | Batch 1840/2990 | Loss: 935.2765\n",
      "Epoch 2/10 | Batch 1860/2990 | Loss: 1916.9702\n",
      "Epoch 2/10 | Batch 1880/2990 | Loss: 1599.7126\n",
      "Epoch 2/10 | Batch 1900/2990 | Loss: 1025.1389\n",
      "Epoch 2/10 | Batch 1920/2990 | Loss: 336.0239\n",
      "Epoch 2/10 | Batch 1940/2990 | Loss: 1562.6184\n",
      "Epoch 2/10 | Batch 1960/2990 | Loss: 2799.7231\n",
      "Epoch 2/10 | Batch 1980/2990 | Loss: 1050.2311\n",
      "Epoch 2/10 | Batch 2000/2990 | Loss: 2016.9501\n",
      "Epoch 2/10 | Batch 2020/2990 | Loss: 581.7999\n",
      "Epoch 2/10 | Batch 2040/2990 | Loss: 1579.9951\n",
      "Epoch 2/10 | Batch 2060/2990 | Loss: 3491.4707\n",
      "Epoch 2/10 | Batch 2080/2990 | Loss: 972.8660\n",
      "Epoch 2/10 | Batch 2100/2990 | Loss: 651.7556\n",
      "Epoch 2/10 | Batch 2120/2990 | Loss: 1057.3499\n",
      "Epoch 2/10 | Batch 2140/2990 | Loss: 3705.7231\n",
      "Epoch 2/10 | Batch 2160/2990 | Loss: 965.2938\n",
      "Epoch 2/10 | Batch 2180/2990 | Loss: 1384.5199\n",
      "Epoch 2/10 | Batch 2200/2990 | Loss: 885.8325\n",
      "Epoch 2/10 | Batch 2220/2990 | Loss: 348.3242\n",
      "Epoch 2/10 | Batch 2240/2990 | Loss: 629.3945\n",
      "Epoch 2/10 | Batch 2260/2990 | Loss: 1128.3455\n",
      "Epoch 2/10 | Batch 2280/2990 | Loss: 569.2239\n",
      "Epoch 2/10 | Batch 2300/2990 | Loss: 1182.7584\n",
      "Epoch 2/10 | Batch 2320/2990 | Loss: 395.4077\n",
      "Epoch 2/10 | Batch 2340/2990 | Loss: 587.5322\n",
      "Epoch 2/10 | Batch 2360/2990 | Loss: 670.1660\n",
      "Epoch 2/10 | Batch 2380/2990 | Loss: 655.8552\n",
      "Epoch 2/10 | Batch 2400/2990 | Loss: 400.7481\n",
      "Epoch 2/10 | Batch 2420/2990 | Loss: 848.2769\n",
      "Epoch 2/10 | Batch 2440/2990 | Loss: 1853.3416\n",
      "Epoch 2/10 | Batch 2460/2990 | Loss: 1699.0553\n",
      "Epoch 2/10 | Batch 2480/2990 | Loss: 1480.6290\n",
      "Epoch 2/10 | Batch 2500/2990 | Loss: 587.1546\n",
      "Epoch 2/10 | Batch 2520/2990 | Loss: 642.9458\n",
      "Epoch 2/10 | Batch 2540/2990 | Loss: 1152.0265\n",
      "Epoch 2/10 | Batch 2560/2990 | Loss: 238.0854\n",
      "Epoch 2/10 | Batch 2580/2990 | Loss: 1127.1639\n",
      "Epoch 2/10 | Batch 2600/2990 | Loss: 1064.4673\n",
      "Epoch 2/10 | Batch 2620/2990 | Loss: 807.3921\n",
      "Epoch 2/10 | Batch 2640/2990 | Loss: 628.6802\n",
      "Epoch 2/10 | Batch 2660/2990 | Loss: 663.9355\n",
      "Epoch 2/10 | Batch 2680/2990 | Loss: 400.4409\n",
      "Epoch 2/10 | Batch 2700/2990 | Loss: 649.9391\n",
      "Epoch 2/10 | Batch 2720/2990 | Loss: 596.0839\n",
      "Epoch 2/10 | Batch 2740/2990 | Loss: 309.9665\n",
      "Epoch 2/10 | Batch 2760/2990 | Loss: 690.9197\n",
      "Epoch 2/10 | Batch 2780/2990 | Loss: 662.0187\n",
      "Epoch 2/10 | Batch 2800/2990 | Loss: 667.7081\n",
      "Epoch 2/10 | Batch 2820/2990 | Loss: 672.4764\n",
      "Epoch 2/10 | Batch 2840/2990 | Loss: 625.3510\n",
      "Epoch 2/10 | Batch 2860/2990 | Loss: 325.8947\n",
      "Epoch 2/10 | Batch 2880/2990 | Loss: 248.2722\n",
      "Epoch 2/10 | Batch 2900/2990 | Loss: 242.2916\n",
      "Epoch 2/10 | Batch 2920/2990 | Loss: 723.0106\n",
      "Epoch 2/10 | Batch 2940/2990 | Loss: 393.4038\n",
      "Epoch 2/10 | Batch 2960/2990 | Loss: 1040.2228\n",
      "Epoch 2/10 | Batch 2980/2990 | Loss: 1208.8755\n",
      "✅ Epoch 2/10 | Train Loss: 1065.2430 | Val Loss: 592.7431\n",
      "Epoch 3/10 | Batch 0/2990 | Loss: 400.8564\n",
      "Epoch 3/10 | Batch 20/2990 | Loss: 369.2033\n",
      "Epoch 3/10 | Batch 40/2990 | Loss: 473.1899\n",
      "Epoch 3/10 | Batch 60/2990 | Loss: 225.4885\n",
      "Epoch 3/10 | Batch 80/2990 | Loss: 845.8196\n",
      "Epoch 3/10 | Batch 100/2990 | Loss: 268.9250\n",
      "Epoch 3/10 | Batch 120/2990 | Loss: 1293.8151\n",
      "Epoch 3/10 | Batch 140/2990 | Loss: 516.4894\n",
      "Epoch 3/10 | Batch 160/2990 | Loss: 413.7930\n",
      "Epoch 3/10 | Batch 180/2990 | Loss: 719.4205\n",
      "Epoch 3/10 | Batch 200/2990 | Loss: 865.3268\n",
      "Epoch 3/10 | Batch 220/2990 | Loss: 618.5263\n",
      "Epoch 3/10 | Batch 240/2990 | Loss: 1123.1564\n",
      "Epoch 3/10 | Batch 260/2990 | Loss: 581.1006\n",
      "Epoch 3/10 | Batch 280/2990 | Loss: 545.8662\n",
      "Epoch 3/10 | Batch 300/2990 | Loss: 1047.7556\n",
      "Epoch 3/10 | Batch 320/2990 | Loss: 1232.0745\n",
      "Epoch 3/10 | Batch 340/2990 | Loss: 280.0921\n",
      "Epoch 3/10 | Batch 360/2990 | Loss: 1581.4519\n",
      "Epoch 3/10 | Batch 380/2990 | Loss: 544.3126\n",
      "Epoch 3/10 | Batch 400/2990 | Loss: 537.6480\n",
      "Epoch 3/10 | Batch 420/2990 | Loss: 920.9282\n",
      "Epoch 3/10 | Batch 440/2990 | Loss: 283.5885\n",
      "Epoch 3/10 | Batch 460/2990 | Loss: 1266.9834\n",
      "Epoch 3/10 | Batch 480/2990 | Loss: 1075.0344\n",
      "Epoch 3/10 | Batch 500/2990 | Loss: 830.6093\n",
      "Epoch 3/10 | Batch 520/2990 | Loss: 422.0839\n",
      "Epoch 3/10 | Batch 540/2990 | Loss: 468.8926\n",
      "Epoch 3/10 | Batch 560/2990 | Loss: 1498.6008\n",
      "Epoch 3/10 | Batch 580/2990 | Loss: 583.0704\n",
      "Epoch 3/10 | Batch 600/2990 | Loss: 613.5912\n",
      "Epoch 3/10 | Batch 620/2990 | Loss: 423.7462\n",
      "Epoch 3/10 | Batch 640/2990 | Loss: 1582.3718\n",
      "Epoch 3/10 | Batch 660/2990 | Loss: 456.9481\n",
      "Epoch 3/10 | Batch 680/2990 | Loss: 1288.2334\n",
      "Epoch 3/10 | Batch 700/2990 | Loss: 632.1418\n",
      "Epoch 3/10 | Batch 720/2990 | Loss: 226.8655\n",
      "Epoch 3/10 | Batch 740/2990 | Loss: 827.0276\n",
      "Epoch 3/10 | Batch 760/2990 | Loss: 1093.2471\n",
      "Epoch 3/10 | Batch 780/2990 | Loss: 316.5735\n",
      "Epoch 3/10 | Batch 800/2990 | Loss: 1391.0574\n",
      "Epoch 3/10 | Batch 820/2990 | Loss: 520.0433\n",
      "Epoch 3/10 | Batch 840/2990 | Loss: 883.6346\n",
      "Epoch 3/10 | Batch 860/2990 | Loss: 392.9615\n",
      "Epoch 3/10 | Batch 880/2990 | Loss: 998.0934\n",
      "Epoch 3/10 | Batch 900/2990 | Loss: 247.4706\n",
      "Epoch 3/10 | Batch 920/2990 | Loss: 780.4384\n",
      "Epoch 3/10 | Batch 940/2990 | Loss: 405.4183\n",
      "Epoch 3/10 | Batch 960/2990 | Loss: 590.4122\n",
      "Epoch 3/10 | Batch 980/2990 | Loss: 198.5203\n",
      "Epoch 3/10 | Batch 1000/2990 | Loss: 370.0575\n",
      "Epoch 3/10 | Batch 1020/2990 | Loss: 955.6862\n",
      "Epoch 3/10 | Batch 1040/2990 | Loss: 777.3398\n",
      "Epoch 3/10 | Batch 1060/2990 | Loss: 668.6100\n",
      "Epoch 3/10 | Batch 1080/2990 | Loss: 171.7633\n",
      "Epoch 3/10 | Batch 1100/2990 | Loss: 707.2729\n",
      "Epoch 3/10 | Batch 1120/2990 | Loss: 997.0692\n",
      "Epoch 3/10 | Batch 1140/2990 | Loss: 640.2336\n",
      "Epoch 3/10 | Batch 1160/2990 | Loss: 403.5483\n",
      "Epoch 3/10 | Batch 1180/2990 | Loss: 1677.0212\n",
      "Epoch 3/10 | Batch 1200/2990 | Loss: 569.0319\n",
      "Epoch 3/10 | Batch 1220/2990 | Loss: 584.8347\n",
      "Epoch 3/10 | Batch 1240/2990 | Loss: 404.4092\n",
      "Epoch 3/10 | Batch 1260/2990 | Loss: 227.5108\n",
      "Epoch 3/10 | Batch 1280/2990 | Loss: 597.2004\n",
      "Epoch 3/10 | Batch 1300/2990 | Loss: 885.8610\n",
      "Epoch 3/10 | Batch 1320/2990 | Loss: 838.4764\n",
      "Epoch 3/10 | Batch 1340/2990 | Loss: 1059.4595\n",
      "Epoch 3/10 | Batch 1360/2990 | Loss: 163.1667\n",
      "Epoch 3/10 | Batch 1380/2990 | Loss: 568.9773\n",
      "Epoch 3/10 | Batch 1400/2990 | Loss: 415.7946\n",
      "Epoch 3/10 | Batch 1420/2990 | Loss: 360.4910\n",
      "Epoch 3/10 | Batch 1440/2990 | Loss: 1574.1455\n",
      "Epoch 3/10 | Batch 1460/2990 | Loss: 1657.4183\n",
      "Epoch 3/10 | Batch 1480/2990 | Loss: 456.0892\n",
      "Epoch 3/10 | Batch 1500/2990 | Loss: 295.4008\n",
      "Epoch 3/10 | Batch 1520/2990 | Loss: 398.3073\n",
      "Epoch 3/10 | Batch 1540/2990 | Loss: 571.9918\n",
      "Epoch 3/10 | Batch 1560/2990 | Loss: 348.7588\n",
      "Epoch 3/10 | Batch 1580/2990 | Loss: 346.7206\n",
      "Epoch 3/10 | Batch 1600/2990 | Loss: 464.4458\n",
      "Epoch 3/10 | Batch 1620/2990 | Loss: 246.1107\n",
      "Epoch 3/10 | Batch 1640/2990 | Loss: 833.1298\n",
      "Epoch 3/10 | Batch 1660/2990 | Loss: 790.0078\n",
      "Epoch 3/10 | Batch 1680/2990 | Loss: 343.5820\n",
      "Epoch 3/10 | Batch 1700/2990 | Loss: 534.4230\n",
      "Epoch 3/10 | Batch 1720/2990 | Loss: 714.8439\n",
      "Epoch 3/10 | Batch 1740/2990 | Loss: 693.6077\n",
      "Epoch 3/10 | Batch 1760/2990 | Loss: 717.8086\n",
      "Epoch 3/10 | Batch 1780/2990 | Loss: 785.8044\n",
      "Epoch 3/10 | Batch 1800/2990 | Loss: 649.5247\n",
      "Epoch 3/10 | Batch 1820/2990 | Loss: 433.9879\n",
      "Epoch 3/10 | Batch 1840/2990 | Loss: 206.3154\n",
      "Epoch 3/10 | Batch 1860/2990 | Loss: 707.5640\n",
      "Epoch 3/10 | Batch 1880/2990 | Loss: 496.9228\n",
      "Epoch 3/10 | Batch 1900/2990 | Loss: 1174.8921\n",
      "Epoch 3/10 | Batch 1920/2990 | Loss: 564.9832\n",
      "Epoch 3/10 | Batch 1940/2990 | Loss: 479.2744\n",
      "Epoch 3/10 | Batch 1960/2990 | Loss: 318.6377\n",
      "Epoch 3/10 | Batch 1980/2990 | Loss: 582.8301\n",
      "Epoch 3/10 | Batch 2000/2990 | Loss: 340.0344\n",
      "Epoch 3/10 | Batch 2020/2990 | Loss: 353.7586\n",
      "Epoch 3/10 | Batch 2040/2990 | Loss: 308.5269\n",
      "Epoch 3/10 | Batch 2060/2990 | Loss: 511.5885\n",
      "Epoch 3/10 | Batch 2080/2990 | Loss: 398.6645\n",
      "Epoch 3/10 | Batch 2100/2990 | Loss: 417.3369\n",
      "Epoch 3/10 | Batch 2120/2990 | Loss: 434.5421\n",
      "Epoch 3/10 | Batch 2140/2990 | Loss: 471.2036\n",
      "Epoch 3/10 | Batch 2160/2990 | Loss: 271.9838\n",
      "Epoch 3/10 | Batch 2180/2990 | Loss: 452.8544\n",
      "Epoch 3/10 | Batch 2200/2990 | Loss: 572.7766\n",
      "Epoch 3/10 | Batch 2220/2990 | Loss: 395.5306\n",
      "Epoch 3/10 | Batch 2240/2990 | Loss: 534.3005\n",
      "Epoch 3/10 | Batch 2260/2990 | Loss: 372.4185\n",
      "Epoch 3/10 | Batch 2280/2990 | Loss: 534.1929\n",
      "Epoch 3/10 | Batch 2300/2990 | Loss: 496.9109\n",
      "Epoch 3/10 | Batch 2320/2990 | Loss: 246.1274\n",
      "Epoch 3/10 | Batch 2340/2990 | Loss: 747.7318\n",
      "Epoch 3/10 | Batch 2360/2990 | Loss: 459.8577\n",
      "Epoch 3/10 | Batch 2380/2990 | Loss: 310.1604\n",
      "Epoch 3/10 | Batch 2400/2990 | Loss: 330.7629\n",
      "Epoch 3/10 | Batch 2420/2990 | Loss: 329.6612\n",
      "Epoch 3/10 | Batch 2440/2990 | Loss: 1746.3511\n",
      "Epoch 3/10 | Batch 2460/2990 | Loss: 853.5471\n",
      "Epoch 3/10 | Batch 2480/2990 | Loss: 384.6117\n",
      "Epoch 3/10 | Batch 2500/2990 | Loss: 307.9194\n",
      "Epoch 3/10 | Batch 2520/2990 | Loss: 580.7540\n",
      "Epoch 3/10 | Batch 2540/2990 | Loss: 372.5776\n",
      "Epoch 3/10 | Batch 2560/2990 | Loss: 324.4735\n",
      "Epoch 3/10 | Batch 2580/2990 | Loss: 496.0703\n",
      "Epoch 3/10 | Batch 2600/2990 | Loss: 556.8479\n",
      "Epoch 3/10 | Batch 2620/2990 | Loss: 529.4051\n",
      "Epoch 3/10 | Batch 2640/2990 | Loss: 1009.0171\n",
      "Epoch 3/10 | Batch 2660/2990 | Loss: 553.2581\n",
      "Epoch 3/10 | Batch 2680/2990 | Loss: 1212.9642\n",
      "Epoch 3/10 | Batch 2700/2990 | Loss: 708.1372\n",
      "Epoch 3/10 | Batch 2720/2990 | Loss: 1249.1042\n",
      "Epoch 3/10 | Batch 2740/2990 | Loss: 685.1074\n",
      "Epoch 3/10 | Batch 2760/2990 | Loss: 742.3972\n",
      "Epoch 3/10 | Batch 2780/2990 | Loss: 178.6010\n",
      "Epoch 3/10 | Batch 2800/2990 | Loss: 1276.2729\n",
      "Epoch 3/10 | Batch 2820/2990 | Loss: 460.3649\n",
      "Epoch 3/10 | Batch 2840/2990 | Loss: 781.3591\n",
      "Epoch 3/10 | Batch 2860/2990 | Loss: 308.6527\n",
      "Epoch 3/10 | Batch 2880/2990 | Loss: 480.0922\n",
      "Epoch 3/10 | Batch 2900/2990 | Loss: 617.6322\n",
      "Epoch 3/10 | Batch 2920/2990 | Loss: 507.4146\n",
      "Epoch 3/10 | Batch 2940/2990 | Loss: 958.0576\n",
      "Epoch 3/10 | Batch 2960/2990 | Loss: 293.3755\n",
      "Epoch 3/10 | Batch 2980/2990 | Loss: 638.5715\n",
      "✅ Epoch 3/10 | Train Loss: 661.2867 | Val Loss: 433.5355\n",
      "Epoch 4/10 | Batch 0/2990 | Loss: 395.1790\n",
      "Epoch 4/10 | Batch 20/2990 | Loss: 232.4517\n",
      "Epoch 4/10 | Batch 40/2990 | Loss: 365.1216\n",
      "Epoch 4/10 | Batch 60/2990 | Loss: 647.6036\n",
      "Epoch 4/10 | Batch 80/2990 | Loss: 303.6437\n",
      "Epoch 4/10 | Batch 100/2990 | Loss: 346.2262\n",
      "Epoch 4/10 | Batch 120/2990 | Loss: 266.8313\n",
      "Epoch 4/10 | Batch 140/2990 | Loss: 510.2284\n",
      "Epoch 4/10 | Batch 160/2990 | Loss: 396.9382\n",
      "Epoch 4/10 | Batch 180/2990 | Loss: 604.5704\n",
      "Epoch 4/10 | Batch 200/2990 | Loss: 393.9742\n",
      "Epoch 4/10 | Batch 220/2990 | Loss: 738.1992\n",
      "Epoch 4/10 | Batch 240/2990 | Loss: 171.4620\n",
      "Epoch 4/10 | Batch 260/2990 | Loss: 176.3074\n",
      "Epoch 4/10 | Batch 280/2990 | Loss: 487.8204\n",
      "Epoch 4/10 | Batch 300/2990 | Loss: 634.1268\n",
      "Epoch 4/10 | Batch 320/2990 | Loss: 1394.0941\n",
      "Epoch 4/10 | Batch 340/2990 | Loss: 291.4820\n",
      "Epoch 4/10 | Batch 360/2990 | Loss: 477.8249\n",
      "Epoch 4/10 | Batch 380/2990 | Loss: 386.7038\n",
      "Epoch 4/10 | Batch 400/2990 | Loss: 356.2985\n",
      "Epoch 4/10 | Batch 420/2990 | Loss: 691.2982\n",
      "Epoch 4/10 | Batch 440/2990 | Loss: 283.3835\n",
      "Epoch 4/10 | Batch 460/2990 | Loss: 849.6940\n",
      "Epoch 4/10 | Batch 480/2990 | Loss: 235.3648\n",
      "Epoch 4/10 | Batch 500/2990 | Loss: 396.9873\n",
      "Epoch 4/10 | Batch 520/2990 | Loss: 247.0999\n",
      "Epoch 4/10 | Batch 540/2990 | Loss: 412.8136\n",
      "Epoch 4/10 | Batch 560/2990 | Loss: 485.1469\n",
      "Epoch 4/10 | Batch 580/2990 | Loss: 221.2479\n",
      "Epoch 4/10 | Batch 600/2990 | Loss: 767.9937\n",
      "Epoch 4/10 | Batch 620/2990 | Loss: 435.6804\n",
      "Epoch 4/10 | Batch 640/2990 | Loss: 481.0243\n",
      "Epoch 4/10 | Batch 660/2990 | Loss: 364.5834\n",
      "Epoch 4/10 | Batch 680/2990 | Loss: 800.2103\n",
      "Epoch 4/10 | Batch 700/2990 | Loss: 207.3845\n",
      "Epoch 4/10 | Batch 720/2990 | Loss: 1912.6614\n",
      "Epoch 4/10 | Batch 740/2990 | Loss: 510.8569\n",
      "Epoch 4/10 | Batch 760/2990 | Loss: 388.9261\n",
      "Epoch 4/10 | Batch 780/2990 | Loss: 395.8264\n",
      "Epoch 4/10 | Batch 800/2990 | Loss: 631.0679\n",
      "Epoch 4/10 | Batch 820/2990 | Loss: 490.8161\n",
      "Epoch 4/10 | Batch 840/2990 | Loss: 445.6810\n",
      "Epoch 4/10 | Batch 860/2990 | Loss: 352.7482\n",
      "Epoch 4/10 | Batch 880/2990 | Loss: 746.2269\n",
      "Epoch 4/10 | Batch 900/2990 | Loss: 220.1020\n",
      "Epoch 4/10 | Batch 920/2990 | Loss: 475.2884\n",
      "Epoch 4/10 | Batch 940/2990 | Loss: 448.7405\n",
      "Epoch 4/10 | Batch 960/2990 | Loss: 206.4832\n",
      "Epoch 4/10 | Batch 980/2990 | Loss: 185.7758\n",
      "Epoch 4/10 | Batch 1000/2990 | Loss: 633.0040\n",
      "Epoch 4/10 | Batch 1020/2990 | Loss: 212.8825\n",
      "Epoch 4/10 | Batch 1040/2990 | Loss: 343.2910\n",
      "Epoch 4/10 | Batch 1060/2990 | Loss: 238.3239\n",
      "Epoch 4/10 | Batch 1080/2990 | Loss: 291.6596\n",
      "Epoch 4/10 | Batch 1100/2990 | Loss: 1123.3875\n",
      "Epoch 4/10 | Batch 1120/2990 | Loss: 271.0593\n",
      "Epoch 4/10 | Batch 1140/2990 | Loss: 163.6234\n",
      "Epoch 4/10 | Batch 1160/2990 | Loss: 462.5591\n",
      "Epoch 4/10 | Batch 1180/2990 | Loss: 445.0128\n",
      "Epoch 4/10 | Batch 1200/2990 | Loss: 239.5378\n",
      "Epoch 4/10 | Batch 1220/2990 | Loss: 405.6240\n",
      "Epoch 4/10 | Batch 1240/2990 | Loss: 328.8038\n",
      "Epoch 4/10 | Batch 1260/2990 | Loss: 710.3334\n",
      "Epoch 4/10 | Batch 1280/2990 | Loss: 321.3199\n",
      "Epoch 4/10 | Batch 1300/2990 | Loss: 759.7346\n",
      "Epoch 4/10 | Batch 1320/2990 | Loss: 165.9401\n",
      "Epoch 4/10 | Batch 1340/2990 | Loss: 246.7017\n",
      "Epoch 4/10 | Batch 1360/2990 | Loss: 188.5341\n",
      "Epoch 4/10 | Batch 1380/2990 | Loss: 638.5129\n",
      "Epoch 4/10 | Batch 1400/2990 | Loss: 385.3234\n",
      "Epoch 4/10 | Batch 1420/2990 | Loss: 353.6144\n",
      "Epoch 4/10 | Batch 1440/2990 | Loss: 342.9165\n",
      "Epoch 4/10 | Batch 1460/2990 | Loss: 541.7714\n",
      "Epoch 4/10 | Batch 1480/2990 | Loss: 171.3539\n",
      "Epoch 4/10 | Batch 1500/2990 | Loss: 345.5234\n",
      "Epoch 4/10 | Batch 1520/2990 | Loss: 279.8962\n",
      "Epoch 4/10 | Batch 1540/2990 | Loss: 369.4589\n",
      "Epoch 4/10 | Batch 1560/2990 | Loss: 523.9452\n",
      "Epoch 4/10 | Batch 1580/2990 | Loss: 389.2799\n",
      "Epoch 4/10 | Batch 1600/2990 | Loss: 408.6188\n",
      "Epoch 4/10 | Batch 1620/2990 | Loss: 451.1574\n",
      "Epoch 4/10 | Batch 1640/2990 | Loss: 257.5792\n",
      "Epoch 4/10 | Batch 1660/2990 | Loss: 626.3335\n",
      "Epoch 4/10 | Batch 1680/2990 | Loss: 322.3386\n",
      "Epoch 4/10 | Batch 1700/2990 | Loss: 281.4821\n",
      "Epoch 4/10 | Batch 1720/2990 | Loss: 670.2457\n",
      "Epoch 4/10 | Batch 1740/2990 | Loss: 371.4892\n",
      "Epoch 4/10 | Batch 1760/2990 | Loss: 1561.5608\n",
      "Epoch 4/10 | Batch 1780/2990 | Loss: 361.0922\n",
      "Epoch 4/10 | Batch 1800/2990 | Loss: 361.9535\n",
      "Epoch 4/10 | Batch 1820/2990 | Loss: 384.2046\n",
      "Epoch 4/10 | Batch 1840/2990 | Loss: 905.4410\n",
      "Epoch 4/10 | Batch 1860/2990 | Loss: 417.0516\n",
      "Epoch 4/10 | Batch 1880/2990 | Loss: 564.1558\n",
      "Epoch 4/10 | Batch 1900/2990 | Loss: 472.4183\n",
      "Epoch 4/10 | Batch 1920/2990 | Loss: 847.6257\n",
      "Epoch 4/10 | Batch 1940/2990 | Loss: 278.4478\n",
      "Epoch 4/10 | Batch 1960/2990 | Loss: 119.1792\n",
      "Epoch 4/10 | Batch 1980/2990 | Loss: 286.7492\n",
      "Epoch 4/10 | Batch 2000/2990 | Loss: 136.3216\n",
      "Epoch 4/10 | Batch 2020/2990 | Loss: 258.4239\n",
      "Epoch 4/10 | Batch 2040/2990 | Loss: 585.6425\n",
      "Epoch 4/10 | Batch 2060/2990 | Loss: 293.3081\n",
      "Epoch 4/10 | Batch 2080/2990 | Loss: 370.3793\n",
      "Epoch 4/10 | Batch 2100/2990 | Loss: 465.1351\n",
      "Epoch 4/10 | Batch 2120/2990 | Loss: 352.6780\n",
      "Epoch 4/10 | Batch 2140/2990 | Loss: 189.2077\n",
      "Epoch 4/10 | Batch 2160/2990 | Loss: 260.6358\n",
      "Epoch 4/10 | Batch 2180/2990 | Loss: 276.9807\n",
      "Epoch 4/10 | Batch 2200/2990 | Loss: 531.4110\n",
      "Epoch 4/10 | Batch 2220/2990 | Loss: 543.4661\n",
      "Epoch 4/10 | Batch 2240/2990 | Loss: 335.2541\n",
      "Epoch 4/10 | Batch 2260/2990 | Loss: 525.7235\n",
      "Epoch 4/10 | Batch 2280/2990 | Loss: 162.6274\n",
      "Epoch 4/10 | Batch 2300/2990 | Loss: 912.6991\n",
      "Epoch 4/10 | Batch 2320/2990 | Loss: 373.0251\n",
      "Epoch 4/10 | Batch 2340/2990 | Loss: 466.8751\n",
      "Epoch 4/10 | Batch 2360/2990 | Loss: 240.8003\n",
      "Epoch 4/10 | Batch 2380/2990 | Loss: 462.3375\n",
      "Epoch 4/10 | Batch 2400/2990 | Loss: 313.2284\n",
      "Epoch 4/10 | Batch 2420/2990 | Loss: 144.8466\n",
      "Epoch 4/10 | Batch 2440/2990 | Loss: 313.9877\n",
      "Epoch 4/10 | Batch 2460/2990 | Loss: 599.7516\n",
      "Epoch 4/10 | Batch 2480/2990 | Loss: 256.7314\n",
      "Epoch 4/10 | Batch 2500/2990 | Loss: 383.2928\n",
      "Epoch 4/10 | Batch 2520/2990 | Loss: 1084.2136\n",
      "Epoch 4/10 | Batch 2540/2990 | Loss: 527.0819\n",
      "Epoch 4/10 | Batch 2560/2990 | Loss: 930.2312\n",
      "Epoch 4/10 | Batch 2580/2990 | Loss: 114.6572\n",
      "Epoch 4/10 | Batch 2600/2990 | Loss: 833.9702\n",
      "Epoch 4/10 | Batch 2620/2990 | Loss: 430.9945\n",
      "Epoch 4/10 | Batch 2640/2990 | Loss: 604.9570\n",
      "Epoch 4/10 | Batch 2660/2990 | Loss: 653.4098\n",
      "Epoch 4/10 | Batch 2680/2990 | Loss: 372.9857\n",
      "Epoch 4/10 | Batch 2700/2990 | Loss: 703.0676\n",
      "Epoch 4/10 | Batch 2720/2990 | Loss: 539.2511\n",
      "Epoch 4/10 | Batch 2740/2990 | Loss: 186.5957\n",
      "Epoch 4/10 | Batch 2760/2990 | Loss: 279.0942\n",
      "Epoch 4/10 | Batch 2780/2990 | Loss: 717.1438\n",
      "Epoch 4/10 | Batch 2800/2990 | Loss: 322.6670\n",
      "Epoch 4/10 | Batch 2820/2990 | Loss: 555.0681\n",
      "Epoch 4/10 | Batch 2840/2990 | Loss: 362.5661\n",
      "Epoch 4/10 | Batch 2860/2990 | Loss: 470.0538\n",
      "Epoch 4/10 | Batch 2880/2990 | Loss: 708.3296\n",
      "Epoch 4/10 | Batch 2900/2990 | Loss: 450.8250\n",
      "Epoch 4/10 | Batch 2920/2990 | Loss: 232.3359\n",
      "Epoch 4/10 | Batch 2940/2990 | Loss: 287.1418\n",
      "Epoch 4/10 | Batch 2960/2990 | Loss: 118.6765\n",
      "Epoch 4/10 | Batch 2980/2990 | Loss: 437.0865\n",
      "✅ Epoch 4/10 | Train Loss: 471.0015 | Val Loss: 383.6682\n",
      "Epoch 5/10 | Batch 0/2990 | Loss: 412.0381\n",
      "Epoch 5/10 | Batch 20/2990 | Loss: 422.7573\n",
      "Epoch 5/10 | Batch 40/2990 | Loss: 540.7305\n",
      "Epoch 5/10 | Batch 60/2990 | Loss: 468.1123\n",
      "Epoch 5/10 | Batch 80/2990 | Loss: 429.4250\n",
      "Epoch 5/10 | Batch 100/2990 | Loss: 546.2642\n",
      "Epoch 5/10 | Batch 120/2990 | Loss: 362.4958\n",
      "Epoch 5/10 | Batch 140/2990 | Loss: 281.6473\n",
      "Epoch 5/10 | Batch 160/2990 | Loss: 269.1140\n",
      "Epoch 5/10 | Batch 180/2990 | Loss: 216.5439\n",
      "Epoch 5/10 | Batch 200/2990 | Loss: 299.1993\n",
      "Epoch 5/10 | Batch 220/2990 | Loss: 262.2333\n",
      "Epoch 5/10 | Batch 240/2990 | Loss: 630.3702\n",
      "Epoch 5/10 | Batch 260/2990 | Loss: 363.2099\n",
      "Epoch 5/10 | Batch 280/2990 | Loss: 1070.8010\n",
      "Epoch 5/10 | Batch 300/2990 | Loss: 505.9639\n",
      "Epoch 5/10 | Batch 320/2990 | Loss: 519.7450\n",
      "Epoch 5/10 | Batch 340/2990 | Loss: 151.3264\n",
      "Epoch 5/10 | Batch 360/2990 | Loss: 147.7670\n",
      "Epoch 5/10 | Batch 380/2990 | Loss: 146.3375\n",
      "Epoch 5/10 | Batch 400/2990 | Loss: 67.8752\n",
      "Epoch 5/10 | Batch 420/2990 | Loss: 373.4166\n",
      "Epoch 5/10 | Batch 440/2990 | Loss: 194.8142\n",
      "Epoch 5/10 | Batch 460/2990 | Loss: 246.9236\n",
      "Epoch 5/10 | Batch 480/2990 | Loss: 382.5162\n",
      "Epoch 5/10 | Batch 500/2990 | Loss: 252.5536\n",
      "Epoch 5/10 | Batch 520/2990 | Loss: 671.3340\n",
      "Epoch 5/10 | Batch 540/2990 | Loss: 322.7946\n",
      "Epoch 5/10 | Batch 560/2990 | Loss: 163.6252\n",
      "Epoch 5/10 | Batch 580/2990 | Loss: 128.3393\n",
      "Epoch 5/10 | Batch 600/2990 | Loss: 480.2147\n",
      "Epoch 5/10 | Batch 620/2990 | Loss: 665.4441\n",
      "Epoch 5/10 | Batch 640/2990 | Loss: 548.1869\n",
      "Epoch 5/10 | Batch 660/2990 | Loss: 585.9477\n",
      "Epoch 5/10 | Batch 680/2990 | Loss: 1350.4391\n",
      "Epoch 5/10 | Batch 700/2990 | Loss: 141.1223\n",
      "Epoch 5/10 | Batch 720/2990 | Loss: 1161.5067\n",
      "Epoch 5/10 | Batch 740/2990 | Loss: 495.0063\n",
      "Epoch 5/10 | Batch 760/2990 | Loss: 207.5627\n",
      "Epoch 5/10 | Batch 780/2990 | Loss: 98.5663\n",
      "Epoch 5/10 | Batch 800/2990 | Loss: 427.4090\n",
      "Epoch 5/10 | Batch 820/2990 | Loss: 250.9368\n",
      "Epoch 5/10 | Batch 840/2990 | Loss: 176.3968\n",
      "Epoch 5/10 | Batch 860/2990 | Loss: 276.7983\n",
      "Epoch 5/10 | Batch 880/2990 | Loss: 399.1976\n",
      "Epoch 5/10 | Batch 900/2990 | Loss: 537.1043\n",
      "Epoch 5/10 | Batch 920/2990 | Loss: 96.3925\n",
      "Epoch 5/10 | Batch 940/2990 | Loss: 278.3901\n",
      "Epoch 5/10 | Batch 960/2990 | Loss: 515.6010\n",
      "Epoch 5/10 | Batch 980/2990 | Loss: 245.2767\n",
      "Epoch 5/10 | Batch 1000/2990 | Loss: 310.7671\n",
      "Epoch 5/10 | Batch 1020/2990 | Loss: 229.9358\n",
      "Epoch 5/10 | Batch 1040/2990 | Loss: 228.1742\n",
      "Epoch 5/10 | Batch 1060/2990 | Loss: 238.6432\n",
      "Epoch 5/10 | Batch 1080/2990 | Loss: 371.7878\n",
      "Epoch 5/10 | Batch 1100/2990 | Loss: 460.9963\n",
      "Epoch 5/10 | Batch 1120/2990 | Loss: 188.8894\n",
      "Epoch 5/10 | Batch 1140/2990 | Loss: 501.2693\n",
      "Epoch 5/10 | Batch 1160/2990 | Loss: 104.2588\n",
      "Epoch 5/10 | Batch 1180/2990 | Loss: 1291.7925\n",
      "Epoch 5/10 | Batch 1200/2990 | Loss: 252.4954\n",
      "Epoch 5/10 | Batch 1220/2990 | Loss: 130.9257\n",
      "Epoch 5/10 | Batch 1240/2990 | Loss: 622.8585\n",
      "Epoch 5/10 | Batch 1260/2990 | Loss: 331.0791\n",
      "Epoch 5/10 | Batch 1280/2990 | Loss: 197.8482\n",
      "Epoch 5/10 | Batch 1300/2990 | Loss: 207.7854\n",
      "Epoch 5/10 | Batch 1320/2990 | Loss: 198.7875\n",
      "Epoch 5/10 | Batch 1340/2990 | Loss: 194.7587\n",
      "Epoch 5/10 | Batch 1360/2990 | Loss: 169.1871\n",
      "Epoch 5/10 | Batch 1380/2990 | Loss: 221.4873\n",
      "Epoch 5/10 | Batch 1400/2990 | Loss: 398.6896\n",
      "Epoch 5/10 | Batch 1420/2990 | Loss: 535.6923\n",
      "Epoch 5/10 | Batch 1440/2990 | Loss: 643.1906\n",
      "Epoch 5/10 | Batch 1460/2990 | Loss: 343.4406\n",
      "Epoch 5/10 | Batch 1480/2990 | Loss: 270.0662\n",
      "Epoch 5/10 | Batch 1500/2990 | Loss: 1030.7627\n",
      "Epoch 5/10 | Batch 1520/2990 | Loss: 381.4147\n",
      "Epoch 5/10 | Batch 1540/2990 | Loss: 800.1228\n",
      "Epoch 5/10 | Batch 1560/2990 | Loss: 605.3251\n",
      "Epoch 5/10 | Batch 1580/2990 | Loss: 286.0304\n",
      "Epoch 5/10 | Batch 1600/2990 | Loss: 351.9738\n",
      "Epoch 5/10 | Batch 1620/2990 | Loss: 760.7771\n",
      "Epoch 5/10 | Batch 1640/2990 | Loss: 183.1054\n",
      "Epoch 5/10 | Batch 1660/2990 | Loss: 273.4856\n",
      "Epoch 5/10 | Batch 1680/2990 | Loss: 291.3136\n",
      "Epoch 5/10 | Batch 1700/2990 | Loss: 430.5816\n",
      "Epoch 5/10 | Batch 1720/2990 | Loss: 121.4093\n",
      "Epoch 5/10 | Batch 1740/2990 | Loss: 251.3281\n",
      "Epoch 5/10 | Batch 1760/2990 | Loss: 622.1894\n",
      "Epoch 5/10 | Batch 1780/2990 | Loss: 127.1609\n",
      "Epoch 5/10 | Batch 1800/2990 | Loss: 426.8128\n",
      "Epoch 5/10 | Batch 1820/2990 | Loss: 346.0694\n",
      "Epoch 5/10 | Batch 1840/2990 | Loss: 156.6672\n",
      "Epoch 5/10 | Batch 1860/2990 | Loss: 178.2542\n",
      "Epoch 5/10 | Batch 1880/2990 | Loss: 293.5473\n",
      "Epoch 5/10 | Batch 1900/2990 | Loss: 524.6453\n",
      "Epoch 5/10 | Batch 1920/2990 | Loss: 200.4830\n",
      "Epoch 5/10 | Batch 1940/2990 | Loss: 468.4948\n",
      "Epoch 5/10 | Batch 1960/2990 | Loss: 219.3807\n",
      "Epoch 5/10 | Batch 1980/2990 | Loss: 200.0924\n",
      "Epoch 5/10 | Batch 2000/2990 | Loss: 261.2505\n",
      "Epoch 5/10 | Batch 2020/2990 | Loss: 465.9013\n",
      "Epoch 5/10 | Batch 2040/2990 | Loss: 347.1600\n",
      "Epoch 5/10 | Batch 2060/2990 | Loss: 252.1346\n",
      "Epoch 5/10 | Batch 2080/2990 | Loss: 341.5334\n",
      "Epoch 5/10 | Batch 2100/2990 | Loss: 399.0327\n",
      "Epoch 5/10 | Batch 2120/2990 | Loss: 386.0658\n",
      "Epoch 5/10 | Batch 2140/2990 | Loss: 281.6971\n",
      "Epoch 5/10 | Batch 2160/2990 | Loss: 207.8127\n",
      "Epoch 5/10 | Batch 2180/2990 | Loss: 749.6306\n",
      "Epoch 5/10 | Batch 2200/2990 | Loss: 494.5948\n",
      "Epoch 5/10 | Batch 2220/2990 | Loss: 419.0668\n",
      "Epoch 5/10 | Batch 2240/2990 | Loss: 447.9993\n",
      "Epoch 5/10 | Batch 2260/2990 | Loss: 424.0576\n",
      "Epoch 5/10 | Batch 2280/2990 | Loss: 311.2367\n",
      "Epoch 5/10 | Batch 2300/2990 | Loss: 562.6678\n",
      "Epoch 5/10 | Batch 2320/2990 | Loss: 174.5137\n",
      "Epoch 5/10 | Batch 2340/2990 | Loss: 233.9268\n",
      "Epoch 5/10 | Batch 2360/2990 | Loss: 263.4872\n",
      "Epoch 5/10 | Batch 2380/2990 | Loss: 713.1302\n",
      "Epoch 5/10 | Batch 2400/2990 | Loss: 395.5068\n",
      "Epoch 5/10 | Batch 2420/2990 | Loss: 309.0556\n",
      "Epoch 5/10 | Batch 2440/2990 | Loss: 292.7237\n",
      "Epoch 5/10 | Batch 2460/2990 | Loss: 388.0300\n",
      "Epoch 5/10 | Batch 2480/2990 | Loss: 453.5580\n",
      "Epoch 5/10 | Batch 2500/2990 | Loss: 402.2659\n",
      "Epoch 5/10 | Batch 2520/2990 | Loss: 187.8826\n",
      "Epoch 5/10 | Batch 2540/2990 | Loss: 511.5840\n",
      "Epoch 5/10 | Batch 2560/2990 | Loss: 263.7201\n",
      "Epoch 5/10 | Batch 2580/2990 | Loss: 242.9588\n",
      "Epoch 5/10 | Batch 2600/2990 | Loss: 742.0419\n",
      "Epoch 5/10 | Batch 2620/2990 | Loss: 199.7553\n",
      "Epoch 5/10 | Batch 2640/2990 | Loss: 321.5701\n",
      "Epoch 5/10 | Batch 2660/2990 | Loss: 229.9696\n",
      "Epoch 5/10 | Batch 2680/2990 | Loss: 772.9859\n",
      "Epoch 5/10 | Batch 2700/2990 | Loss: 286.3907\n",
      "Epoch 5/10 | Batch 2720/2990 | Loss: 118.9970\n",
      "Epoch 5/10 | Batch 2740/2990 | Loss: 336.1311\n",
      "Epoch 5/10 | Batch 2760/2990 | Loss: 537.2539\n",
      "Epoch 5/10 | Batch 2780/2990 | Loss: 599.7751\n",
      "Epoch 5/10 | Batch 2800/2990 | Loss: 370.6223\n",
      "Epoch 5/10 | Batch 2820/2990 | Loss: 251.0324\n",
      "Epoch 5/10 | Batch 2840/2990 | Loss: 202.6211\n",
      "Epoch 5/10 | Batch 2860/2990 | Loss: 305.7842\n",
      "Epoch 5/10 | Batch 2880/2990 | Loss: 1016.4164\n",
      "Epoch 5/10 | Batch 2900/2990 | Loss: 394.2403\n",
      "Epoch 5/10 | Batch 2920/2990 | Loss: 218.2838\n",
      "Epoch 5/10 | Batch 2940/2990 | Loss: 503.6810\n",
      "Epoch 5/10 | Batch 2960/2990 | Loss: 561.6976\n",
      "Epoch 5/10 | Batch 2980/2990 | Loss: 174.9402\n",
      "✅ Epoch 5/10 | Train Loss: 379.3517 | Val Loss: 345.1938\n",
      "Epoch 6/10 | Batch 0/2990 | Loss: 217.6184\n",
      "Epoch 6/10 | Batch 20/2990 | Loss: 310.2496\n",
      "Epoch 6/10 | Batch 40/2990 | Loss: 830.0266\n",
      "Epoch 6/10 | Batch 60/2990 | Loss: 243.0514\n",
      "Epoch 6/10 | Batch 80/2990 | Loss: 516.5671\n",
      "Epoch 6/10 | Batch 100/2990 | Loss: 351.5061\n",
      "Epoch 6/10 | Batch 120/2990 | Loss: 193.0616\n",
      "Epoch 6/10 | Batch 140/2990 | Loss: 281.0336\n",
      "Epoch 6/10 | Batch 160/2990 | Loss: 246.9949\n",
      "Epoch 6/10 | Batch 180/2990 | Loss: 269.2502\n",
      "Epoch 6/10 | Batch 200/2990 | Loss: 56.2274\n",
      "Epoch 6/10 | Batch 220/2990 | Loss: 183.0784\n",
      "Epoch 6/10 | Batch 240/2990 | Loss: 428.2227\n",
      "Epoch 6/10 | Batch 260/2990 | Loss: 482.5808\n",
      "Epoch 6/10 | Batch 280/2990 | Loss: 452.2566\n",
      "Epoch 6/10 | Batch 300/2990 | Loss: 227.5993\n",
      "Epoch 6/10 | Batch 320/2990 | Loss: 303.6163\n",
      "Epoch 6/10 | Batch 340/2990 | Loss: 219.0487\n",
      "Epoch 6/10 | Batch 360/2990 | Loss: 363.1518\n",
      "Epoch 6/10 | Batch 380/2990 | Loss: 410.3630\n",
      "Epoch 6/10 | Batch 400/2990 | Loss: 194.7746\n",
      "Epoch 6/10 | Batch 420/2990 | Loss: 174.0827\n",
      "Epoch 6/10 | Batch 440/2990 | Loss: 283.3382\n",
      "Epoch 6/10 | Batch 460/2990 | Loss: 779.0772\n",
      "Epoch 6/10 | Batch 480/2990 | Loss: 515.1298\n",
      "Epoch 6/10 | Batch 500/2990 | Loss: 135.3325\n",
      "Epoch 6/10 | Batch 520/2990 | Loss: 176.8407\n",
      "Epoch 6/10 | Batch 540/2990 | Loss: 397.2774\n",
      "Epoch 6/10 | Batch 560/2990 | Loss: 95.2959\n",
      "Epoch 6/10 | Batch 580/2990 | Loss: 367.9471\n",
      "Epoch 6/10 | Batch 600/2990 | Loss: 167.8228\n",
      "Epoch 6/10 | Batch 620/2990 | Loss: 161.5418\n",
      "Epoch 6/10 | Batch 640/2990 | Loss: 260.5490\n",
      "Epoch 6/10 | Batch 660/2990 | Loss: 205.8804\n",
      "Epoch 6/10 | Batch 680/2990 | Loss: 254.4991\n",
      "Epoch 6/10 | Batch 700/2990 | Loss: 221.4340\n",
      "Epoch 6/10 | Batch 720/2990 | Loss: 616.0206\n",
      "Epoch 6/10 | Batch 740/2990 | Loss: 407.8149\n",
      "Epoch 6/10 | Batch 760/2990 | Loss: 216.6296\n",
      "Epoch 6/10 | Batch 780/2990 | Loss: 422.7598\n",
      "Epoch 6/10 | Batch 800/2990 | Loss: 124.7615\n",
      "Epoch 6/10 | Batch 820/2990 | Loss: 394.9769\n",
      "Epoch 6/10 | Batch 840/2990 | Loss: 272.9099\n",
      "Epoch 6/10 | Batch 860/2990 | Loss: 605.2159\n",
      "Epoch 6/10 | Batch 880/2990 | Loss: 138.9865\n",
      "Epoch 6/10 | Batch 900/2990 | Loss: 576.2281\n",
      "Epoch 6/10 | Batch 920/2990 | Loss: 163.5438\n",
      "Epoch 6/10 | Batch 940/2990 | Loss: 319.6134\n",
      "Epoch 6/10 | Batch 960/2990 | Loss: 312.2877\n",
      "Epoch 6/10 | Batch 980/2990 | Loss: 247.5481\n",
      "Epoch 6/10 | Batch 1000/2990 | Loss: 392.6379\n",
      "Epoch 6/10 | Batch 1020/2990 | Loss: 179.5055\n",
      "Epoch 6/10 | Batch 1040/2990 | Loss: 170.1962\n",
      "Epoch 6/10 | Batch 1060/2990 | Loss: 340.4479\n",
      "Epoch 6/10 | Batch 1080/2990 | Loss: 170.0489\n",
      "Epoch 6/10 | Batch 1100/2990 | Loss: 485.9716\n",
      "Epoch 6/10 | Batch 1120/2990 | Loss: 652.3146\n",
      "Epoch 6/10 | Batch 1140/2990 | Loss: 111.1708\n",
      "Epoch 6/10 | Batch 1160/2990 | Loss: 189.7035\n",
      "Epoch 6/10 | Batch 1180/2990 | Loss: 188.0391\n",
      "Epoch 6/10 | Batch 1200/2990 | Loss: 442.0820\n",
      "Epoch 6/10 | Batch 1220/2990 | Loss: 417.5262\n",
      "Epoch 6/10 | Batch 1240/2990 | Loss: 264.1572\n",
      "Epoch 6/10 | Batch 1260/2990 | Loss: 261.3402\n",
      "Epoch 6/10 | Batch 1280/2990 | Loss: 88.7866\n",
      "Epoch 6/10 | Batch 1300/2990 | Loss: 501.7356\n",
      "Epoch 6/10 | Batch 1320/2990 | Loss: 192.5900\n",
      "Epoch 6/10 | Batch 1340/2990 | Loss: 328.7723\n",
      "Epoch 6/10 | Batch 1360/2990 | Loss: 776.9391\n",
      "Epoch 6/10 | Batch 1380/2990 | Loss: 250.3896\n",
      "Epoch 6/10 | Batch 1400/2990 | Loss: 248.1510\n",
      "Epoch 6/10 | Batch 1420/2990 | Loss: 287.7980\n",
      "Epoch 6/10 | Batch 1440/2990 | Loss: 417.6125\n",
      "Epoch 6/10 | Batch 1460/2990 | Loss: 371.4324\n",
      "Epoch 6/10 | Batch 1480/2990 | Loss: 176.7542\n",
      "Epoch 6/10 | Batch 1500/2990 | Loss: 436.6292\n",
      "Epoch 6/10 | Batch 1520/2990 | Loss: 215.9185\n",
      "Epoch 6/10 | Batch 1540/2990 | Loss: 280.7327\n",
      "Epoch 6/10 | Batch 1560/2990 | Loss: 476.7569\n",
      "Epoch 6/10 | Batch 1580/2990 | Loss: 310.0482\n",
      "Epoch 6/10 | Batch 1600/2990 | Loss: 221.2954\n",
      "Epoch 6/10 | Batch 1620/2990 | Loss: 434.8461\n",
      "Epoch 6/10 | Batch 1640/2990 | Loss: 262.3749\n",
      "Epoch 6/10 | Batch 1660/2990 | Loss: 213.5630\n",
      "Epoch 6/10 | Batch 1680/2990 | Loss: 330.5887\n",
      "Epoch 6/10 | Batch 1700/2990 | Loss: 157.4495\n",
      "Epoch 6/10 | Batch 1720/2990 | Loss: 179.9346\n",
      "Epoch 6/10 | Batch 1740/2990 | Loss: 514.3907\n",
      "Epoch 6/10 | Batch 1760/2990 | Loss: 104.1662\n",
      "Epoch 6/10 | Batch 1780/2990 | Loss: 208.2577\n",
      "Epoch 6/10 | Batch 1800/2990 | Loss: 154.3306\n",
      "Epoch 6/10 | Batch 1820/2990 | Loss: 214.0605\n",
      "Epoch 6/10 | Batch 1840/2990 | Loss: 127.2272\n",
      "Epoch 6/10 | Batch 1860/2990 | Loss: 255.6942\n",
      "Epoch 6/10 | Batch 1880/2990 | Loss: 292.8228\n",
      "Epoch 6/10 | Batch 1900/2990 | Loss: 182.6387\n",
      "Epoch 6/10 | Batch 1920/2990 | Loss: 220.0632\n",
      "Epoch 6/10 | Batch 1940/2990 | Loss: 69.5550\n",
      "Epoch 6/10 | Batch 1960/2990 | Loss: 133.3003\n",
      "Epoch 6/10 | Batch 1980/2990 | Loss: 162.2349\n",
      "Epoch 6/10 | Batch 2000/2990 | Loss: 475.7102\n",
      "Epoch 6/10 | Batch 2020/2990 | Loss: 319.3171\n",
      "Epoch 6/10 | Batch 2040/2990 | Loss: 133.7898\n",
      "Epoch 6/10 | Batch 2060/2990 | Loss: 641.1265\n",
      "Epoch 6/10 | Batch 2080/2990 | Loss: 294.1543\n",
      "Epoch 6/10 | Batch 2100/2990 | Loss: 203.5463\n",
      "Epoch 6/10 | Batch 2120/2990 | Loss: 153.2929\n",
      "Epoch 6/10 | Batch 2140/2990 | Loss: 314.8014\n",
      "Epoch 6/10 | Batch 2160/2990 | Loss: 131.7445\n",
      "Epoch 6/10 | Batch 2180/2990 | Loss: 325.1170\n",
      "Epoch 6/10 | Batch 2200/2990 | Loss: 378.2357\n",
      "Epoch 6/10 | Batch 2220/2990 | Loss: 173.2830\n",
      "Epoch 6/10 | Batch 2240/2990 | Loss: 171.1457\n",
      "Epoch 6/10 | Batch 2260/2990 | Loss: 461.9290\n",
      "Epoch 6/10 | Batch 2280/2990 | Loss: 140.4576\n",
      "Epoch 6/10 | Batch 2300/2990 | Loss: 115.4527\n",
      "Epoch 6/10 | Batch 2320/2990 | Loss: 263.5047\n",
      "Epoch 6/10 | Batch 2340/2990 | Loss: 291.8603\n",
      "Epoch 6/10 | Batch 2360/2990 | Loss: 155.6825\n",
      "Epoch 6/10 | Batch 2380/2990 | Loss: 179.9727\n",
      "Epoch 6/10 | Batch 2400/2990 | Loss: 309.5592\n",
      "Epoch 6/10 | Batch 2420/2990 | Loss: 531.4147\n",
      "Epoch 6/10 | Batch 2440/2990 | Loss: 213.1396\n",
      "Epoch 6/10 | Batch 2460/2990 | Loss: 141.9637\n",
      "Epoch 6/10 | Batch 2480/2990 | Loss: 91.5877\n",
      "Epoch 6/10 | Batch 2500/2990 | Loss: 495.5267\n",
      "Epoch 6/10 | Batch 2520/2990 | Loss: 226.5084\n",
      "Epoch 6/10 | Batch 2540/2990 | Loss: 221.5208\n",
      "Epoch 6/10 | Batch 2560/2990 | Loss: 265.8341\n",
      "Epoch 6/10 | Batch 2580/2990 | Loss: 419.3323\n",
      "Epoch 6/10 | Batch 2600/2990 | Loss: 291.6006\n",
      "Epoch 6/10 | Batch 2620/2990 | Loss: 179.4705\n",
      "Epoch 6/10 | Batch 2640/2990 | Loss: 301.6387\n",
      "Epoch 6/10 | Batch 2660/2990 | Loss: 346.0773\n",
      "Epoch 6/10 | Batch 2680/2990 | Loss: 228.6277\n",
      "Epoch 6/10 | Batch 2700/2990 | Loss: 876.0001\n",
      "Epoch 6/10 | Batch 2720/2990 | Loss: 436.8127\n",
      "Epoch 6/10 | Batch 2740/2990 | Loss: 145.2955\n",
      "Epoch 6/10 | Batch 2760/2990 | Loss: 145.3952\n",
      "Epoch 6/10 | Batch 2780/2990 | Loss: 328.2112\n",
      "Epoch 6/10 | Batch 2800/2990 | Loss: 142.7822\n",
      "Epoch 6/10 | Batch 2820/2990 | Loss: 401.8771\n",
      "Epoch 6/10 | Batch 2840/2990 | Loss: 178.1398\n",
      "Epoch 6/10 | Batch 2860/2990 | Loss: 147.3573\n",
      "Epoch 6/10 | Batch 2880/2990 | Loss: 234.3264\n",
      "Epoch 6/10 | Batch 2900/2990 | Loss: 361.8370\n",
      "Epoch 6/10 | Batch 2920/2990 | Loss: 716.4014\n",
      "Epoch 6/10 | Batch 2940/2990 | Loss: 313.1945\n",
      "Epoch 6/10 | Batch 2960/2990 | Loss: 702.0900\n",
      "Epoch 6/10 | Batch 2980/2990 | Loss: 703.6759\n",
      "✅ Epoch 6/10 | Train Loss: 303.8164 | Val Loss: 230.2391\n",
      "Epoch 7/10 | Batch 0/2990 | Loss: 289.4382\n",
      "Epoch 7/10 | Batch 20/2990 | Loss: 121.0349\n",
      "Epoch 7/10 | Batch 40/2990 | Loss: 233.8425\n",
      "Epoch 7/10 | Batch 60/2990 | Loss: 185.6012\n",
      "Epoch 7/10 | Batch 80/2990 | Loss: 89.5914\n",
      "Epoch 7/10 | Batch 100/2990 | Loss: 188.8893\n",
      "Epoch 7/10 | Batch 120/2990 | Loss: 99.9855\n",
      "Epoch 7/10 | Batch 140/2990 | Loss: 76.5280\n",
      "Epoch 7/10 | Batch 160/2990 | Loss: 417.6814\n",
      "Epoch 7/10 | Batch 180/2990 | Loss: 200.8311\n",
      "Epoch 7/10 | Batch 200/2990 | Loss: 201.2202\n",
      "Epoch 7/10 | Batch 220/2990 | Loss: 682.2460\n",
      "Epoch 7/10 | Batch 240/2990 | Loss: 386.9945\n",
      "Epoch 7/10 | Batch 260/2990 | Loss: 130.2151\n",
      "Epoch 7/10 | Batch 280/2990 | Loss: 218.3025\n",
      "Epoch 7/10 | Batch 300/2990 | Loss: 233.2124\n",
      "Epoch 7/10 | Batch 320/2990 | Loss: 96.1357\n",
      "Epoch 7/10 | Batch 340/2990 | Loss: 437.2722\n",
      "Epoch 7/10 | Batch 360/2990 | Loss: 142.5584\n",
      "Epoch 7/10 | Batch 380/2990 | Loss: 258.8426\n",
      "Epoch 7/10 | Batch 400/2990 | Loss: 266.9590\n",
      "Epoch 7/10 | Batch 420/2990 | Loss: 125.9884\n",
      "Epoch 7/10 | Batch 440/2990 | Loss: 219.8683\n",
      "Epoch 7/10 | Batch 460/2990 | Loss: 155.3774\n",
      "Epoch 7/10 | Batch 480/2990 | Loss: 50.3878\n",
      "Epoch 7/10 | Batch 500/2990 | Loss: 179.4868\n",
      "Epoch 7/10 | Batch 520/2990 | Loss: 97.1065\n",
      "Epoch 7/10 | Batch 540/2990 | Loss: 273.2554\n",
      "Epoch 7/10 | Batch 560/2990 | Loss: 150.4771\n",
      "Epoch 7/10 | Batch 580/2990 | Loss: 77.8955\n",
      "Epoch 7/10 | Batch 600/2990 | Loss: 241.6454\n",
      "Epoch 7/10 | Batch 620/2990 | Loss: 179.2268\n",
      "Epoch 7/10 | Batch 640/2990 | Loss: 193.9642\n",
      "Epoch 7/10 | Batch 660/2990 | Loss: 79.8773\n",
      "Epoch 7/10 | Batch 680/2990 | Loss: 99.8892\n",
      "Epoch 7/10 | Batch 700/2990 | Loss: 225.7533\n",
      "Epoch 7/10 | Batch 720/2990 | Loss: 258.3474\n",
      "Epoch 7/10 | Batch 740/2990 | Loss: 236.2973\n",
      "Epoch 7/10 | Batch 760/2990 | Loss: 279.3175\n",
      "Epoch 7/10 | Batch 780/2990 | Loss: 277.1730\n",
      "Epoch 7/10 | Batch 800/2990 | Loss: 298.0866\n",
      "Epoch 7/10 | Batch 820/2990 | Loss: 439.5602\n",
      "Epoch 7/10 | Batch 840/2990 | Loss: 198.0486\n",
      "Epoch 7/10 | Batch 860/2990 | Loss: 184.3316\n",
      "Epoch 7/10 | Batch 880/2990 | Loss: 372.3087\n",
      "Epoch 7/10 | Batch 900/2990 | Loss: 550.1922\n",
      "Epoch 7/10 | Batch 920/2990 | Loss: 172.5332\n",
      "Epoch 7/10 | Batch 940/2990 | Loss: 128.4121\n",
      "Epoch 7/10 | Batch 960/2990 | Loss: 251.7702\n",
      "Epoch 7/10 | Batch 980/2990 | Loss: 96.2037\n",
      "Epoch 7/10 | Batch 1000/2990 | Loss: 304.7771\n",
      "Epoch 7/10 | Batch 1020/2990 | Loss: 120.6226\n",
      "Epoch 7/10 | Batch 1040/2990 | Loss: 145.7886\n",
      "Epoch 7/10 | Batch 1060/2990 | Loss: 355.2602\n",
      "Epoch 7/10 | Batch 1080/2990 | Loss: 345.3523\n",
      "Epoch 7/10 | Batch 1100/2990 | Loss: 129.1137\n",
      "Epoch 7/10 | Batch 1120/2990 | Loss: 207.7555\n",
      "Epoch 7/10 | Batch 1140/2990 | Loss: 127.6788\n",
      "Epoch 7/10 | Batch 1160/2990 | Loss: 161.1683\n",
      "Epoch 7/10 | Batch 1180/2990 | Loss: 222.6068\n",
      "Epoch 7/10 | Batch 1200/2990 | Loss: 189.3828\n",
      "Epoch 7/10 | Batch 1220/2990 | Loss: 205.9221\n",
      "Epoch 7/10 | Batch 1240/2990 | Loss: 186.2619\n",
      "Epoch 7/10 | Batch 1260/2990 | Loss: 484.9872\n",
      "Epoch 7/10 | Batch 1280/2990 | Loss: 197.7526\n",
      "Epoch 7/10 | Batch 1300/2990 | Loss: 168.5524\n",
      "Epoch 7/10 | Batch 1320/2990 | Loss: 222.1628\n",
      "Epoch 7/10 | Batch 1340/2990 | Loss: 162.8081\n",
      "Epoch 7/10 | Batch 1360/2990 | Loss: 245.4639\n",
      "Epoch 7/10 | Batch 1380/2990 | Loss: 404.1127\n",
      "Epoch 7/10 | Batch 1400/2990 | Loss: 146.2356\n",
      "Epoch 7/10 | Batch 1420/2990 | Loss: 313.5930\n",
      "Epoch 7/10 | Batch 1440/2990 | Loss: 89.5100\n",
      "Epoch 7/10 | Batch 1460/2990 | Loss: 152.9848\n",
      "Epoch 7/10 | Batch 1480/2990 | Loss: 157.1955\n",
      "Epoch 7/10 | Batch 1500/2990 | Loss: 334.9055\n",
      "Epoch 7/10 | Batch 1520/2990 | Loss: 226.7706\n",
      "Epoch 7/10 | Batch 1540/2990 | Loss: 233.3577\n",
      "Epoch 7/10 | Batch 1560/2990 | Loss: 173.0490\n",
      "Epoch 7/10 | Batch 1580/2990 | Loss: 108.3041\n",
      "Epoch 7/10 | Batch 1600/2990 | Loss: 388.0074\n",
      "Epoch 7/10 | Batch 1620/2990 | Loss: 198.0671\n",
      "Epoch 7/10 | Batch 1640/2990 | Loss: 376.5911\n",
      "Epoch 7/10 | Batch 1660/2990 | Loss: 447.7709\n",
      "Epoch 7/10 | Batch 1680/2990 | Loss: 298.6753\n",
      "Epoch 7/10 | Batch 1700/2990 | Loss: 661.5693\n",
      "Epoch 7/10 | Batch 1720/2990 | Loss: 124.2291\n",
      "Epoch 7/10 | Batch 1740/2990 | Loss: 354.4501\n",
      "Epoch 7/10 | Batch 1760/2990 | Loss: 144.5647\n",
      "Epoch 7/10 | Batch 1780/2990 | Loss: 281.1822\n",
      "Epoch 7/10 | Batch 1800/2990 | Loss: 283.3161\n",
      "Epoch 7/10 | Batch 1820/2990 | Loss: 439.2169\n",
      "Epoch 7/10 | Batch 1840/2990 | Loss: 217.6246\n",
      "Epoch 7/10 | Batch 1860/2990 | Loss: 170.7635\n",
      "Epoch 7/10 | Batch 1880/2990 | Loss: 343.5529\n",
      "Epoch 7/10 | Batch 1900/2990 | Loss: 238.3535\n",
      "Epoch 7/10 | Batch 1920/2990 | Loss: 148.2014\n",
      "Epoch 7/10 | Batch 1940/2990 | Loss: 292.5350\n",
      "Epoch 7/10 | Batch 1960/2990 | Loss: 130.6815\n",
      "Epoch 7/10 | Batch 1980/2990 | Loss: 276.3776\n",
      "Epoch 7/10 | Batch 2000/2990 | Loss: 658.9468\n",
      "Epoch 7/10 | Batch 2020/2990 | Loss: 159.2118\n",
      "Epoch 7/10 | Batch 2040/2990 | Loss: 108.8371\n",
      "Epoch 7/10 | Batch 2060/2990 | Loss: 256.8818\n",
      "Epoch 7/10 | Batch 2080/2990 | Loss: 137.3792\n",
      "Epoch 7/10 | Batch 2100/2990 | Loss: 173.4220\n",
      "Epoch 7/10 | Batch 2120/2990 | Loss: 111.8654\n",
      "Epoch 7/10 | Batch 2140/2990 | Loss: 116.5799\n",
      "Epoch 7/10 | Batch 2160/2990 | Loss: 358.7577\n",
      "Epoch 7/10 | Batch 2180/2990 | Loss: 145.1157\n",
      "Epoch 7/10 | Batch 2200/2990 | Loss: 332.9877\n",
      "Epoch 7/10 | Batch 2220/2990 | Loss: 197.4513\n",
      "Epoch 7/10 | Batch 2240/2990 | Loss: 193.8713\n",
      "Epoch 7/10 | Batch 2260/2990 | Loss: 244.1002\n",
      "Epoch 7/10 | Batch 2280/2990 | Loss: 195.8292\n",
      "Epoch 7/10 | Batch 2300/2990 | Loss: 181.6354\n",
      "Epoch 7/10 | Batch 2320/2990 | Loss: 166.7973\n",
      "Epoch 7/10 | Batch 2340/2990 | Loss: 240.6483\n",
      "Epoch 7/10 | Batch 2360/2990 | Loss: 188.4332\n",
      "Epoch 7/10 | Batch 2380/2990 | Loss: 271.5738\n",
      "Epoch 7/10 | Batch 2400/2990 | Loss: 270.3461\n",
      "Epoch 7/10 | Batch 2420/2990 | Loss: 298.1695\n",
      "Epoch 7/10 | Batch 2440/2990 | Loss: 320.7392\n",
      "Epoch 7/10 | Batch 2460/2990 | Loss: 83.5160\n",
      "Epoch 7/10 | Batch 2480/2990 | Loss: 245.4076\n",
      "Epoch 7/10 | Batch 2500/2990 | Loss: 209.9099\n",
      "Epoch 7/10 | Batch 2520/2990 | Loss: 169.7147\n",
      "Epoch 7/10 | Batch 2540/2990 | Loss: 180.1304\n",
      "Epoch 7/10 | Batch 2560/2990 | Loss: 404.3201\n",
      "Epoch 7/10 | Batch 2580/2990 | Loss: 140.7016\n",
      "Epoch 7/10 | Batch 2600/2990 | Loss: 114.0332\n",
      "Epoch 7/10 | Batch 2620/2990 | Loss: 187.2688\n",
      "Epoch 7/10 | Batch 2640/2990 | Loss: 168.8470\n",
      "Epoch 7/10 | Batch 2660/2990 | Loss: 312.7264\n",
      "Epoch 7/10 | Batch 2680/2990 | Loss: 254.4257\n",
      "Epoch 7/10 | Batch 2700/2990 | Loss: 200.6114\n",
      "Epoch 7/10 | Batch 2720/2990 | Loss: 103.1937\n",
      "Epoch 7/10 | Batch 2740/2990 | Loss: 240.3902\n",
      "Epoch 7/10 | Batch 2760/2990 | Loss: 82.5757\n",
      "Epoch 7/10 | Batch 2780/2990 | Loss: 164.4507\n",
      "Epoch 7/10 | Batch 2800/2990 | Loss: 129.0720\n",
      "Epoch 7/10 | Batch 2820/2990 | Loss: 217.4765\n",
      "Epoch 7/10 | Batch 2840/2990 | Loss: 487.2003\n",
      "Epoch 7/10 | Batch 2860/2990 | Loss: 264.7977\n",
      "Epoch 7/10 | Batch 2880/2990 | Loss: 203.3343\n",
      "Epoch 7/10 | Batch 2900/2990 | Loss: 121.4554\n",
      "Epoch 7/10 | Batch 2920/2990 | Loss: 482.4492\n",
      "Epoch 7/10 | Batch 2940/2990 | Loss: 395.3870\n",
      "Epoch 7/10 | Batch 2960/2990 | Loss: 386.9634\n",
      "Epoch 7/10 | Batch 2980/2990 | Loss: 136.6113\n",
      "✅ Epoch 7/10 | Train Loss: 249.9569 | Val Loss: 183.3467\n",
      "Epoch 8/10 | Batch 0/2990 | Loss: 155.7708\n",
      "Epoch 8/10 | Batch 20/2990 | Loss: 132.7299\n",
      "Epoch 8/10 | Batch 40/2990 | Loss: 248.5077\n",
      "Epoch 8/10 | Batch 60/2990 | Loss: 468.9160\n",
      "Epoch 8/10 | Batch 80/2990 | Loss: 71.0063\n",
      "Epoch 8/10 | Batch 100/2990 | Loss: 259.2432\n",
      "Epoch 8/10 | Batch 120/2990 | Loss: 260.1528\n",
      "Epoch 8/10 | Batch 140/2990 | Loss: 160.0734\n",
      "Epoch 8/10 | Batch 160/2990 | Loss: 103.9847\n",
      "Epoch 8/10 | Batch 180/2990 | Loss: 126.4837\n",
      "Epoch 8/10 | Batch 200/2990 | Loss: 154.0495\n",
      "Epoch 8/10 | Batch 220/2990 | Loss: 254.1062\n",
      "Epoch 8/10 | Batch 240/2990 | Loss: 382.6913\n",
      "Epoch 8/10 | Batch 260/2990 | Loss: 384.4858\n",
      "Epoch 8/10 | Batch 280/2990 | Loss: 278.6393\n",
      "Epoch 8/10 | Batch 300/2990 | Loss: 125.7776\n",
      "Epoch 8/10 | Batch 320/2990 | Loss: 219.7036\n",
      "Epoch 8/10 | Batch 340/2990 | Loss: 330.4105\n",
      "Epoch 8/10 | Batch 360/2990 | Loss: 412.7548\n",
      "Epoch 8/10 | Batch 380/2990 | Loss: 209.6393\n",
      "Epoch 8/10 | Batch 400/2990 | Loss: 204.1636\n",
      "Epoch 8/10 | Batch 420/2990 | Loss: 371.9741\n",
      "Epoch 8/10 | Batch 440/2990 | Loss: 118.5649\n",
      "Epoch 8/10 | Batch 460/2990 | Loss: 541.4797\n",
      "Epoch 8/10 | Batch 480/2990 | Loss: 225.3714\n",
      "Epoch 8/10 | Batch 500/2990 | Loss: 362.8170\n",
      "Epoch 8/10 | Batch 520/2990 | Loss: 294.0634\n",
      "Epoch 8/10 | Batch 540/2990 | Loss: 216.7553\n",
      "Epoch 8/10 | Batch 560/2990 | Loss: 190.7178\n",
      "Epoch 8/10 | Batch 580/2990 | Loss: 162.3337\n",
      "Epoch 8/10 | Batch 600/2990 | Loss: 194.9886\n",
      "Epoch 8/10 | Batch 620/2990 | Loss: 151.9404\n",
      "Epoch 8/10 | Batch 640/2990 | Loss: 348.1880\n",
      "Epoch 8/10 | Batch 660/2990 | Loss: 161.5037\n",
      "Epoch 8/10 | Batch 680/2990 | Loss: 322.6841\n",
      "Epoch 8/10 | Batch 700/2990 | Loss: 189.1829\n",
      "Epoch 8/10 | Batch 720/2990 | Loss: 310.8744\n",
      "Epoch 8/10 | Batch 740/2990 | Loss: 156.2895\n",
      "Epoch 8/10 | Batch 760/2990 | Loss: 249.8769\n",
      "Epoch 8/10 | Batch 780/2990 | Loss: 204.8630\n",
      "Epoch 8/10 | Batch 800/2990 | Loss: 305.8516\n",
      "Epoch 8/10 | Batch 820/2990 | Loss: 110.1038\n",
      "Epoch 8/10 | Batch 840/2990 | Loss: 208.7961\n",
      "Epoch 8/10 | Batch 860/2990 | Loss: 183.4034\n",
      "Epoch 8/10 | Batch 880/2990 | Loss: 176.7051\n",
      "Epoch 8/10 | Batch 900/2990 | Loss: 135.6049\n",
      "Epoch 8/10 | Batch 920/2990 | Loss: 148.8366\n",
      "Epoch 8/10 | Batch 940/2990 | Loss: 95.1950\n",
      "Epoch 8/10 | Batch 960/2990 | Loss: 92.9255\n",
      "Epoch 8/10 | Batch 980/2990 | Loss: 196.2601\n",
      "Epoch 8/10 | Batch 1000/2990 | Loss: 94.8614\n",
      "Epoch 8/10 | Batch 1020/2990 | Loss: 62.4314\n",
      "Epoch 8/10 | Batch 1040/2990 | Loss: 329.2842\n",
      "Epoch 8/10 | Batch 1060/2990 | Loss: 287.2349\n",
      "Epoch 8/10 | Batch 1080/2990 | Loss: 248.1126\n",
      "Epoch 8/10 | Batch 1100/2990 | Loss: 141.6001\n",
      "Epoch 8/10 | Batch 1120/2990 | Loss: 260.7781\n",
      "Epoch 8/10 | Batch 1140/2990 | Loss: 71.8195\n",
      "Epoch 8/10 | Batch 1160/2990 | Loss: 138.1944\n",
      "Epoch 8/10 | Batch 1180/2990 | Loss: 134.2841\n",
      "Epoch 8/10 | Batch 1200/2990 | Loss: 87.7470\n",
      "Epoch 8/10 | Batch 1220/2990 | Loss: 425.6782\n",
      "Epoch 8/10 | Batch 1240/2990 | Loss: 213.9980\n",
      "Epoch 8/10 | Batch 1260/2990 | Loss: 117.2381\n",
      "Epoch 8/10 | Batch 1280/2990 | Loss: 134.3572\n",
      "Epoch 8/10 | Batch 1300/2990 | Loss: 199.7640\n",
      "Epoch 8/10 | Batch 1320/2990 | Loss: 81.0722\n",
      "Epoch 8/10 | Batch 1340/2990 | Loss: 61.6818\n",
      "Epoch 8/10 | Batch 1360/2990 | Loss: 139.9487\n",
      "Epoch 8/10 | Batch 1380/2990 | Loss: 65.0318\n",
      "Epoch 8/10 | Batch 1400/2990 | Loss: 227.4135\n",
      "Epoch 8/10 | Batch 1420/2990 | Loss: 373.9789\n",
      "Epoch 8/10 | Batch 1440/2990 | Loss: 152.4953\n",
      "Epoch 8/10 | Batch 1460/2990 | Loss: 240.6976\n",
      "Epoch 8/10 | Batch 1480/2990 | Loss: 87.9246\n",
      "Epoch 8/10 | Batch 1500/2990 | Loss: 212.9449\n",
      "Epoch 8/10 | Batch 1520/2990 | Loss: 115.9983\n",
      "Epoch 8/10 | Batch 1540/2990 | Loss: 244.0918\n",
      "Epoch 8/10 | Batch 1560/2990 | Loss: 297.0440\n",
      "Epoch 8/10 | Batch 1580/2990 | Loss: 159.9158\n",
      "Epoch 8/10 | Batch 1600/2990 | Loss: 180.4321\n",
      "Epoch 8/10 | Batch 1620/2990 | Loss: 246.0614\n",
      "Epoch 8/10 | Batch 1640/2990 | Loss: 151.0641\n",
      "Epoch 8/10 | Batch 1660/2990 | Loss: 138.6989\n",
      "Epoch 8/10 | Batch 1680/2990 | Loss: 248.9266\n",
      "Epoch 8/10 | Batch 1700/2990 | Loss: 237.7332\n",
      "Epoch 8/10 | Batch 1720/2990 | Loss: 209.7698\n",
      "Epoch 8/10 | Batch 1740/2990 | Loss: 145.7631\n",
      "Epoch 8/10 | Batch 1760/2990 | Loss: 102.9445\n",
      "Epoch 8/10 | Batch 1780/2990 | Loss: 429.2017\n",
      "Epoch 8/10 | Batch 1800/2990 | Loss: 222.0618\n",
      "Epoch 8/10 | Batch 1820/2990 | Loss: 85.1562\n",
      "Epoch 8/10 | Batch 1840/2990 | Loss: 191.6074\n",
      "Epoch 8/10 | Batch 1860/2990 | Loss: 203.5644\n",
      "Epoch 8/10 | Batch 1880/2990 | Loss: 191.5282\n",
      "Epoch 8/10 | Batch 1900/2990 | Loss: 135.8944\n",
      "Epoch 8/10 | Batch 1920/2990 | Loss: 163.2451\n",
      "Epoch 8/10 | Batch 1940/2990 | Loss: 172.9033\n",
      "Epoch 8/10 | Batch 1960/2990 | Loss: 319.4973\n",
      "Epoch 8/10 | Batch 1980/2990 | Loss: 149.2645\n",
      "Epoch 8/10 | Batch 2000/2990 | Loss: 295.4935\n",
      "Epoch 8/10 | Batch 2020/2990 | Loss: 290.5397\n",
      "Epoch 8/10 | Batch 2040/2990 | Loss: 202.9972\n",
      "Epoch 8/10 | Batch 2060/2990 | Loss: 200.1768\n",
      "Epoch 8/10 | Batch 2080/2990 | Loss: 95.1160\n",
      "Epoch 8/10 | Batch 2100/2990 | Loss: 114.9877\n",
      "Epoch 8/10 | Batch 2120/2990 | Loss: 147.6793\n",
      "Epoch 8/10 | Batch 2140/2990 | Loss: 97.1748\n",
      "Epoch 8/10 | Batch 2160/2990 | Loss: 117.5094\n",
      "Epoch 8/10 | Batch 2180/2990 | Loss: 147.6525\n",
      "Epoch 8/10 | Batch 2200/2990 | Loss: 263.5524\n",
      "Epoch 8/10 | Batch 2220/2990 | Loss: 170.8409\n",
      "Epoch 8/10 | Batch 2240/2990 | Loss: 325.1502\n",
      "Epoch 8/10 | Batch 2260/2990 | Loss: 148.1552\n",
      "Epoch 8/10 | Batch 2280/2990 | Loss: 333.3984\n",
      "Epoch 8/10 | Batch 2300/2990 | Loss: 248.1897\n",
      "Epoch 8/10 | Batch 2320/2990 | Loss: 200.0800\n",
      "Epoch 8/10 | Batch 2340/2990 | Loss: 199.2612\n",
      "Epoch 8/10 | Batch 2360/2990 | Loss: 389.9381\n",
      "Epoch 8/10 | Batch 2380/2990 | Loss: 196.4706\n",
      "Epoch 8/10 | Batch 2400/2990 | Loss: 81.2790\n",
      "Epoch 8/10 | Batch 2420/2990 | Loss: 138.8624\n",
      "Epoch 8/10 | Batch 2440/2990 | Loss: 110.8558\n",
      "Epoch 8/10 | Batch 2460/2990 | Loss: 167.0484\n",
      "Epoch 8/10 | Batch 2480/2990 | Loss: 107.8045\n",
      "Epoch 8/10 | Batch 2500/2990 | Loss: 146.9890\n",
      "Epoch 8/10 | Batch 2520/2990 | Loss: 252.6071\n",
      "Epoch 8/10 | Batch 2540/2990 | Loss: 90.3174\n",
      "Epoch 8/10 | Batch 2560/2990 | Loss: 169.4248\n",
      "Epoch 8/10 | Batch 2580/2990 | Loss: 279.7185\n",
      "Epoch 8/10 | Batch 2600/2990 | Loss: 337.3488\n",
      "Epoch 8/10 | Batch 2620/2990 | Loss: 242.7029\n",
      "Epoch 8/10 | Batch 2640/2990 | Loss: 133.0263\n",
      "Epoch 8/10 | Batch 2660/2990 | Loss: 60.1281\n",
      "Epoch 8/10 | Batch 2680/2990 | Loss: 164.9658\n",
      "Epoch 8/10 | Batch 2700/2990 | Loss: 231.2927\n",
      "Epoch 8/10 | Batch 2720/2990 | Loss: 186.2083\n",
      "Epoch 8/10 | Batch 2740/2990 | Loss: 260.3978\n",
      "Epoch 8/10 | Batch 2760/2990 | Loss: 94.0916\n",
      "Epoch 8/10 | Batch 2780/2990 | Loss: 256.4377\n",
      "Epoch 8/10 | Batch 2800/2990 | Loss: 254.1563\n",
      "Epoch 8/10 | Batch 2820/2990 | Loss: 112.5934\n",
      "Epoch 8/10 | Batch 2840/2990 | Loss: 125.2257\n",
      "Epoch 8/10 | Batch 2860/2990 | Loss: 203.9392\n",
      "Epoch 8/10 | Batch 2880/2990 | Loss: 219.7936\n",
      "Epoch 8/10 | Batch 2900/2990 | Loss: 181.8405\n",
      "Epoch 8/10 | Batch 2920/2990 | Loss: 184.8109\n",
      "Epoch 8/10 | Batch 2940/2990 | Loss: 197.5875\n",
      "Epoch 8/10 | Batch 2960/2990 | Loss: 182.4296\n",
      "Epoch 8/10 | Batch 2980/2990 | Loss: 164.0919\n",
      "✅ Epoch 8/10 | Train Loss: 215.8390 | Val Loss: 239.8879\n",
      "Epoch 9/10 | Batch 0/2990 | Loss: 268.1666\n",
      "Epoch 9/10 | Batch 20/2990 | Loss: 239.9231\n",
      "Epoch 9/10 | Batch 40/2990 | Loss: 173.6107\n",
      "Epoch 9/10 | Batch 60/2990 | Loss: 84.5355\n",
      "Epoch 9/10 | Batch 80/2990 | Loss: 136.9503\n",
      "Epoch 9/10 | Batch 100/2990 | Loss: 331.1369\n",
      "Epoch 9/10 | Batch 120/2990 | Loss: 78.7911\n",
      "Epoch 9/10 | Batch 140/2990 | Loss: 119.6676\n",
      "Epoch 9/10 | Batch 160/2990 | Loss: 155.7844\n",
      "Epoch 9/10 | Batch 180/2990 | Loss: 354.6852\n",
      "Epoch 9/10 | Batch 200/2990 | Loss: 119.2778\n",
      "Epoch 9/10 | Batch 220/2990 | Loss: 303.7300\n",
      "Epoch 9/10 | Batch 240/2990 | Loss: 226.0749\n",
      "Epoch 9/10 | Batch 260/2990 | Loss: 181.1033\n",
      "Epoch 9/10 | Batch 280/2990 | Loss: 198.8314\n",
      "Epoch 9/10 | Batch 300/2990 | Loss: 155.1605\n",
      "Epoch 9/10 | Batch 320/2990 | Loss: 84.6741\n",
      "Epoch 9/10 | Batch 340/2990 | Loss: 117.6899\n",
      "Epoch 9/10 | Batch 360/2990 | Loss: 194.4984\n",
      "Epoch 9/10 | Batch 380/2990 | Loss: 251.4782\n",
      "Epoch 9/10 | Batch 400/2990 | Loss: 127.1031\n",
      "Epoch 9/10 | Batch 420/2990 | Loss: 201.1051\n",
      "Epoch 9/10 | Batch 440/2990 | Loss: 209.0891\n",
      "Epoch 9/10 | Batch 460/2990 | Loss: 304.3016\n",
      "Epoch 9/10 | Batch 480/2990 | Loss: 139.0925\n",
      "Epoch 9/10 | Batch 500/2990 | Loss: 849.3171\n",
      "Epoch 9/10 | Batch 520/2990 | Loss: 286.4711\n",
      "Epoch 9/10 | Batch 540/2990 | Loss: 119.1701\n",
      "Epoch 9/10 | Batch 560/2990 | Loss: 174.7155\n",
      "Epoch 9/10 | Batch 580/2990 | Loss: 178.6349\n",
      "Epoch 9/10 | Batch 600/2990 | Loss: 196.2890\n",
      "Epoch 9/10 | Batch 620/2990 | Loss: 164.5941\n",
      "Epoch 9/10 | Batch 640/2990 | Loss: 130.4543\n",
      "Epoch 9/10 | Batch 660/2990 | Loss: 109.0384\n",
      "Epoch 9/10 | Batch 680/2990 | Loss: 386.7124\n",
      "Epoch 9/10 | Batch 700/2990 | Loss: 339.1239\n",
      "Epoch 9/10 | Batch 720/2990 | Loss: 86.4518\n",
      "Epoch 9/10 | Batch 740/2990 | Loss: 91.4053\n",
      "Epoch 9/10 | Batch 760/2990 | Loss: 87.7742\n",
      "Epoch 9/10 | Batch 780/2990 | Loss: 116.1510\n",
      "Epoch 9/10 | Batch 800/2990 | Loss: 90.6271\n",
      "Epoch 9/10 | Batch 820/2990 | Loss: 160.6765\n",
      "Epoch 9/10 | Batch 840/2990 | Loss: 84.7154\n",
      "Epoch 9/10 | Batch 860/2990 | Loss: 113.3559\n",
      "Epoch 9/10 | Batch 880/2990 | Loss: 227.7789\n",
      "Epoch 9/10 | Batch 900/2990 | Loss: 236.0172\n",
      "Epoch 9/10 | Batch 920/2990 | Loss: 272.3112\n",
      "Epoch 9/10 | Batch 940/2990 | Loss: 193.5721\n",
      "Epoch 9/10 | Batch 960/2990 | Loss: 131.2447\n",
      "Epoch 9/10 | Batch 980/2990 | Loss: 182.7508\n",
      "Epoch 9/10 | Batch 1000/2990 | Loss: 103.7390\n",
      "Epoch 9/10 | Batch 1020/2990 | Loss: 230.6364\n",
      "Epoch 9/10 | Batch 1040/2990 | Loss: 130.4095\n",
      "Epoch 9/10 | Batch 1060/2990 | Loss: 233.8293\n",
      "Epoch 9/10 | Batch 1080/2990 | Loss: 86.6637\n",
      "Epoch 9/10 | Batch 1100/2990 | Loss: 212.1337\n",
      "Epoch 9/10 | Batch 1120/2990 | Loss: 188.4963\n",
      "Epoch 9/10 | Batch 1140/2990 | Loss: 172.6110\n",
      "Epoch 9/10 | Batch 1160/2990 | Loss: 487.0189\n",
      "Epoch 9/10 | Batch 1180/2990 | Loss: 228.9438\n",
      "Epoch 9/10 | Batch 1200/2990 | Loss: 420.9388\n",
      "Epoch 9/10 | Batch 1220/2990 | Loss: 153.4561\n",
      "Epoch 9/10 | Batch 1240/2990 | Loss: 162.3112\n",
      "Epoch 9/10 | Batch 1260/2990 | Loss: 173.2904\n",
      "Epoch 9/10 | Batch 1280/2990 | Loss: 99.9370\n",
      "Epoch 9/10 | Batch 1300/2990 | Loss: 372.4164\n",
      "Epoch 9/10 | Batch 1320/2990 | Loss: 152.9144\n",
      "Epoch 9/10 | Batch 1340/2990 | Loss: 87.4449\n",
      "Epoch 9/10 | Batch 1360/2990 | Loss: 124.4124\n",
      "Epoch 9/10 | Batch 1380/2990 | Loss: 155.5653\n",
      "Epoch 9/10 | Batch 1400/2990 | Loss: 348.4173\n",
      "Epoch 9/10 | Batch 1420/2990 | Loss: 293.5952\n",
      "Epoch 9/10 | Batch 1440/2990 | Loss: 194.2631\n",
      "Epoch 9/10 | Batch 1460/2990 | Loss: 125.9091\n",
      "Epoch 9/10 | Batch 1480/2990 | Loss: 117.8992\n",
      "Epoch 9/10 | Batch 1500/2990 | Loss: 177.7742\n",
      "Epoch 9/10 | Batch 1520/2990 | Loss: 247.2403\n",
      "Epoch 9/10 | Batch 1540/2990 | Loss: 150.9282\n",
      "Epoch 9/10 | Batch 1560/2990 | Loss: 170.4733\n",
      "Epoch 9/10 | Batch 1580/2990 | Loss: 114.3247\n",
      "Epoch 9/10 | Batch 1600/2990 | Loss: 85.5270\n",
      "Epoch 9/10 | Batch 1620/2990 | Loss: 295.2338\n",
      "Epoch 9/10 | Batch 1640/2990 | Loss: 148.8905\n",
      "Epoch 9/10 | Batch 1660/2990 | Loss: 152.7595\n",
      "Epoch 9/10 | Batch 1680/2990 | Loss: 362.6769\n",
      "Epoch 9/10 | Batch 1700/2990 | Loss: 323.3838\n",
      "Epoch 9/10 | Batch 1720/2990 | Loss: 161.1709\n",
      "Epoch 9/10 | Batch 1740/2990 | Loss: 76.4671\n",
      "Epoch 9/10 | Batch 1760/2990 | Loss: 95.2567\n",
      "Epoch 9/10 | Batch 1780/2990 | Loss: 194.0999\n",
      "Epoch 9/10 | Batch 1800/2990 | Loss: 163.2270\n",
      "Epoch 9/10 | Batch 1820/2990 | Loss: 136.2728\n",
      "Epoch 9/10 | Batch 1840/2990 | Loss: 184.8051\n",
      "Epoch 9/10 | Batch 1860/2990 | Loss: 143.8390\n",
      "Epoch 9/10 | Batch 1880/2990 | Loss: 69.6903\n",
      "Epoch 9/10 | Batch 1900/2990 | Loss: 338.9188\n",
      "Epoch 9/10 | Batch 1920/2990 | Loss: 188.1600\n",
      "Epoch 9/10 | Batch 1940/2990 | Loss: 149.1980\n",
      "Epoch 9/10 | Batch 1960/2990 | Loss: 202.2538\n",
      "Epoch 9/10 | Batch 1980/2990 | Loss: 89.3155\n",
      "Epoch 9/10 | Batch 2000/2990 | Loss: 154.7819\n",
      "Epoch 9/10 | Batch 2020/2990 | Loss: 200.4341\n",
      "Epoch 9/10 | Batch 2040/2990 | Loss: 358.1287\n",
      "Epoch 9/10 | Batch 2060/2990 | Loss: 125.3722\n",
      "Epoch 9/10 | Batch 2080/2990 | Loss: 98.2431\n",
      "Epoch 9/10 | Batch 2100/2990 | Loss: 90.9438\n",
      "Epoch 9/10 | Batch 2120/2990 | Loss: 125.5542\n",
      "Epoch 9/10 | Batch 2140/2990 | Loss: 257.3570\n",
      "Epoch 9/10 | Batch 2160/2990 | Loss: 489.4911\n",
      "Epoch 9/10 | Batch 2180/2990 | Loss: 217.8748\n",
      "Epoch 9/10 | Batch 2200/2990 | Loss: 135.0135\n",
      "Epoch 9/10 | Batch 2220/2990 | Loss: 479.2474\n",
      "Epoch 9/10 | Batch 2240/2990 | Loss: 178.3079\n",
      "Epoch 9/10 | Batch 2260/2990 | Loss: 265.7814\n",
      "Epoch 9/10 | Batch 2280/2990 | Loss: 168.7368\n",
      "Epoch 9/10 | Batch 2300/2990 | Loss: 228.6766\n",
      "Epoch 9/10 | Batch 2320/2990 | Loss: 154.0744\n",
      "Epoch 9/10 | Batch 2340/2990 | Loss: 207.0474\n",
      "Epoch 9/10 | Batch 2360/2990 | Loss: 174.3651\n",
      "Epoch 9/10 | Batch 2380/2990 | Loss: 303.2716\n",
      "Epoch 9/10 | Batch 2400/2990 | Loss: 149.3724\n",
      "Epoch 9/10 | Batch 2420/2990 | Loss: 361.4724\n",
      "Epoch 9/10 | Batch 2440/2990 | Loss: 76.5648\n",
      "Epoch 9/10 | Batch 2460/2990 | Loss: 67.1042\n",
      "Epoch 9/10 | Batch 2480/2990 | Loss: 262.4634\n",
      "Epoch 9/10 | Batch 2500/2990 | Loss: 169.0313\n",
      "Epoch 9/10 | Batch 2520/2990 | Loss: 166.1316\n",
      "Epoch 9/10 | Batch 2540/2990 | Loss: 106.5778\n",
      "Epoch 9/10 | Batch 2560/2990 | Loss: 144.8165\n",
      "Epoch 9/10 | Batch 2580/2990 | Loss: 205.0136\n",
      "Epoch 9/10 | Batch 2600/2990 | Loss: 229.2680\n",
      "Epoch 9/10 | Batch 2620/2990 | Loss: 180.8049\n",
      "Epoch 9/10 | Batch 2640/2990 | Loss: 146.1472\n",
      "Epoch 9/10 | Batch 2660/2990 | Loss: 59.9553\n",
      "Epoch 9/10 | Batch 2680/2990 | Loss: 198.6182\n",
      "Epoch 9/10 | Batch 2700/2990 | Loss: 194.2563\n",
      "Epoch 9/10 | Batch 2720/2990 | Loss: 55.6648\n",
      "Epoch 9/10 | Batch 2740/2990 | Loss: 163.5075\n",
      "Epoch 9/10 | Batch 2760/2990 | Loss: 116.8874\n",
      "Epoch 9/10 | Batch 2780/2990 | Loss: 190.8047\n",
      "Epoch 9/10 | Batch 2800/2990 | Loss: 95.1043\n",
      "Epoch 9/10 | Batch 2820/2990 | Loss: 148.7183\n",
      "Epoch 9/10 | Batch 2840/2990 | Loss: 112.5203\n",
      "Epoch 9/10 | Batch 2860/2990 | Loss: 92.1873\n",
      "Epoch 9/10 | Batch 2880/2990 | Loss: 69.1730\n",
      "Epoch 9/10 | Batch 2900/2990 | Loss: 191.9453\n",
      "Epoch 9/10 | Batch 2920/2990 | Loss: 170.0184\n",
      "Epoch 9/10 | Batch 2940/2990 | Loss: 173.9815\n",
      "Epoch 9/10 | Batch 2960/2990 | Loss: 183.8904\n",
      "Epoch 9/10 | Batch 2980/2990 | Loss: 56.2833\n",
      "✅ Epoch 9/10 | Train Loss: 190.5317 | Val Loss: 180.2335\n",
      "Epoch 10/10 | Batch 0/2990 | Loss: 150.0257\n",
      "Epoch 10/10 | Batch 20/2990 | Loss: 116.9793\n",
      "Epoch 10/10 | Batch 40/2990 | Loss: 275.9224\n",
      "Epoch 10/10 | Batch 60/2990 | Loss: 103.4952\n",
      "Epoch 10/10 | Batch 80/2990 | Loss: 106.1514\n",
      "Epoch 10/10 | Batch 100/2990 | Loss: 190.4357\n",
      "Epoch 10/10 | Batch 120/2990 | Loss: 163.6537\n",
      "Epoch 10/10 | Batch 140/2990 | Loss: 92.3413\n",
      "Epoch 10/10 | Batch 160/2990 | Loss: 112.0061\n",
      "Epoch 10/10 | Batch 180/2990 | Loss: 110.3251\n",
      "Epoch 10/10 | Batch 200/2990 | Loss: 351.7340\n",
      "Epoch 10/10 | Batch 220/2990 | Loss: 179.2140\n",
      "Epoch 10/10 | Batch 240/2990 | Loss: 39.3940\n",
      "Epoch 10/10 | Batch 260/2990 | Loss: 376.1424\n",
      "Epoch 10/10 | Batch 280/2990 | Loss: 140.9411\n",
      "Epoch 10/10 | Batch 300/2990 | Loss: 110.3155\n",
      "Epoch 10/10 | Batch 320/2990 | Loss: 110.8975\n",
      "Epoch 10/10 | Batch 340/2990 | Loss: 111.2410\n",
      "Epoch 10/10 | Batch 360/2990 | Loss: 183.4281\n",
      "Epoch 10/10 | Batch 380/2990 | Loss: 153.4108\n",
      "Epoch 10/10 | Batch 400/2990 | Loss: 50.9071\n",
      "Epoch 10/10 | Batch 420/2990 | Loss: 124.3885\n",
      "Epoch 10/10 | Batch 440/2990 | Loss: 115.2514\n",
      "Epoch 10/10 | Batch 460/2990 | Loss: 171.1239\n",
      "Epoch 10/10 | Batch 480/2990 | Loss: 79.3135\n",
      "Epoch 10/10 | Batch 500/2990 | Loss: 214.4146\n",
      "Epoch 10/10 | Batch 520/2990 | Loss: 127.1420\n",
      "Epoch 10/10 | Batch 540/2990 | Loss: 121.4223\n",
      "Epoch 10/10 | Batch 560/2990 | Loss: 310.1391\n",
      "Epoch 10/10 | Batch 580/2990 | Loss: 277.9371\n",
      "Epoch 10/10 | Batch 600/2990 | Loss: 240.5147\n",
      "Epoch 10/10 | Batch 620/2990 | Loss: 75.3358\n",
      "Epoch 10/10 | Batch 640/2990 | Loss: 215.9535\n",
      "Epoch 10/10 | Batch 660/2990 | Loss: 228.4756\n",
      "Epoch 10/10 | Batch 680/2990 | Loss: 119.5511\n",
      "Epoch 10/10 | Batch 700/2990 | Loss: 385.0857\n",
      "Epoch 10/10 | Batch 720/2990 | Loss: 77.8890\n",
      "Epoch 10/10 | Batch 740/2990 | Loss: 213.2092\n",
      "Epoch 10/10 | Batch 760/2990 | Loss: 131.1386\n",
      "Epoch 10/10 | Batch 780/2990 | Loss: 89.7946\n",
      "Epoch 10/10 | Batch 800/2990 | Loss: 219.8982\n",
      "Epoch 10/10 | Batch 820/2990 | Loss: 81.7226\n",
      "Epoch 10/10 | Batch 840/2990 | Loss: 149.7931\n",
      "Epoch 10/10 | Batch 860/2990 | Loss: 209.3144\n",
      "Epoch 10/10 | Batch 880/2990 | Loss: 167.1347\n",
      "Epoch 10/10 | Batch 900/2990 | Loss: 155.9017\n",
      "Epoch 10/10 | Batch 920/2990 | Loss: 59.9028\n",
      "Epoch 10/10 | Batch 940/2990 | Loss: 112.4416\n",
      "Epoch 10/10 | Batch 960/2990 | Loss: 166.1082\n",
      "Epoch 10/10 | Batch 980/2990 | Loss: 253.0204\n",
      "Epoch 10/10 | Batch 1000/2990 | Loss: 126.6204\n",
      "Epoch 10/10 | Batch 1020/2990 | Loss: 154.1982\n",
      "Epoch 10/10 | Batch 1040/2990 | Loss: 144.9766\n",
      "Epoch 10/10 | Batch 1060/2990 | Loss: 82.7328\n",
      "Epoch 10/10 | Batch 1080/2990 | Loss: 247.2143\n",
      "Epoch 10/10 | Batch 1100/2990 | Loss: 204.9909\n",
      "Epoch 10/10 | Batch 1120/2990 | Loss: 468.3050\n",
      "Epoch 10/10 | Batch 1140/2990 | Loss: 207.2992\n",
      "Epoch 10/10 | Batch 1160/2990 | Loss: 234.2309\n",
      "Epoch 10/10 | Batch 1180/2990 | Loss: 133.1514\n",
      "Epoch 10/10 | Batch 1200/2990 | Loss: 90.0564\n",
      "Epoch 10/10 | Batch 1220/2990 | Loss: 192.5853\n",
      "Epoch 10/10 | Batch 1240/2990 | Loss: 151.4108\n",
      "Epoch 10/10 | Batch 1260/2990 | Loss: 165.1421\n",
      "Epoch 10/10 | Batch 1280/2990 | Loss: 241.3295\n",
      "Epoch 10/10 | Batch 1300/2990 | Loss: 118.5333\n",
      "Epoch 10/10 | Batch 1320/2990 | Loss: 199.5849\n",
      "Epoch 10/10 | Batch 1340/2990 | Loss: 157.4796\n",
      "Epoch 10/10 | Batch 1360/2990 | Loss: 304.6240\n",
      "Epoch 10/10 | Batch 1380/2990 | Loss: 446.6009\n",
      "Epoch 10/10 | Batch 1400/2990 | Loss: 287.5287\n",
      "Epoch 10/10 | Batch 1420/2990 | Loss: 108.2974\n",
      "Epoch 10/10 | Batch 1440/2990 | Loss: 167.1258\n",
      "Epoch 10/10 | Batch 1460/2990 | Loss: 197.6186\n",
      "Epoch 10/10 | Batch 1480/2990 | Loss: 143.6268\n",
      "Epoch 10/10 | Batch 1500/2990 | Loss: 104.3751\n",
      "Epoch 10/10 | Batch 1520/2990 | Loss: 88.1256\n",
      "Epoch 10/10 | Batch 1540/2990 | Loss: 83.7677\n",
      "Epoch 10/10 | Batch 1560/2990 | Loss: 294.9169\n",
      "Epoch 10/10 | Batch 1580/2990 | Loss: 159.9298\n",
      "Epoch 10/10 | Batch 1600/2990 | Loss: 56.0815\n",
      "Epoch 10/10 | Batch 1620/2990 | Loss: 214.4082\n",
      "Epoch 10/10 | Batch 1640/2990 | Loss: 268.2166\n",
      "Epoch 10/10 | Batch 1660/2990 | Loss: 212.0725\n",
      "Epoch 10/10 | Batch 1680/2990 | Loss: 154.3772\n",
      "Epoch 10/10 | Batch 1700/2990 | Loss: 158.0243\n",
      "Epoch 10/10 | Batch 1720/2990 | Loss: 127.7924\n",
      "Epoch 10/10 | Batch 1740/2990 | Loss: 105.1096\n",
      "Epoch 10/10 | Batch 1760/2990 | Loss: 232.2823\n",
      "Epoch 10/10 | Batch 1780/2990 | Loss: 154.9302\n",
      "Epoch 10/10 | Batch 1800/2990 | Loss: 248.3833\n",
      "Epoch 10/10 | Batch 1820/2990 | Loss: 102.0150\n",
      "Epoch 10/10 | Batch 1840/2990 | Loss: 50.4018\n",
      "Epoch 10/10 | Batch 1860/2990 | Loss: 94.2174\n",
      "Epoch 10/10 | Batch 1880/2990 | Loss: 119.9612\n",
      "Epoch 10/10 | Batch 1900/2990 | Loss: 120.2290\n",
      "Epoch 10/10 | Batch 1920/2990 | Loss: 234.3783\n",
      "Epoch 10/10 | Batch 1940/2990 | Loss: 230.4875\n",
      "Epoch 10/10 | Batch 1960/2990 | Loss: 123.3990\n",
      "Epoch 10/10 | Batch 1980/2990 | Loss: 83.1307\n",
      "Epoch 10/10 | Batch 2000/2990 | Loss: 240.7699\n",
      "Epoch 10/10 | Batch 2020/2990 | Loss: 224.3628\n",
      "Epoch 10/10 | Batch 2040/2990 | Loss: 53.9902\n",
      "Epoch 10/10 | Batch 2060/2990 | Loss: 281.5244\n",
      "Epoch 10/10 | Batch 2080/2990 | Loss: 64.0081\n",
      "Epoch 10/10 | Batch 2100/2990 | Loss: 114.9985\n",
      "Epoch 10/10 | Batch 2120/2990 | Loss: 107.2339\n",
      "Epoch 10/10 | Batch 2140/2990 | Loss: 130.5754\n",
      "Epoch 10/10 | Batch 2160/2990 | Loss: 72.1013\n",
      "Epoch 10/10 | Batch 2180/2990 | Loss: 103.7370\n",
      "Epoch 10/10 | Batch 2200/2990 | Loss: 185.4164\n",
      "Epoch 10/10 | Batch 2220/2990 | Loss: 209.2395\n",
      "Epoch 10/10 | Batch 2240/2990 | Loss: 276.3371\n",
      "Epoch 10/10 | Batch 2260/2990 | Loss: 118.7414\n",
      "Epoch 10/10 | Batch 2280/2990 | Loss: 192.1228\n",
      "Epoch 10/10 | Batch 2300/2990 | Loss: 148.8858\n",
      "Epoch 10/10 | Batch 2320/2990 | Loss: 140.6325\n",
      "Epoch 10/10 | Batch 2340/2990 | Loss: 116.3443\n",
      "Epoch 10/10 | Batch 2360/2990 | Loss: 91.2726\n",
      "Epoch 10/10 | Batch 2380/2990 | Loss: 110.4681\n",
      "Epoch 10/10 | Batch 2400/2990 | Loss: 223.3991\n",
      "Epoch 10/10 | Batch 2420/2990 | Loss: 98.6416\n",
      "Epoch 10/10 | Batch 2440/2990 | Loss: 168.1148\n",
      "Epoch 10/10 | Batch 2460/2990 | Loss: 63.5046\n",
      "Epoch 10/10 | Batch 2480/2990 | Loss: 143.9083\n",
      "Epoch 10/10 | Batch 2500/2990 | Loss: 94.4530\n",
      "Epoch 10/10 | Batch 2520/2990 | Loss: 123.4085\n",
      "Epoch 10/10 | Batch 2540/2990 | Loss: 319.8001\n",
      "Epoch 10/10 | Batch 2560/2990 | Loss: 121.6267\n",
      "Epoch 10/10 | Batch 2580/2990 | Loss: 126.8203\n",
      "Epoch 10/10 | Batch 2600/2990 | Loss: 205.7738\n",
      "Epoch 10/10 | Batch 2620/2990 | Loss: 142.4396\n",
      "Epoch 10/10 | Batch 2640/2990 | Loss: 164.9272\n",
      "Epoch 10/10 | Batch 2660/2990 | Loss: 289.5885\n",
      "Epoch 10/10 | Batch 2680/2990 | Loss: 114.6292\n",
      "Epoch 10/10 | Batch 2700/2990 | Loss: 352.6403\n",
      "Epoch 10/10 | Batch 2720/2990 | Loss: 175.6459\n",
      "Epoch 10/10 | Batch 2740/2990 | Loss: 173.7242\n",
      "Epoch 10/10 | Batch 2760/2990 | Loss: 250.6270\n",
      "Epoch 10/10 | Batch 2780/2990 | Loss: 75.7265\n",
      "Epoch 10/10 | Batch 2800/2990 | Loss: 209.7377\n",
      "Epoch 10/10 | Batch 2820/2990 | Loss: 73.3562\n",
      "Epoch 10/10 | Batch 2840/2990 | Loss: 137.4637\n",
      "Epoch 10/10 | Batch 2860/2990 | Loss: 241.7633\n",
      "Epoch 10/10 | Batch 2880/2990 | Loss: 92.5474\n",
      "Epoch 10/10 | Batch 2900/2990 | Loss: 116.8393\n",
      "Epoch 10/10 | Batch 2920/2990 | Loss: 37.4090\n",
      "Epoch 10/10 | Batch 2940/2990 | Loss: 192.6476\n",
      "Epoch 10/10 | Batch 2960/2990 | Loss: 104.7614\n",
      "Epoch 10/10 | Batch 2980/2990 | Loss: 142.0336\n",
      "✅ Epoch 10/10 | Train Loss: 167.1561 | Val Loss: 137.3001\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Load the saved model\n",
    "model.load_state_dict(torch.load(\"efficientnet_wheat_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Run test\n",
    "preds, actuals = test_model(model, test_loader)\n",
    "\n",
    "# ✅ Print sample predictions\n",
    "for p, a in zip(preds[:10], actuals[:10]):\n",
    "    print(f\"Predicted: {p:.2f}, Actual: {a:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
