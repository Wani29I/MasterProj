{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from CoAtNetModel import CoAtNetWheatModel\n",
    "from dataLoaderFunc import loadSplitData, createLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Compute Validation Loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                outputs = model(rgb_batch, dsm_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save Best Model\n",
    "        torch.save(model.state_dict(), \"coatnet_wheat_model.pth\")\n",
    "\n",
    "\n",
    "# ✅ Test the model on validation set\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb_batch, dsm_batch, label_batch in test_loader:\n",
    "            rgb_batch, dsm_batch = rgb_batch.to(device), dsm_batch.to(device)\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            actuals.extend(label_batch.cpu().numpy().flatten())\n",
    "\n",
    "    return predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ✅ Initialize Model, Loss Function, and Optimizer\n",
    "\n",
    "# ✅ Universal device selection (works on both Mac M2 & Windows with RTX 4060)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ✅ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ✅ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ✅ Default to CPU if no GPU is available\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "model = CoAtNetWheatModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 0/2990 | Loss: 120645.7656\n",
      "Epoch 1/10 | Batch 50/2990 | Loss: 10883.1357\n",
      "Epoch 1/10 | Batch 100/2990 | Loss: 11027.0078\n",
      "Epoch 1/10 | Batch 150/2990 | Loss: 2443.8435\n",
      "Epoch 1/10 | Batch 200/2990 | Loss: 9839.1211\n",
      "Epoch 1/10 | Batch 250/2990 | Loss: 2958.2566\n",
      "Epoch 1/10 | Batch 300/2990 | Loss: 4616.9434\n",
      "Epoch 1/10 | Batch 350/2990 | Loss: 1950.3430\n",
      "Epoch 1/10 | Batch 400/2990 | Loss: 2197.8569\n",
      "Epoch 1/10 | Batch 450/2990 | Loss: 3891.9268\n",
      "Epoch 1/10 | Batch 500/2990 | Loss: 3604.5291\n",
      "Epoch 1/10 | Batch 550/2990 | Loss: 3274.6655\n",
      "Epoch 1/10 | Batch 600/2990 | Loss: 4195.3418\n",
      "Epoch 1/10 | Batch 650/2990 | Loss: 4345.6328\n",
      "Epoch 1/10 | Batch 700/2990 | Loss: 1331.4562\n",
      "Epoch 1/10 | Batch 750/2990 | Loss: 1780.7058\n",
      "Epoch 1/10 | Batch 800/2990 | Loss: 2858.3533\n",
      "Epoch 1/10 | Batch 850/2990 | Loss: 2642.7563\n",
      "Epoch 1/10 | Batch 900/2990 | Loss: 1735.2305\n",
      "Epoch 1/10 | Batch 950/2990 | Loss: 3640.8032\n",
      "Epoch 1/10 | Batch 1000/2990 | Loss: 3499.1736\n",
      "Epoch 1/10 | Batch 1050/2990 | Loss: 735.6541\n",
      "Epoch 1/10 | Batch 1100/2990 | Loss: 3600.2432\n",
      "Epoch 1/10 | Batch 1150/2990 | Loss: 3569.9619\n",
      "Epoch 1/10 | Batch 1200/2990 | Loss: 6332.1455\n",
      "Epoch 1/10 | Batch 1250/2990 | Loss: 4268.5449\n",
      "Epoch 1/10 | Batch 1300/2990 | Loss: 4567.0654\n",
      "Epoch 1/10 | Batch 1350/2990 | Loss: 1998.6990\n",
      "Epoch 1/10 | Batch 1400/2990 | Loss: 701.5609\n",
      "Epoch 1/10 | Batch 1450/2990 | Loss: 2607.7307\n",
      "Epoch 1/10 | Batch 1500/2990 | Loss: 3948.9744\n",
      "Epoch 1/10 | Batch 1550/2990 | Loss: 4122.4614\n",
      "Epoch 1/10 | Batch 1600/2990 | Loss: 2027.9353\n",
      "Epoch 1/10 | Batch 1650/2990 | Loss: 2379.8625\n",
      "Epoch 1/10 | Batch 1700/2990 | Loss: 4757.5034\n",
      "Epoch 1/10 | Batch 1750/2990 | Loss: 1796.6047\n",
      "Epoch 1/10 | Batch 1800/2990 | Loss: 2366.3137\n",
      "Epoch 1/10 | Batch 1850/2990 | Loss: 8032.8652\n",
      "Epoch 1/10 | Batch 1900/2990 | Loss: 2028.3873\n",
      "Epoch 1/10 | Batch 1950/2990 | Loss: 1964.9926\n",
      "Epoch 1/10 | Batch 2000/2990 | Loss: 2696.1016\n",
      "Epoch 1/10 | Batch 2050/2990 | Loss: 2636.7290\n",
      "Epoch 1/10 | Batch 2100/2990 | Loss: 2318.6514\n",
      "Epoch 1/10 | Batch 2150/2990 | Loss: 2496.8059\n",
      "Epoch 1/10 | Batch 2200/2990 | Loss: 2209.1311\n",
      "Epoch 1/10 | Batch 2250/2990 | Loss: 1683.0352\n",
      "Epoch 1/10 | Batch 2300/2990 | Loss: 1787.1437\n",
      "Epoch 1/10 | Batch 2350/2990 | Loss: 2457.4939\n",
      "Epoch 1/10 | Batch 2400/2990 | Loss: 1325.9397\n",
      "Epoch 1/10 | Batch 2450/2990 | Loss: 1213.6228\n",
      "Epoch 1/10 | Batch 2500/2990 | Loss: 2917.7410\n",
      "Epoch 1/10 | Batch 2550/2990 | Loss: 1143.0227\n",
      "Epoch 1/10 | Batch 2600/2990 | Loss: 1469.1912\n",
      "Epoch 1/10 | Batch 2650/2990 | Loss: 1217.7954\n",
      "Epoch 1/10 | Batch 2700/2990 | Loss: 1864.2356\n",
      "Epoch 1/10 | Batch 2750/2990 | Loss: 2472.8689\n",
      "Epoch 1/10 | Batch 2800/2990 | Loss: 2484.6567\n",
      "Epoch 1/10 | Batch 2850/2990 | Loss: 1778.4028\n",
      "Epoch 1/10 | Batch 2900/2990 | Loss: 845.1284\n",
      "Epoch 1/10 | Batch 2950/2990 | Loss: 1963.1001\n",
      "✅ Epoch 1/10 | Train Loss: 4088.6408 | Val Loss: 1999.1243\n",
      "Epoch 2/10 | Batch 0/2990 | Loss: 2124.8286\n",
      "Epoch 2/10 | Batch 50/2990 | Loss: 2175.1855\n",
      "Epoch 2/10 | Batch 100/2990 | Loss: 1544.1201\n",
      "Epoch 2/10 | Batch 150/2990 | Loss: 633.3691\n",
      "Epoch 2/10 | Batch 200/2990 | Loss: 1368.3032\n",
      "Epoch 2/10 | Batch 250/2990 | Loss: 1255.1656\n",
      "Epoch 2/10 | Batch 300/2990 | Loss: 2126.5952\n",
      "Epoch 2/10 | Batch 350/2990 | Loss: 899.2778\n",
      "Epoch 2/10 | Batch 400/2990 | Loss: 1720.9806\n",
      "Epoch 2/10 | Batch 450/2990 | Loss: 1299.4042\n",
      "Epoch 2/10 | Batch 500/2990 | Loss: 575.4094\n",
      "Epoch 2/10 | Batch 550/2990 | Loss: 2342.9492\n",
      "Epoch 2/10 | Batch 600/2990 | Loss: 2904.2996\n",
      "Epoch 2/10 | Batch 650/2990 | Loss: 3302.0535\n",
      "Epoch 2/10 | Batch 700/2990 | Loss: 1239.1299\n",
      "Epoch 2/10 | Batch 750/2990 | Loss: 1019.0547\n",
      "Epoch 2/10 | Batch 800/2990 | Loss: 1517.0403\n",
      "Epoch 2/10 | Batch 850/2990 | Loss: 2486.7473\n",
      "Epoch 2/10 | Batch 900/2990 | Loss: 1127.0010\n",
      "Epoch 2/10 | Batch 950/2990 | Loss: 872.7220\n",
      "Epoch 2/10 | Batch 1000/2990 | Loss: 1190.6799\n",
      "Epoch 2/10 | Batch 1050/2990 | Loss: 1220.7241\n",
      "Epoch 2/10 | Batch 1100/2990 | Loss: 1145.1495\n",
      "Epoch 2/10 | Batch 1150/2990 | Loss: 1769.3860\n",
      "Epoch 2/10 | Batch 1200/2990 | Loss: 1017.2946\n",
      "Epoch 2/10 | Batch 1250/2990 | Loss: 1784.9840\n",
      "Epoch 2/10 | Batch 1300/2990 | Loss: 649.7213\n",
      "Epoch 2/10 | Batch 1350/2990 | Loss: 1420.4807\n",
      "Epoch 2/10 | Batch 1400/2990 | Loss: 1463.5149\n",
      "Epoch 2/10 | Batch 1450/2990 | Loss: 1126.2317\n",
      "Epoch 2/10 | Batch 1500/2990 | Loss: 1381.3142\n",
      "Epoch 2/10 | Batch 1550/2990 | Loss: 1027.4193\n",
      "Epoch 2/10 | Batch 1600/2990 | Loss: 992.9852\n",
      "Epoch 2/10 | Batch 1650/2990 | Loss: 1170.3353\n",
      "Epoch 2/10 | Batch 1700/2990 | Loss: 1673.0323\n",
      "Epoch 2/10 | Batch 1750/2990 | Loss: 1552.8569\n",
      "Epoch 2/10 | Batch 1800/2990 | Loss: 2402.7612\n",
      "Epoch 2/10 | Batch 1850/2990 | Loss: 1860.0229\n",
      "Epoch 2/10 | Batch 1900/2990 | Loss: 1152.9740\n",
      "Epoch 2/10 | Batch 1950/2990 | Loss: 542.4841\n",
      "Epoch 2/10 | Batch 2000/2990 | Loss: 847.6163\n",
      "Epoch 2/10 | Batch 2050/2990 | Loss: 653.2734\n",
      "Epoch 2/10 | Batch 2100/2990 | Loss: 1387.3396\n",
      "Epoch 2/10 | Batch 2150/2990 | Loss: 1019.0518\n",
      "Epoch 2/10 | Batch 2200/2990 | Loss: 1456.9945\n",
      "Epoch 2/10 | Batch 2250/2990 | Loss: 2449.9951\n",
      "Epoch 2/10 | Batch 2300/2990 | Loss: 667.7395\n",
      "Epoch 2/10 | Batch 2350/2990 | Loss: 872.6997\n",
      "Epoch 2/10 | Batch 2400/2990 | Loss: 791.7061\n",
      "Epoch 2/10 | Batch 2450/2990 | Loss: 1079.3584\n",
      "Epoch 2/10 | Batch 2500/2990 | Loss: 1237.6096\n",
      "Epoch 2/10 | Batch 2550/2990 | Loss: 985.9646\n",
      "Epoch 2/10 | Batch 2600/2990 | Loss: 1404.1053\n",
      "Epoch 2/10 | Batch 2650/2990 | Loss: 3121.5669\n",
      "Epoch 2/10 | Batch 2700/2990 | Loss: 629.1643\n",
      "Epoch 2/10 | Batch 2750/2990 | Loss: 968.8110\n",
      "Epoch 2/10 | Batch 2800/2990 | Loss: 721.2993\n",
      "Epoch 2/10 | Batch 2850/2990 | Loss: 2107.2065\n",
      "Epoch 2/10 | Batch 2900/2990 | Loss: 1001.5388\n",
      "Epoch 2/10 | Batch 2950/2990 | Loss: 1265.6588\n",
      "✅ Epoch 2/10 | Train Loss: 1443.6273 | Val Loss: 1157.3342\n",
      "Epoch 3/10 | Batch 0/2990 | Loss: 1191.4287\n",
      "Epoch 3/10 | Batch 50/2990 | Loss: 818.4975\n",
      "Epoch 3/10 | Batch 100/2990 | Loss: 883.2827\n",
      "Epoch 3/10 | Batch 150/2990 | Loss: 359.5619\n",
      "Epoch 3/10 | Batch 200/2990 | Loss: 608.8204\n",
      "Epoch 3/10 | Batch 250/2990 | Loss: 787.3139\n",
      "Epoch 3/10 | Batch 300/2990 | Loss: 678.3068\n",
      "Epoch 3/10 | Batch 350/2990 | Loss: 697.2829\n",
      "Epoch 3/10 | Batch 400/2990 | Loss: 1859.7357\n",
      "Epoch 3/10 | Batch 450/2990 | Loss: 846.7703\n",
      "Epoch 3/10 | Batch 500/2990 | Loss: 710.8706\n",
      "Epoch 3/10 | Batch 550/2990 | Loss: 841.6170\n",
      "Epoch 3/10 | Batch 600/2990 | Loss: 1260.6945\n",
      "Epoch 3/10 | Batch 650/2990 | Loss: 1344.3180\n",
      "Epoch 3/10 | Batch 700/2990 | Loss: 613.8883\n",
      "Epoch 3/10 | Batch 750/2990 | Loss: 1276.0349\n",
      "Epoch 3/10 | Batch 800/2990 | Loss: 647.1672\n",
      "Epoch 3/10 | Batch 850/2990 | Loss: 442.4001\n",
      "Epoch 3/10 | Batch 900/2990 | Loss: 590.9829\n",
      "Epoch 3/10 | Batch 950/2990 | Loss: 253.2549\n",
      "Epoch 3/10 | Batch 1000/2990 | Loss: 546.3920\n",
      "Epoch 3/10 | Batch 1050/2990 | Loss: 956.6727\n",
      "Epoch 3/10 | Batch 1100/2990 | Loss: 1811.5439\n",
      "Epoch 3/10 | Batch 1150/2990 | Loss: 383.1641\n",
      "Epoch 3/10 | Batch 1200/2990 | Loss: 343.4359\n",
      "Epoch 3/10 | Batch 1250/2990 | Loss: 280.2130\n",
      "Epoch 3/10 | Batch 1300/2990 | Loss: 508.7040\n",
      "Epoch 3/10 | Batch 1350/2990 | Loss: 624.5309\n",
      "Epoch 3/10 | Batch 1400/2990 | Loss: 1198.0609\n",
      "Epoch 3/10 | Batch 1450/2990 | Loss: 1253.5439\n",
      "Epoch 3/10 | Batch 1500/2990 | Loss: 499.0767\n",
      "Epoch 3/10 | Batch 1550/2990 | Loss: 808.7979\n",
      "Epoch 3/10 | Batch 1600/2990 | Loss: 874.4664\n",
      "Epoch 3/10 | Batch 1650/2990 | Loss: 579.2501\n",
      "Epoch 3/10 | Batch 1700/2990 | Loss: 910.4243\n",
      "Epoch 3/10 | Batch 1750/2990 | Loss: 1231.2488\n",
      "Epoch 3/10 | Batch 1800/2990 | Loss: 327.3941\n",
      "Epoch 3/10 | Batch 1850/2990 | Loss: 553.7891\n",
      "Epoch 3/10 | Batch 1900/2990 | Loss: 507.0228\n",
      "Epoch 3/10 | Batch 1950/2990 | Loss: 834.9161\n",
      "Epoch 3/10 | Batch 2000/2990 | Loss: 292.6723\n",
      "Epoch 3/10 | Batch 2050/2990 | Loss: 919.2575\n",
      "Epoch 3/10 | Batch 2100/2990 | Loss: 629.5630\n",
      "Epoch 3/10 | Batch 2150/2990 | Loss: 717.8022\n",
      "Epoch 3/10 | Batch 2200/2990 | Loss: 340.2441\n",
      "Epoch 3/10 | Batch 2250/2990 | Loss: 1106.1348\n",
      "Epoch 3/10 | Batch 2300/2990 | Loss: 854.8624\n",
      "Epoch 3/10 | Batch 2350/2990 | Loss: 618.2043\n",
      "Epoch 3/10 | Batch 2400/2990 | Loss: 506.5415\n",
      "Epoch 3/10 | Batch 2450/2990 | Loss: 757.7344\n",
      "Epoch 3/10 | Batch 2500/2990 | Loss: 328.2053\n",
      "Epoch 3/10 | Batch 2550/2990 | Loss: 1155.3347\n",
      "Epoch 3/10 | Batch 2600/2990 | Loss: 1198.7532\n",
      "Epoch 3/10 | Batch 2650/2990 | Loss: 1209.5336\n",
      "Epoch 3/10 | Batch 2700/2990 | Loss: 550.1487\n",
      "Epoch 3/10 | Batch 2750/2990 | Loss: 571.9711\n",
      "Epoch 3/10 | Batch 2800/2990 | Loss: 636.3262\n",
      "Epoch 3/10 | Batch 2850/2990 | Loss: 451.1033\n",
      "Epoch 3/10 | Batch 2900/2990 | Loss: 1157.6509\n",
      "Epoch 3/10 | Batch 2950/2990 | Loss: 521.2476\n",
      "✅ Epoch 3/10 | Train Loss: 765.1654 | Val Loss: 591.1401\n",
      "Epoch 4/10 | Batch 0/2990 | Loss: 255.7730\n",
      "Epoch 4/10 | Batch 50/2990 | Loss: 310.0343\n",
      "Epoch 4/10 | Batch 100/2990 | Loss: 592.6688\n",
      "Epoch 4/10 | Batch 150/2990 | Loss: 658.6348\n",
      "Epoch 4/10 | Batch 200/2990 | Loss: 371.1903\n",
      "Epoch 4/10 | Batch 250/2990 | Loss: 256.3634\n",
      "Epoch 4/10 | Batch 300/2990 | Loss: 432.5700\n",
      "Epoch 4/10 | Batch 350/2990 | Loss: 415.1156\n",
      "Epoch 4/10 | Batch 400/2990 | Loss: 491.1606\n",
      "Epoch 4/10 | Batch 450/2990 | Loss: 253.3986\n",
      "Epoch 4/10 | Batch 500/2990 | Loss: 235.0196\n",
      "Epoch 4/10 | Batch 550/2990 | Loss: 760.3231\n",
      "Epoch 4/10 | Batch 600/2990 | Loss: 165.7157\n",
      "Epoch 4/10 | Batch 650/2990 | Loss: 322.3044\n",
      "Epoch 4/10 | Batch 700/2990 | Loss: 544.8784\n",
      "Epoch 4/10 | Batch 750/2990 | Loss: 786.8806\n",
      "Epoch 4/10 | Batch 800/2990 | Loss: 366.1331\n",
      "Epoch 4/10 | Batch 850/2990 | Loss: 315.0111\n",
      "Epoch 4/10 | Batch 900/2990 | Loss: 876.4030\n",
      "Epoch 4/10 | Batch 950/2990 | Loss: 255.9473\n",
      "Epoch 4/10 | Batch 1000/2990 | Loss: 660.3659\n",
      "Epoch 4/10 | Batch 1050/2990 | Loss: 934.8958\n",
      "Epoch 4/10 | Batch 1100/2990 | Loss: 446.2301\n",
      "Epoch 4/10 | Batch 1150/2990 | Loss: 602.8403\n",
      "Epoch 4/10 | Batch 1200/2990 | Loss: 502.3613\n",
      "Epoch 4/10 | Batch 1250/2990 | Loss: 418.7642\n",
      "Epoch 4/10 | Batch 1300/2990 | Loss: 202.7273\n",
      "Epoch 4/10 | Batch 1350/2990 | Loss: 312.6763\n",
      "Epoch 4/10 | Batch 1400/2990 | Loss: 372.2053\n",
      "Epoch 4/10 | Batch 1450/2990 | Loss: 872.6670\n",
      "Epoch 4/10 | Batch 1500/2990 | Loss: 994.3340\n",
      "Epoch 4/10 | Batch 1550/2990 | Loss: 1152.4080\n",
      "Epoch 4/10 | Batch 1600/2990 | Loss: 250.0622\n",
      "Epoch 4/10 | Batch 1650/2990 | Loss: 477.6156\n",
      "Epoch 4/10 | Batch 1700/2990 | Loss: 436.1237\n",
      "Epoch 4/10 | Batch 1750/2990 | Loss: 144.8672\n",
      "Epoch 4/10 | Batch 1800/2990 | Loss: 225.5450\n",
      "Epoch 4/10 | Batch 1850/2990 | Loss: 623.5223\n",
      "Epoch 4/10 | Batch 1900/2990 | Loss: 285.7747\n",
      "Epoch 4/10 | Batch 1950/2990 | Loss: 254.6878\n",
      "Epoch 4/10 | Batch 2000/2990 | Loss: 231.9170\n",
      "Epoch 4/10 | Batch 2050/2990 | Loss: 375.5667\n",
      "Epoch 4/10 | Batch 2100/2990 | Loss: 383.9702\n",
      "Epoch 4/10 | Batch 2150/2990 | Loss: 438.1086\n",
      "Epoch 4/10 | Batch 2200/2990 | Loss: 368.0272\n",
      "Epoch 4/10 | Batch 2250/2990 | Loss: 422.2981\n",
      "Epoch 4/10 | Batch 2300/2990 | Loss: 718.0944\n",
      "Epoch 4/10 | Batch 2350/2990 | Loss: 579.5886\n",
      "Epoch 4/10 | Batch 2400/2990 | Loss: 352.7272\n",
      "Epoch 4/10 | Batch 2450/2990 | Loss: 348.5148\n",
      "Epoch 4/10 | Batch 2500/2990 | Loss: 257.7406\n",
      "Epoch 4/10 | Batch 2550/2990 | Loss: 484.7250\n",
      "Epoch 4/10 | Batch 2600/2990 | Loss: 696.3432\n",
      "Epoch 4/10 | Batch 2650/2990 | Loss: 393.4497\n",
      "Epoch 4/10 | Batch 2700/2990 | Loss: 292.9328\n",
      "Epoch 4/10 | Batch 2750/2990 | Loss: 1066.6027\n",
      "Epoch 4/10 | Batch 2800/2990 | Loss: 457.4537\n",
      "Epoch 4/10 | Batch 2850/2990 | Loss: 449.3257\n",
      "Epoch 4/10 | Batch 2900/2990 | Loss: 497.9702\n",
      "Epoch 4/10 | Batch 2950/2990 | Loss: 931.6226\n",
      "✅ Epoch 4/10 | Train Loss: 479.9290 | Val Loss: 646.6841\n",
      "Epoch 5/10 | Batch 0/2990 | Loss: 734.6560\n",
      "Epoch 5/10 | Batch 50/2990 | Loss: 554.5261\n",
      "Epoch 5/10 | Batch 100/2990 | Loss: 889.7723\n",
      "Epoch 5/10 | Batch 150/2990 | Loss: 439.7226\n",
      "Epoch 5/10 | Batch 200/2990 | Loss: 391.4680\n",
      "Epoch 5/10 | Batch 250/2990 | Loss: 464.0441\n",
      "Epoch 5/10 | Batch 300/2990 | Loss: 392.2118\n",
      "Epoch 5/10 | Batch 350/2990 | Loss: 75.4204\n",
      "Epoch 5/10 | Batch 400/2990 | Loss: 237.4841\n",
      "Epoch 5/10 | Batch 450/2990 | Loss: 126.9843\n",
      "Epoch 5/10 | Batch 500/2990 | Loss: 417.8855\n",
      "Epoch 5/10 | Batch 550/2990 | Loss: 266.7000\n",
      "Epoch 5/10 | Batch 600/2990 | Loss: 346.0090\n",
      "Epoch 5/10 | Batch 650/2990 | Loss: 422.1363\n",
      "Epoch 5/10 | Batch 700/2990 | Loss: 292.8002\n",
      "Epoch 5/10 | Batch 750/2990 | Loss: 203.7895\n",
      "Epoch 5/10 | Batch 800/2990 | Loss: 270.1700\n",
      "Epoch 5/10 | Batch 850/2990 | Loss: 600.3785\n",
      "Epoch 5/10 | Batch 900/2990 | Loss: 254.3710\n",
      "Epoch 5/10 | Batch 950/2990 | Loss: 243.5025\n",
      "Epoch 5/10 | Batch 1000/2990 | Loss: 301.7401\n",
      "Epoch 5/10 | Batch 1050/2990 | Loss: 296.1033\n",
      "Epoch 5/10 | Batch 1100/2990 | Loss: 380.1057\n",
      "Epoch 5/10 | Batch 1150/2990 | Loss: 222.2081\n",
      "Epoch 5/10 | Batch 1200/2990 | Loss: 309.4275\n",
      "Epoch 5/10 | Batch 1250/2990 | Loss: 604.5064\n",
      "Epoch 5/10 | Batch 1300/2990 | Loss: 267.0227\n",
      "Epoch 5/10 | Batch 1350/2990 | Loss: 367.6437\n",
      "Epoch 5/10 | Batch 1400/2990 | Loss: 258.0493\n",
      "Epoch 5/10 | Batch 1450/2990 | Loss: 311.3860\n",
      "Epoch 5/10 | Batch 1500/2990 | Loss: 608.1333\n",
      "Epoch 5/10 | Batch 1550/2990 | Loss: 177.4495\n",
      "Epoch 5/10 | Batch 1600/2990 | Loss: 155.0996\n",
      "Epoch 5/10 | Batch 1650/2990 | Loss: 295.2542\n",
      "Epoch 5/10 | Batch 1700/2990 | Loss: 377.1784\n",
      "Epoch 5/10 | Batch 1750/2990 | Loss: 292.9926\n",
      "Epoch 5/10 | Batch 1800/2990 | Loss: 501.7739\n",
      "Epoch 5/10 | Batch 1850/2990 | Loss: 374.2791\n",
      "Epoch 5/10 | Batch 1900/2990 | Loss: 535.6122\n",
      "Epoch 5/10 | Batch 1950/2990 | Loss: 295.4927\n",
      "Epoch 5/10 | Batch 2000/2990 | Loss: 250.2919\n",
      "Epoch 5/10 | Batch 2050/2990 | Loss: 297.7966\n",
      "Epoch 5/10 | Batch 2100/2990 | Loss: 157.1847\n",
      "Epoch 5/10 | Batch 2150/2990 | Loss: 216.5367\n",
      "Epoch 5/10 | Batch 2200/2990 | Loss: 462.6807\n",
      "Epoch 5/10 | Batch 2250/2990 | Loss: 323.1747\n",
      "Epoch 5/10 | Batch 2300/2990 | Loss: 445.8000\n",
      "Epoch 5/10 | Batch 2350/2990 | Loss: 793.4790\n",
      "Epoch 5/10 | Batch 2400/2990 | Loss: 514.7424\n",
      "Epoch 5/10 | Batch 2450/2990 | Loss: 261.8232\n",
      "Epoch 5/10 | Batch 2500/2990 | Loss: 181.9362\n",
      "Epoch 5/10 | Batch 2550/2990 | Loss: 301.8683\n",
      "Epoch 5/10 | Batch 2600/2990 | Loss: 161.5650\n",
      "Epoch 5/10 | Batch 2650/2990 | Loss: 402.4941\n",
      "Epoch 5/10 | Batch 2700/2990 | Loss: 291.4747\n",
      "Epoch 5/10 | Batch 2750/2990 | Loss: 330.8576\n",
      "Epoch 5/10 | Batch 2800/2990 | Loss: 230.3341\n",
      "Epoch 5/10 | Batch 2850/2990 | Loss: 245.9674\n",
      "Epoch 5/10 | Batch 2900/2990 | Loss: 416.7180\n",
      "Epoch 5/10 | Batch 2950/2990 | Loss: 158.3744\n",
      "✅ Epoch 5/10 | Train Loss: 384.2236 | Val Loss: 600.7247\n",
      "Epoch 6/10 | Batch 0/2990 | Loss: 248.0662\n",
      "Epoch 6/10 | Batch 50/2990 | Loss: 681.0453\n",
      "Epoch 6/10 | Batch 100/2990 | Loss: 135.9305\n",
      "Epoch 6/10 | Batch 150/2990 | Loss: 232.0412\n",
      "Epoch 6/10 | Batch 200/2990 | Loss: 86.0064\n",
      "Epoch 6/10 | Batch 250/2990 | Loss: 71.1838\n",
      "Epoch 6/10 | Batch 300/2990 | Loss: 205.7776\n",
      "Epoch 6/10 | Batch 350/2990 | Loss: 284.5599\n",
      "Epoch 6/10 | Batch 400/2990 | Loss: 187.7464\n",
      "Epoch 6/10 | Batch 450/2990 | Loss: 384.3924\n",
      "Epoch 6/10 | Batch 500/2990 | Loss: 166.0211\n",
      "Epoch 6/10 | Batch 550/2990 | Loss: 311.9208\n",
      "Epoch 6/10 | Batch 600/2990 | Loss: 225.3220\n",
      "Epoch 6/10 | Batch 650/2990 | Loss: 151.5130\n",
      "Epoch 6/10 | Batch 700/2990 | Loss: 107.2111\n",
      "Epoch 6/10 | Batch 750/2990 | Loss: 288.4533\n",
      "Epoch 6/10 | Batch 800/2990 | Loss: 135.8477\n",
      "Epoch 6/10 | Batch 850/2990 | Loss: 286.0000\n",
      "Epoch 6/10 | Batch 900/2990 | Loss: 495.2011\n",
      "Epoch 6/10 | Batch 950/2990 | Loss: 347.5125\n",
      "Epoch 6/10 | Batch 1000/2990 | Loss: 187.6057\n",
      "Epoch 6/10 | Batch 1050/2990 | Loss: 549.5569\n",
      "Epoch 6/10 | Batch 1100/2990 | Loss: 413.1742\n",
      "Epoch 6/10 | Batch 1150/2990 | Loss: 469.4498\n",
      "Epoch 6/10 | Batch 1200/2990 | Loss: 176.9618\n",
      "Epoch 6/10 | Batch 1250/2990 | Loss: 513.1042\n",
      "Epoch 6/10 | Batch 1300/2990 | Loss: 511.6056\n",
      "Epoch 6/10 | Batch 1350/2990 | Loss: 146.5534\n",
      "Epoch 6/10 | Batch 1400/2990 | Loss: 89.2476\n",
      "Epoch 6/10 | Batch 1450/2990 | Loss: 93.1564\n",
      "Epoch 6/10 | Batch 1500/2990 | Loss: 521.2234\n",
      "Epoch 6/10 | Batch 1550/2990 | Loss: 230.2329\n",
      "Epoch 6/10 | Batch 1600/2990 | Loss: 182.8177\n",
      "Epoch 6/10 | Batch 1650/2990 | Loss: 162.4970\n",
      "Epoch 6/10 | Batch 1700/2990 | Loss: 213.5740\n",
      "Epoch 6/10 | Batch 1750/2990 | Loss: 204.8146\n",
      "Epoch 6/10 | Batch 1800/2990 | Loss: 138.9409\n",
      "Epoch 6/10 | Batch 1850/2990 | Loss: 202.3310\n",
      "Epoch 6/10 | Batch 1900/2990 | Loss: 314.5086\n",
      "Epoch 6/10 | Batch 1950/2990 | Loss: 245.9633\n",
      "Epoch 6/10 | Batch 2000/2990 | Loss: 390.3134\n",
      "Epoch 6/10 | Batch 2050/2990 | Loss: 238.0414\n",
      "Epoch 6/10 | Batch 2100/2990 | Loss: 196.6108\n",
      "Epoch 6/10 | Batch 2150/2990 | Loss: 216.1637\n",
      "Epoch 6/10 | Batch 2200/2990 | Loss: 757.0959\n",
      "Epoch 6/10 | Batch 2250/2990 | Loss: 133.6273\n",
      "Epoch 6/10 | Batch 2300/2990 | Loss: 404.2052\n",
      "Epoch 6/10 | Batch 2350/2990 | Loss: 282.2146\n",
      "Epoch 6/10 | Batch 2400/2990 | Loss: 191.7739\n",
      "Epoch 6/10 | Batch 2450/2990 | Loss: 232.7032\n",
      "Epoch 6/10 | Batch 2500/2990 | Loss: 375.2053\n",
      "Epoch 6/10 | Batch 2550/2990 | Loss: 402.9723\n",
      "Epoch 6/10 | Batch 2600/2990 | Loss: 529.3632\n",
      "Epoch 6/10 | Batch 2650/2990 | Loss: 314.4781\n",
      "Epoch 6/10 | Batch 2700/2990 | Loss: 315.3249\n",
      "Epoch 6/10 | Batch 2750/2990 | Loss: 197.0003\n",
      "Epoch 6/10 | Batch 2800/2990 | Loss: 110.4962\n",
      "Epoch 6/10 | Batch 2850/2990 | Loss: 275.4983\n",
      "Epoch 6/10 | Batch 2900/2990 | Loss: 385.0673\n",
      "Epoch 6/10 | Batch 2950/2990 | Loss: 271.5611\n",
      "✅ Epoch 6/10 | Train Loss: 284.5120 | Val Loss: 318.7049\n",
      "Epoch 7/10 | Batch 0/2990 | Loss: 312.3109\n",
      "Epoch 7/10 | Batch 50/2990 | Loss: 326.6527\n",
      "Epoch 7/10 | Batch 100/2990 | Loss: 217.1032\n",
      "Epoch 7/10 | Batch 150/2990 | Loss: 386.6780\n",
      "Epoch 7/10 | Batch 200/2990 | Loss: 123.3343\n",
      "Epoch 7/10 | Batch 250/2990 | Loss: 124.9007\n",
      "Epoch 7/10 | Batch 300/2990 | Loss: 191.7361\n",
      "Epoch 7/10 | Batch 350/2990 | Loss: 102.5182\n",
      "Epoch 7/10 | Batch 400/2990 | Loss: 295.9097\n",
      "Epoch 7/10 | Batch 450/2990 | Loss: 218.0022\n",
      "Epoch 7/10 | Batch 500/2990 | Loss: 144.5523\n",
      "Epoch 7/10 | Batch 550/2990 | Loss: 158.4182\n",
      "Epoch 7/10 | Batch 600/2990 | Loss: 235.8305\n",
      "Epoch 7/10 | Batch 650/2990 | Loss: 261.2657\n",
      "Epoch 7/10 | Batch 700/2990 | Loss: 115.3193\n",
      "Epoch 7/10 | Batch 750/2990 | Loss: 240.8696\n",
      "Epoch 7/10 | Batch 800/2990 | Loss: 274.5985\n",
      "Epoch 7/10 | Batch 850/2990 | Loss: 409.8903\n",
      "Epoch 7/10 | Batch 900/2990 | Loss: 310.3810\n",
      "Epoch 7/10 | Batch 950/2990 | Loss: 145.6707\n",
      "Epoch 7/10 | Batch 1000/2990 | Loss: 399.7112\n",
      "Epoch 7/10 | Batch 1050/2990 | Loss: 283.3995\n",
      "Epoch 7/10 | Batch 1100/2990 | Loss: 186.1749\n",
      "Epoch 7/10 | Batch 1150/2990 | Loss: 136.2706\n",
      "Epoch 7/10 | Batch 1200/2990 | Loss: 109.9042\n",
      "Epoch 7/10 | Batch 1250/2990 | Loss: 297.3231\n",
      "Epoch 7/10 | Batch 1300/2990 | Loss: 800.5172\n",
      "Epoch 7/10 | Batch 1350/2990 | Loss: 470.8448\n",
      "Epoch 7/10 | Batch 1400/2990 | Loss: 137.9427\n",
      "Epoch 7/10 | Batch 1450/2990 | Loss: 203.4789\n",
      "Epoch 7/10 | Batch 1500/2990 | Loss: 114.3240\n",
      "Epoch 7/10 | Batch 1550/2990 | Loss: 187.9644\n",
      "Epoch 7/10 | Batch 1600/2990 | Loss: 187.2252\n",
      "Epoch 7/10 | Batch 1650/2990 | Loss: 408.7358\n",
      "Epoch 7/10 | Batch 1700/2990 | Loss: 238.5550\n",
      "Epoch 7/10 | Batch 1750/2990 | Loss: 120.5839\n",
      "Epoch 7/10 | Batch 1800/2990 | Loss: 216.9111\n",
      "Epoch 7/10 | Batch 1850/2990 | Loss: 356.0369\n",
      "Epoch 7/10 | Batch 1900/2990 | Loss: 130.5597\n",
      "Epoch 7/10 | Batch 1950/2990 | Loss: 195.9072\n",
      "Epoch 7/10 | Batch 2000/2990 | Loss: 161.0169\n",
      "Epoch 7/10 | Batch 2050/2990 | Loss: 581.6456\n",
      "Epoch 7/10 | Batch 2100/2990 | Loss: 324.2899\n",
      "Epoch 7/10 | Batch 2150/2990 | Loss: 164.4608\n",
      "Epoch 7/10 | Batch 2200/2990 | Loss: 162.8734\n",
      "Epoch 7/10 | Batch 2250/2990 | Loss: 353.6308\n",
      "Epoch 7/10 | Batch 2300/2990 | Loss: 196.4570\n",
      "Epoch 7/10 | Batch 2350/2990 | Loss: 515.7671\n",
      "Epoch 7/10 | Batch 2400/2990 | Loss: 289.9478\n",
      "Epoch 7/10 | Batch 2450/2990 | Loss: 101.6196\n",
      "Epoch 7/10 | Batch 2500/2990 | Loss: 172.5177\n",
      "Epoch 7/10 | Batch 2550/2990 | Loss: 76.6582\n",
      "Epoch 7/10 | Batch 2600/2990 | Loss: 300.8434\n",
      "Epoch 7/10 | Batch 2650/2990 | Loss: 252.1707\n",
      "Epoch 7/10 | Batch 2700/2990 | Loss: 209.9108\n",
      "Epoch 7/10 | Batch 2750/2990 | Loss: 176.5789\n",
      "Epoch 7/10 | Batch 2800/2990 | Loss: 893.5401\n",
      "Epoch 7/10 | Batch 2850/2990 | Loss: 268.5886\n",
      "Epoch 7/10 | Batch 2900/2990 | Loss: 330.6481\n",
      "Epoch 7/10 | Batch 2950/2990 | Loss: 347.8934\n",
      "✅ Epoch 7/10 | Train Loss: 259.6473 | Val Loss: 445.7860\n",
      "Epoch 8/10 | Batch 0/2990 | Loss: 360.4249\n",
      "Epoch 8/10 | Batch 50/2990 | Loss: 211.6735\n",
      "Epoch 8/10 | Batch 100/2990 | Loss: 253.4549\n",
      "Epoch 8/10 | Batch 150/2990 | Loss: 207.1361\n",
      "Epoch 8/10 | Batch 200/2990 | Loss: 251.7409\n",
      "Epoch 8/10 | Batch 250/2990 | Loss: 204.3717\n",
      "Epoch 8/10 | Batch 300/2990 | Loss: 185.5614\n",
      "Epoch 8/10 | Batch 350/2990 | Loss: 98.3295\n",
      "Epoch 8/10 | Batch 400/2990 | Loss: 261.8544\n",
      "Epoch 8/10 | Batch 450/2990 | Loss: 152.0035\n",
      "Epoch 8/10 | Batch 500/2990 | Loss: 161.1021\n",
      "Epoch 8/10 | Batch 550/2990 | Loss: 223.3629\n",
      "Epoch 8/10 | Batch 600/2990 | Loss: 365.4557\n",
      "Epoch 8/10 | Batch 650/2990 | Loss: 257.0744\n",
      "Epoch 8/10 | Batch 700/2990 | Loss: 559.3309\n",
      "Epoch 8/10 | Batch 750/2990 | Loss: 175.9454\n",
      "Epoch 8/10 | Batch 800/2990 | Loss: 96.3609\n",
      "Epoch 8/10 | Batch 850/2990 | Loss: 167.8447\n",
      "Epoch 8/10 | Batch 900/2990 | Loss: 429.9907\n",
      "Epoch 8/10 | Batch 950/2990 | Loss: 437.2550\n",
      "Epoch 8/10 | Batch 1000/2990 | Loss: 100.5668\n",
      "Epoch 8/10 | Batch 1050/2990 | Loss: 234.1320\n",
      "Epoch 8/10 | Batch 1100/2990 | Loss: 109.7404\n",
      "Epoch 8/10 | Batch 1150/2990 | Loss: 139.3683\n",
      "Epoch 8/10 | Batch 1200/2990 | Loss: 110.1701\n",
      "Epoch 8/10 | Batch 1250/2990 | Loss: 176.3042\n",
      "Epoch 8/10 | Batch 1300/2990 | Loss: 276.7184\n",
      "Epoch 8/10 | Batch 1350/2990 | Loss: 231.3177\n",
      "Epoch 8/10 | Batch 1400/2990 | Loss: 268.8297\n",
      "Epoch 8/10 | Batch 1450/2990 | Loss: 46.4467\n",
      "Epoch 8/10 | Batch 1500/2990 | Loss: 112.9550\n",
      "Epoch 8/10 | Batch 1550/2990 | Loss: 166.0234\n",
      "Epoch 8/10 | Batch 1600/2990 | Loss: 228.1182\n",
      "Epoch 8/10 | Batch 1650/2990 | Loss: 194.7614\n",
      "Epoch 8/10 | Batch 1700/2990 | Loss: 94.1301\n",
      "Epoch 8/10 | Batch 1750/2990 | Loss: 251.2023\n",
      "Epoch 8/10 | Batch 1800/2990 | Loss: 323.8468\n",
      "Epoch 8/10 | Batch 1850/2990 | Loss: 146.3867\n",
      "Epoch 8/10 | Batch 1900/2990 | Loss: 182.4447\n",
      "Epoch 8/10 | Batch 1950/2990 | Loss: 103.3222\n",
      "Epoch 8/10 | Batch 2000/2990 | Loss: 94.0467\n",
      "Epoch 8/10 | Batch 2050/2990 | Loss: 421.9928\n",
      "Epoch 8/10 | Batch 2100/2990 | Loss: 303.8967\n",
      "Epoch 8/10 | Batch 2150/2990 | Loss: 293.5331\n",
      "Epoch 8/10 | Batch 2200/2990 | Loss: 193.8494\n",
      "Epoch 8/10 | Batch 2250/2990 | Loss: 171.0028\n",
      "Epoch 8/10 | Batch 2300/2990 | Loss: 230.0607\n",
      "Epoch 8/10 | Batch 2350/2990 | Loss: 80.6358\n",
      "Epoch 8/10 | Batch 2400/2990 | Loss: 426.9217\n",
      "Epoch 8/10 | Batch 2450/2990 | Loss: 127.3587\n",
      "Epoch 8/10 | Batch 2500/2990 | Loss: 159.9818\n",
      "Epoch 8/10 | Batch 2550/2990 | Loss: 127.7833\n",
      "Epoch 8/10 | Batch 2600/2990 | Loss: 95.9454\n",
      "Epoch 8/10 | Batch 2650/2990 | Loss: 167.3414\n",
      "Epoch 8/10 | Batch 2700/2990 | Loss: 157.4214\n",
      "Epoch 8/10 | Batch 2750/2990 | Loss: 945.9373\n",
      "Epoch 8/10 | Batch 2800/2990 | Loss: 275.5106\n",
      "Epoch 8/10 | Batch 2850/2990 | Loss: 199.9371\n",
      "Epoch 8/10 | Batch 2900/2990 | Loss: 186.6588\n",
      "Epoch 8/10 | Batch 2950/2990 | Loss: 209.5643\n",
      "✅ Epoch 8/10 | Train Loss: 237.0711 | Val Loss: 1118.7162\n",
      "Epoch 9/10 | Batch 0/2990 | Loss: 677.4720\n",
      "Epoch 9/10 | Batch 50/2990 | Loss: 402.6361\n",
      "Epoch 9/10 | Batch 100/2990 | Loss: 93.3143\n",
      "Epoch 9/10 | Batch 150/2990 | Loss: 240.0820\n",
      "Epoch 9/10 | Batch 200/2990 | Loss: 158.8340\n",
      "Epoch 9/10 | Batch 250/2990 | Loss: 163.6239\n",
      "Epoch 9/10 | Batch 300/2990 | Loss: 75.8940\n",
      "Epoch 9/10 | Batch 350/2990 | Loss: 104.6568\n",
      "Epoch 9/10 | Batch 400/2990 | Loss: 165.0833\n",
      "Epoch 9/10 | Batch 450/2990 | Loss: 235.4015\n",
      "Epoch 9/10 | Batch 500/2990 | Loss: 408.6323\n",
      "Epoch 9/10 | Batch 550/2990 | Loss: 85.6324\n",
      "Epoch 9/10 | Batch 600/2990 | Loss: 101.6763\n",
      "Epoch 9/10 | Batch 650/2990 | Loss: 73.9599\n",
      "Epoch 9/10 | Batch 700/2990 | Loss: 92.1472\n",
      "Epoch 9/10 | Batch 750/2990 | Loss: 178.5534\n",
      "Epoch 9/10 | Batch 800/2990 | Loss: 194.6081\n",
      "Epoch 9/10 | Batch 850/2990 | Loss: 100.1523\n",
      "Epoch 9/10 | Batch 900/2990 | Loss: 104.7371\n",
      "Epoch 9/10 | Batch 950/2990 | Loss: 209.2554\n",
      "Epoch 9/10 | Batch 1000/2990 | Loss: 205.4514\n",
      "Epoch 9/10 | Batch 1050/2990 | Loss: 138.0050\n",
      "Epoch 9/10 | Batch 1100/2990 | Loss: 107.0575\n",
      "Epoch 9/10 | Batch 1150/2990 | Loss: 159.9588\n",
      "Epoch 9/10 | Batch 1200/2990 | Loss: 442.8706\n",
      "Epoch 9/10 | Batch 1250/2990 | Loss: 163.1369\n",
      "Epoch 9/10 | Batch 1300/2990 | Loss: 94.6065\n",
      "Epoch 9/10 | Batch 1350/2990 | Loss: 253.0054\n",
      "Epoch 9/10 | Batch 1400/2990 | Loss: 175.0198\n",
      "Epoch 9/10 | Batch 1450/2990 | Loss: 195.4914\n",
      "Epoch 9/10 | Batch 1500/2990 | Loss: 69.8398\n",
      "Epoch 9/10 | Batch 1550/2990 | Loss: 184.4998\n",
      "Epoch 9/10 | Batch 1600/2990 | Loss: 93.2909\n",
      "Epoch 9/10 | Batch 1650/2990 | Loss: 148.4677\n",
      "Epoch 9/10 | Batch 1700/2990 | Loss: 124.3742\n",
      "Epoch 9/10 | Batch 1750/2990 | Loss: 114.9631\n",
      "Epoch 9/10 | Batch 1800/2990 | Loss: 377.3028\n",
      "Epoch 9/10 | Batch 1850/2990 | Loss: 213.0564\n",
      "Epoch 9/10 | Batch 1900/2990 | Loss: 246.4612\n",
      "Epoch 9/10 | Batch 1950/2990 | Loss: 35.8782\n",
      "Epoch 9/10 | Batch 2000/2990 | Loss: 207.9571\n",
      "Epoch 9/10 | Batch 2050/2990 | Loss: 170.1966\n",
      "Epoch 9/10 | Batch 2100/2990 | Loss: 79.1253\n",
      "Epoch 9/10 | Batch 2150/2990 | Loss: 182.8759\n",
      "Epoch 9/10 | Batch 2200/2990 | Loss: 31.8086\n",
      "Epoch 9/10 | Batch 2250/2990 | Loss: 157.6011\n",
      "Epoch 9/10 | Batch 2300/2990 | Loss: 145.3259\n",
      "Epoch 9/10 | Batch 2350/2990 | Loss: 143.2003\n",
      "Epoch 9/10 | Batch 2400/2990 | Loss: 166.3604\n",
      "Epoch 9/10 | Batch 2450/2990 | Loss: 107.8810\n",
      "Epoch 9/10 | Batch 2500/2990 | Loss: 144.6756\n",
      "Epoch 9/10 | Batch 2550/2990 | Loss: 139.0270\n",
      "Epoch 9/10 | Batch 2600/2990 | Loss: 87.8936\n",
      "Epoch 9/10 | Batch 2650/2990 | Loss: 230.7815\n",
      "Epoch 9/10 | Batch 2700/2990 | Loss: 112.2575\n",
      "Epoch 9/10 | Batch 2750/2990 | Loss: 173.8283\n",
      "Epoch 9/10 | Batch 2800/2990 | Loss: 192.0847\n",
      "Epoch 9/10 | Batch 2850/2990 | Loss: 98.9197\n",
      "Epoch 9/10 | Batch 2900/2990 | Loss: 56.5758\n",
      "Epoch 9/10 | Batch 2950/2990 | Loss: 117.4614\n",
      "✅ Epoch 9/10 | Train Loss: 182.2363 | Val Loss: 222.9430\n",
      "Epoch 10/10 | Batch 0/2990 | Loss: 95.9532\n",
      "Epoch 10/10 | Batch 50/2990 | Loss: 151.1598\n",
      "Epoch 10/10 | Batch 100/2990 | Loss: 350.6655\n",
      "Epoch 10/10 | Batch 150/2990 | Loss: 156.4856\n",
      "Epoch 10/10 | Batch 200/2990 | Loss: 175.9532\n",
      "Epoch 10/10 | Batch 250/2990 | Loss: 162.7500\n",
      "Epoch 10/10 | Batch 300/2990 | Loss: 45.4339\n",
      "Epoch 10/10 | Batch 350/2990 | Loss: 148.3693\n",
      "Epoch 10/10 | Batch 400/2990 | Loss: 142.5335\n",
      "Epoch 10/10 | Batch 450/2990 | Loss: 275.9787\n",
      "Epoch 10/10 | Batch 500/2990 | Loss: 236.7586\n",
      "Epoch 10/10 | Batch 550/2990 | Loss: 124.0845\n",
      "Epoch 10/10 | Batch 600/2990 | Loss: 85.9732\n",
      "Epoch 10/10 | Batch 650/2990 | Loss: 217.1232\n",
      "Epoch 10/10 | Batch 700/2990 | Loss: 268.8282\n",
      "Epoch 10/10 | Batch 750/2990 | Loss: 496.0394\n",
      "Epoch 10/10 | Batch 800/2990 | Loss: 133.4146\n",
      "Epoch 10/10 | Batch 850/2990 | Loss: 247.5136\n",
      "Epoch 10/10 | Batch 900/2990 | Loss: 173.5919\n",
      "Epoch 10/10 | Batch 950/2990 | Loss: 213.3945\n",
      "Epoch 10/10 | Batch 1000/2990 | Loss: 97.8511\n",
      "Epoch 10/10 | Batch 1050/2990 | Loss: 85.1707\n",
      "Epoch 10/10 | Batch 1100/2990 | Loss: 122.3567\n",
      "Epoch 10/10 | Batch 1150/2990 | Loss: 184.3158\n",
      "Epoch 10/10 | Batch 1200/2990 | Loss: 102.8137\n",
      "Epoch 10/10 | Batch 1250/2990 | Loss: 191.6716\n",
      "Epoch 10/10 | Batch 1300/2990 | Loss: 67.9014\n",
      "Epoch 10/10 | Batch 1350/2990 | Loss: 71.7481\n",
      "Epoch 10/10 | Batch 1400/2990 | Loss: 65.8149\n",
      "Epoch 10/10 | Batch 1450/2990 | Loss: 161.9639\n",
      "Epoch 10/10 | Batch 1500/2990 | Loss: 73.2051\n",
      "Epoch 10/10 | Batch 1550/2990 | Loss: 191.4887\n",
      "Epoch 10/10 | Batch 1600/2990 | Loss: 198.1925\n",
      "Epoch 10/10 | Batch 1650/2990 | Loss: 172.0763\n",
      "Epoch 10/10 | Batch 1700/2990 | Loss: 108.3406\n",
      "Epoch 10/10 | Batch 1750/2990 | Loss: 141.4953\n",
      "Epoch 10/10 | Batch 1800/2990 | Loss: 83.0844\n",
      "Epoch 10/10 | Batch 1850/2990 | Loss: 189.7907\n",
      "Epoch 10/10 | Batch 1900/2990 | Loss: 71.7947\n",
      "Epoch 10/10 | Batch 1950/2990 | Loss: 224.8318\n",
      "Epoch 10/10 | Batch 2000/2990 | Loss: 120.1422\n",
      "Epoch 10/10 | Batch 2050/2990 | Loss: 251.5685\n",
      "Epoch 10/10 | Batch 2100/2990 | Loss: 455.7534\n",
      "Epoch 10/10 | Batch 2150/2990 | Loss: 102.2728\n",
      "Epoch 10/10 | Batch 2200/2990 | Loss: 661.5762\n",
      "Epoch 10/10 | Batch 2250/2990 | Loss: 172.9689\n",
      "Epoch 10/10 | Batch 2300/2990 | Loss: 172.3203\n",
      "Epoch 10/10 | Batch 2350/2990 | Loss: 81.6731\n",
      "Epoch 10/10 | Batch 2400/2990 | Loss: 167.1523\n",
      "Epoch 10/10 | Batch 2450/2990 | Loss: 83.2330\n",
      "Epoch 10/10 | Batch 2500/2990 | Loss: 274.5178\n",
      "Epoch 10/10 | Batch 2550/2990 | Loss: 80.2038\n",
      "Epoch 10/10 | Batch 2600/2990 | Loss: 233.1086\n",
      "Epoch 10/10 | Batch 2650/2990 | Loss: 106.3392\n",
      "Epoch 10/10 | Batch 2700/2990 | Loss: 68.7195\n",
      "Epoch 10/10 | Batch 2750/2990 | Loss: 176.3634\n",
      "Epoch 10/10 | Batch 2800/2990 | Loss: 125.6640\n",
      "Epoch 10/10 | Batch 2850/2990 | Loss: 56.0089\n",
      "Epoch 10/10 | Batch 2900/2990 | Loss: 89.8665\n",
      "Epoch 10/10 | Batch 2950/2990 | Loss: 40.0136\n",
      "✅ Epoch 10/10 | Train Loss: 201.0615 | Val Loss: 313.6962\n"
     ]
    }
   ],
   "source": [
    "# ✅ Start Training\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 378.04, Actual: 368.00\n",
      "Predicted: 563.03, Actual: 552.00\n",
      "Predicted: 458.03, Actual: 454.00\n",
      "Predicted: 298.36, Actual: 281.00\n",
      "Predicted: 474.43, Actual: 472.00\n",
      "Predicted: 433.04, Actual: 425.00\n",
      "Predicted: 303.64, Actual: 295.00\n",
      "Predicted: 457.78, Actual: 430.00\n",
      "Predicted: 410.50, Actual: 392.00\n",
      "Predicted: 133.23, Actual: 129.00\n"
     ]
    }
   ],
   "source": [
    "# ✅ Load the saved model\n",
    "model.load_state_dict(torch.load(\"coatnet_wheat_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Run test\n",
    "preds, actuals = test_model(model, test_loader)\n",
    "\n",
    "# ✅ Print sample predictions\n",
    "for p, a in zip(preds[:10], actuals[:10]):\n",
    "    print(f\"Predicted: {p:.2f}, Actual: {a:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
