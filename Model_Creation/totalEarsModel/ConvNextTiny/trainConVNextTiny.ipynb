{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from ConVNextTinyModel import ConvNeXtTinyWheatModel\n",
    "from dataLoaderFunc import loadSplitData, createLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device='cuda'):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch = rgb_batch.to(device)\n",
    "            dsm_batch = dsm_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # âœ… Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch = rgb_batch.to(device)\n",
    "                dsm_batch = dsm_batch.to(device)\n",
    "                label_batch = label_batch.to(device)\n",
    "\n",
    "                outputs = model(rgb_batch, dsm_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"âœ… Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # âœ… Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"convnext_wheat_model.pth\")\n",
    "            print(\"ðŸ’¾ Saved best model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# âœ… Initialize Model, Loss Function, and Optimizer\n",
    "\n",
    "# âœ… Universal device selection (works on both Mac M2 & Windows with RTX 4060)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # âœ… Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # âœ… Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # âœ… Default to CPU if no GPU is available\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "model = ConvNeXtTinyWheatModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 0/2990 | Loss: 126066.2969\n",
      "Epoch 1/10 | Batch 20/2990 | Loss: 77552.9844\n",
      "Epoch 1/10 | Batch 40/2990 | Loss: 19823.9375\n",
      "Epoch 1/10 | Batch 60/2990 | Loss: 14872.8965\n",
      "Epoch 1/10 | Batch 80/2990 | Loss: 6900.7861\n",
      "Epoch 1/10 | Batch 100/2990 | Loss: 6559.8384\n",
      "Epoch 1/10 | Batch 120/2990 | Loss: 8725.2734\n",
      "Epoch 1/10 | Batch 140/2990 | Loss: 16089.5654\n",
      "Epoch 1/10 | Batch 160/2990 | Loss: 5167.8369\n",
      "Epoch 1/10 | Batch 180/2990 | Loss: 6233.1934\n",
      "Epoch 1/10 | Batch 200/2990 | Loss: 10388.8379\n",
      "Epoch 1/10 | Batch 220/2990 | Loss: 7186.0552\n",
      "Epoch 1/10 | Batch 240/2990 | Loss: 6423.8369\n",
      "Epoch 1/10 | Batch 260/2990 | Loss: 7062.6763\n",
      "Epoch 1/10 | Batch 280/2990 | Loss: 14830.4150\n",
      "Epoch 1/10 | Batch 300/2990 | Loss: 16633.5625\n",
      "Epoch 1/10 | Batch 320/2990 | Loss: 12855.7637\n",
      "Epoch 1/10 | Batch 340/2990 | Loss: 14097.7520\n",
      "Epoch 1/10 | Batch 360/2990 | Loss: 10708.9014\n",
      "Epoch 1/10 | Batch 380/2990 | Loss: 5844.2373\n",
      "Epoch 1/10 | Batch 400/2990 | Loss: 6342.8711\n",
      "Epoch 1/10 | Batch 420/2990 | Loss: 5503.0396\n",
      "Epoch 1/10 | Batch 440/2990 | Loss: 14225.6016\n",
      "Epoch 1/10 | Batch 460/2990 | Loss: 9577.2910\n",
      "Epoch 1/10 | Batch 480/2990 | Loss: 12739.9590\n",
      "Epoch 1/10 | Batch 500/2990 | Loss: 4355.2676\n",
      "Epoch 1/10 | Batch 520/2990 | Loss: 8396.1426\n",
      "Epoch 1/10 | Batch 540/2990 | Loss: 5161.3584\n",
      "Epoch 1/10 | Batch 560/2990 | Loss: 16677.6621\n",
      "Epoch 1/10 | Batch 580/2990 | Loss: 13305.2793\n",
      "Epoch 1/10 | Batch 600/2990 | Loss: 13546.2803\n",
      "Epoch 1/10 | Batch 620/2990 | Loss: 9004.6367\n",
      "Epoch 1/10 | Batch 640/2990 | Loss: 10804.9746\n",
      "Epoch 1/10 | Batch 660/2990 | Loss: 10933.3662\n",
      "Epoch 1/10 | Batch 680/2990 | Loss: 5518.7715\n",
      "Epoch 1/10 | Batch 700/2990 | Loss: 10377.2246\n",
      "Epoch 1/10 | Batch 720/2990 | Loss: 7988.5991\n",
      "Epoch 1/10 | Batch 740/2990 | Loss: 6114.7158\n",
      "Epoch 1/10 | Batch 760/2990 | Loss: 10423.0449\n",
      "Epoch 1/10 | Batch 780/2990 | Loss: 14637.3857\n",
      "Epoch 1/10 | Batch 800/2990 | Loss: 7281.4717\n",
      "Epoch 1/10 | Batch 820/2990 | Loss: 5323.1738\n",
      "Epoch 1/10 | Batch 840/2990 | Loss: 5921.3301\n",
      "Epoch 1/10 | Batch 860/2990 | Loss: 7607.9570\n",
      "Epoch 1/10 | Batch 880/2990 | Loss: 6130.5332\n",
      "Epoch 1/10 | Batch 900/2990 | Loss: 4806.7090\n",
      "Epoch 1/10 | Batch 920/2990 | Loss: 11940.5742\n",
      "Epoch 1/10 | Batch 940/2990 | Loss: 7791.5928\n",
      "Epoch 1/10 | Batch 960/2990 | Loss: 11549.7520\n",
      "Epoch 1/10 | Batch 980/2990 | Loss: 12724.3896\n",
      "Epoch 1/10 | Batch 1000/2990 | Loss: 12561.5498\n",
      "Epoch 1/10 | Batch 1020/2990 | Loss: 7496.8838\n",
      "Epoch 1/10 | Batch 1040/2990 | Loss: 10733.8691\n",
      "Epoch 1/10 | Batch 1060/2990 | Loss: 10176.6152\n",
      "Epoch 1/10 | Batch 1080/2990 | Loss: 7997.2451\n",
      "Epoch 1/10 | Batch 1100/2990 | Loss: 11718.3027\n",
      "Epoch 1/10 | Batch 1120/2990 | Loss: 6386.8232\n",
      "Epoch 1/10 | Batch 1140/2990 | Loss: 14793.0078\n",
      "Epoch 1/10 | Batch 1160/2990 | Loss: 7445.2358\n",
      "Epoch 1/10 | Batch 1180/2990 | Loss: 9387.0801\n",
      "Epoch 1/10 | Batch 1200/2990 | Loss: 4524.7681\n",
      "Epoch 1/10 | Batch 1220/2990 | Loss: 6699.8037\n",
      "Epoch 1/10 | Batch 1240/2990 | Loss: 10764.8867\n",
      "Epoch 1/10 | Batch 1260/2990 | Loss: 5867.3696\n",
      "Epoch 1/10 | Batch 1280/2990 | Loss: 8768.8115\n",
      "Epoch 1/10 | Batch 1300/2990 | Loss: 12302.6074\n",
      "Epoch 1/10 | Batch 1320/2990 | Loss: 12052.3789\n",
      "Epoch 1/10 | Batch 1340/2990 | Loss: 13346.3438\n",
      "Epoch 1/10 | Batch 1360/2990 | Loss: 9374.4668\n",
      "Epoch 1/10 | Batch 1380/2990 | Loss: 7313.7402\n",
      "Epoch 1/10 | Batch 1400/2990 | Loss: 6627.4224\n",
      "Epoch 1/10 | Batch 1420/2990 | Loss: 9936.2900\n",
      "Epoch 1/10 | Batch 1440/2990 | Loss: 4297.4492\n",
      "Epoch 1/10 | Batch 1460/2990 | Loss: 11504.7969\n",
      "Epoch 1/10 | Batch 1480/2990 | Loss: 10098.1436\n",
      "Epoch 1/10 | Batch 1500/2990 | Loss: 5842.2158\n",
      "Epoch 1/10 | Batch 1520/2990 | Loss: 8844.8350\n",
      "Epoch 1/10 | Batch 1540/2990 | Loss: 2309.5486\n",
      "Epoch 1/10 | Batch 1560/2990 | Loss: 8400.8203\n",
      "Epoch 1/10 | Batch 1580/2990 | Loss: 12674.0195\n",
      "Epoch 1/10 | Batch 1600/2990 | Loss: 14703.0215\n",
      "Epoch 1/10 | Batch 1620/2990 | Loss: 5333.4219\n",
      "Epoch 1/10 | Batch 1640/2990 | Loss: 9354.1934\n",
      "Epoch 1/10 | Batch 1660/2990 | Loss: 11652.3750\n",
      "Epoch 1/10 | Batch 1680/2990 | Loss: 10100.3535\n",
      "Epoch 1/10 | Batch 1700/2990 | Loss: 7380.6445\n",
      "Epoch 1/10 | Batch 1720/2990 | Loss: 13010.9766\n",
      "Epoch 1/10 | Batch 1740/2990 | Loss: 12272.9893\n",
      "Epoch 1/10 | Batch 1760/2990 | Loss: 8696.4619\n",
      "Epoch 1/10 | Batch 1780/2990 | Loss: 2192.4456\n",
      "Epoch 1/10 | Batch 1800/2990 | Loss: 14549.3398\n",
      "Epoch 1/10 | Batch 1820/2990 | Loss: 9589.5156\n",
      "Epoch 1/10 | Batch 1840/2990 | Loss: 15780.5449\n",
      "Epoch 1/10 | Batch 1860/2990 | Loss: 12011.2090\n",
      "Epoch 1/10 | Batch 1880/2990 | Loss: 6729.6650\n",
      "Epoch 1/10 | Batch 1900/2990 | Loss: 16138.7520\n",
      "Epoch 1/10 | Batch 1920/2990 | Loss: 9633.8203\n",
      "Epoch 1/10 | Batch 1940/2990 | Loss: 7348.2852\n",
      "Epoch 1/10 | Batch 1960/2990 | Loss: 8297.6865\n",
      "Epoch 1/10 | Batch 1980/2990 | Loss: 7331.2856\n",
      "Epoch 1/10 | Batch 2000/2990 | Loss: 5523.3472\n",
      "Epoch 1/10 | Batch 2020/2990 | Loss: 5890.8613\n",
      "Epoch 1/10 | Batch 2040/2990 | Loss: 6509.6348\n",
      "Epoch 1/10 | Batch 2060/2990 | Loss: 5817.0601\n",
      "Epoch 1/10 | Batch 2080/2990 | Loss: 9642.6484\n",
      "Epoch 1/10 | Batch 2100/2990 | Loss: 4479.6587\n",
      "Epoch 1/10 | Batch 2120/2990 | Loss: 9592.7627\n",
      "Epoch 1/10 | Batch 2140/2990 | Loss: 5580.5620\n",
      "Epoch 1/10 | Batch 2160/2990 | Loss: 7272.5010\n",
      "Epoch 1/10 | Batch 2180/2990 | Loss: 5956.5215\n",
      "Epoch 1/10 | Batch 2200/2990 | Loss: 10939.8643\n",
      "Epoch 1/10 | Batch 2220/2990 | Loss: 4823.6172\n",
      "Epoch 1/10 | Batch 2240/2990 | Loss: 7544.0889\n",
      "Epoch 1/10 | Batch 2260/2990 | Loss: 6684.0693\n",
      "Epoch 1/10 | Batch 2280/2990 | Loss: 10143.8770\n",
      "Epoch 1/10 | Batch 2300/2990 | Loss: 17590.9238\n",
      "Epoch 1/10 | Batch 2320/2990 | Loss: 4024.2610\n",
      "Epoch 1/10 | Batch 2340/2990 | Loss: 9256.0234\n",
      "Epoch 1/10 | Batch 2360/2990 | Loss: 6725.9204\n",
      "Epoch 1/10 | Batch 2380/2990 | Loss: 7587.2188\n",
      "Epoch 1/10 | Batch 2400/2990 | Loss: 12982.1865\n",
      "Epoch 1/10 | Batch 2420/2990 | Loss: 6102.2793\n",
      "Epoch 1/10 | Batch 2440/2990 | Loss: 6049.6387\n",
      "Epoch 1/10 | Batch 2460/2990 | Loss: 8108.2720\n",
      "Epoch 1/10 | Batch 2480/2990 | Loss: 9239.9785\n",
      "Epoch 1/10 | Batch 2500/2990 | Loss: 12568.1465\n",
      "Epoch 1/10 | Batch 2520/2990 | Loss: 15265.4814\n",
      "Epoch 1/10 | Batch 2540/2990 | Loss: 5014.8066\n",
      "Epoch 1/10 | Batch 2560/2990 | Loss: 10867.5107\n",
      "Epoch 1/10 | Batch 2580/2990 | Loss: 7072.9487\n",
      "Epoch 1/10 | Batch 2600/2990 | Loss: 8686.5723\n",
      "Epoch 1/10 | Batch 2620/2990 | Loss: 5953.0654\n",
      "Epoch 1/10 | Batch 2640/2990 | Loss: 9982.0117\n",
      "Epoch 1/10 | Batch 2660/2990 | Loss: 6876.6963\n",
      "Epoch 1/10 | Batch 2680/2990 | Loss: 4699.9551\n",
      "Epoch 1/10 | Batch 2700/2990 | Loss: 10515.5312\n",
      "Epoch 1/10 | Batch 2720/2990 | Loss: 10637.6631\n",
      "Epoch 1/10 | Batch 2740/2990 | Loss: 10168.4648\n",
      "Epoch 1/10 | Batch 2760/2990 | Loss: 4637.7808\n",
      "Epoch 1/10 | Batch 2780/2990 | Loss: 6764.9395\n",
      "Epoch 1/10 | Batch 2800/2990 | Loss: 8937.0771\n",
      "Epoch 1/10 | Batch 2820/2990 | Loss: 9617.5117\n",
      "Epoch 1/10 | Batch 2840/2990 | Loss: 3163.2798\n",
      "Epoch 1/10 | Batch 2860/2990 | Loss: 3230.9727\n",
      "Epoch 1/10 | Batch 2880/2990 | Loss: 9287.5947\n",
      "Epoch 1/10 | Batch 2900/2990 | Loss: 11188.6895\n",
      "Epoch 1/10 | Batch 2920/2990 | Loss: 9345.4102\n",
      "Epoch 1/10 | Batch 2940/2990 | Loss: 8066.7515\n",
      "Epoch 1/10 | Batch 2960/2990 | Loss: 7125.5142\n",
      "Epoch 1/10 | Batch 2980/2990 | Loss: 5868.0015\n",
      "âœ… Epoch 1/10 | Train Loss: 9909.8231 | Val Loss: 8047.4256\n",
      "ðŸ’¾ Saved best model!\n",
      "Epoch 2/10 | Batch 0/2990 | Loss: 13285.3164\n",
      "Epoch 2/10 | Batch 20/2990 | Loss: 8395.4561\n",
      "Epoch 2/10 | Batch 40/2990 | Loss: 9744.8730\n",
      "Epoch 2/10 | Batch 60/2990 | Loss: 6877.3071\n",
      "Epoch 2/10 | Batch 80/2990 | Loss: 8080.9082\n",
      "Epoch 2/10 | Batch 100/2990 | Loss: 13856.7852\n",
      "Epoch 2/10 | Batch 120/2990 | Loss: 13209.1914\n",
      "Epoch 2/10 | Batch 140/2990 | Loss: 10954.9102\n",
      "Epoch 2/10 | Batch 160/2990 | Loss: 8245.9365\n",
      "Epoch 2/10 | Batch 180/2990 | Loss: 13944.2266\n",
      "Epoch 2/10 | Batch 200/2990 | Loss: 10790.6113\n",
      "Epoch 2/10 | Batch 220/2990 | Loss: 11782.6572\n",
      "Epoch 2/10 | Batch 240/2990 | Loss: 5671.8169\n",
      "Epoch 2/10 | Batch 260/2990 | Loss: 9308.2188\n",
      "Epoch 2/10 | Batch 280/2990 | Loss: 7354.1748\n",
      "Epoch 2/10 | Batch 300/2990 | Loss: 3057.9004\n",
      "Epoch 2/10 | Batch 320/2990 | Loss: 8407.2422\n",
      "Epoch 2/10 | Batch 340/2990 | Loss: 11229.8311\n",
      "Epoch 2/10 | Batch 360/2990 | Loss: 4070.5310\n",
      "Epoch 2/10 | Batch 380/2990 | Loss: 5038.5000\n",
      "Epoch 2/10 | Batch 400/2990 | Loss: 2577.0107\n",
      "Epoch 2/10 | Batch 420/2990 | Loss: 8089.6074\n",
      "Epoch 2/10 | Batch 440/2990 | Loss: 9185.4014\n",
      "Epoch 2/10 | Batch 460/2990 | Loss: 7601.0391\n",
      "Epoch 2/10 | Batch 480/2990 | Loss: 7521.9082\n",
      "Epoch 2/10 | Batch 500/2990 | Loss: 5862.8545\n",
      "Epoch 2/10 | Batch 520/2990 | Loss: 2748.9502\n",
      "Epoch 2/10 | Batch 540/2990 | Loss: 6724.0073\n",
      "Epoch 2/10 | Batch 560/2990 | Loss: 6855.7002\n",
      "Epoch 2/10 | Batch 580/2990 | Loss: 6714.3730\n",
      "Epoch 2/10 | Batch 600/2990 | Loss: 6660.5898\n",
      "Epoch 2/10 | Batch 620/2990 | Loss: 3940.7134\n",
      "Epoch 2/10 | Batch 640/2990 | Loss: 9166.7256\n",
      "Epoch 2/10 | Batch 660/2990 | Loss: 10253.1768\n",
      "Epoch 2/10 | Batch 680/2990 | Loss: 8934.0352\n",
      "Epoch 2/10 | Batch 700/2990 | Loss: 8145.4951\n",
      "Epoch 2/10 | Batch 720/2990 | Loss: 9316.1367\n",
      "Epoch 2/10 | Batch 740/2990 | Loss: 2981.3889\n",
      "Epoch 2/10 | Batch 760/2990 | Loss: 4727.3730\n",
      "Epoch 2/10 | Batch 780/2990 | Loss: 2140.1943\n",
      "Epoch 2/10 | Batch 800/2990 | Loss: 4141.0449\n",
      "Epoch 2/10 | Batch 820/2990 | Loss: 4040.9592\n",
      "Epoch 2/10 | Batch 840/2990 | Loss: 7070.2129\n",
      "Epoch 2/10 | Batch 860/2990 | Loss: 3074.3198\n",
      "Epoch 2/10 | Batch 880/2990 | Loss: 3513.5830\n",
      "Epoch 2/10 | Batch 900/2990 | Loss: 6802.7012\n",
      "Epoch 2/10 | Batch 920/2990 | Loss: 3649.1006\n",
      "Epoch 2/10 | Batch 940/2990 | Loss: 6644.2314\n",
      "Epoch 2/10 | Batch 960/2990 | Loss: 6045.9131\n",
      "Epoch 2/10 | Batch 980/2990 | Loss: 5891.5933\n",
      "Epoch 2/10 | Batch 1000/2990 | Loss: 8238.3271\n",
      "Epoch 2/10 | Batch 1020/2990 | Loss: 2700.2800\n",
      "Epoch 2/10 | Batch 1040/2990 | Loss: 2868.4250\n",
      "Epoch 2/10 | Batch 1060/2990 | Loss: 6142.3506\n",
      "Epoch 2/10 | Batch 1080/2990 | Loss: 5181.1670\n",
      "Epoch 2/10 | Batch 1100/2990 | Loss: 5176.4546\n",
      "Epoch 2/10 | Batch 1120/2990 | Loss: 5021.1421\n",
      "Epoch 2/10 | Batch 1140/2990 | Loss: 3872.6626\n",
      "Epoch 2/10 | Batch 1160/2990 | Loss: 3555.5537\n",
      "Epoch 2/10 | Batch 1180/2990 | Loss: 7732.6465\n",
      "Epoch 2/10 | Batch 1200/2990 | Loss: 3288.8809\n",
      "Epoch 2/10 | Batch 1220/2990 | Loss: 3663.7024\n",
      "Epoch 2/10 | Batch 1240/2990 | Loss: 4281.6567\n",
      "Epoch 2/10 | Batch 1260/2990 | Loss: 1264.9772\n",
      "Epoch 2/10 | Batch 1280/2990 | Loss: 1580.2607\n",
      "Epoch 2/10 | Batch 1300/2990 | Loss: 2557.2822\n",
      "Epoch 2/10 | Batch 1320/2990 | Loss: 3990.6245\n",
      "Epoch 2/10 | Batch 1340/2990 | Loss: 6419.5732\n",
      "Epoch 2/10 | Batch 1360/2990 | Loss: 1409.6329\n",
      "Epoch 2/10 | Batch 1380/2990 | Loss: 4658.5020\n",
      "Epoch 2/10 | Batch 1400/2990 | Loss: 2482.4578\n",
      "Epoch 2/10 | Batch 1420/2990 | Loss: 3943.2937\n",
      "Epoch 2/10 | Batch 1440/2990 | Loss: 4834.3247\n",
      "Epoch 2/10 | Batch 1460/2990 | Loss: 5025.9551\n",
      "Epoch 2/10 | Batch 1480/2990 | Loss: 1949.0763\n",
      "Epoch 2/10 | Batch 1500/2990 | Loss: 3082.2827\n",
      "Epoch 2/10 | Batch 1520/2990 | Loss: 2258.4731\n",
      "Epoch 2/10 | Batch 1540/2990 | Loss: 5307.1768\n",
      "Epoch 2/10 | Batch 1560/2990 | Loss: 1999.8538\n",
      "Epoch 2/10 | Batch 1580/2990 | Loss: 2879.0586\n",
      "Epoch 2/10 | Batch 1600/2990 | Loss: 6413.2036\n",
      "Epoch 2/10 | Batch 1620/2990 | Loss: 4472.8320\n",
      "Epoch 2/10 | Batch 1640/2990 | Loss: 2999.6528\n",
      "Epoch 2/10 | Batch 1660/2990 | Loss: 1788.8453\n",
      "Epoch 2/10 | Batch 1680/2990 | Loss: 6557.3770\n",
      "Epoch 2/10 | Batch 1700/2990 | Loss: 3453.0164\n",
      "Epoch 2/10 | Batch 1720/2990 | Loss: 3580.1306\n",
      "Epoch 2/10 | Batch 1740/2990 | Loss: 6609.3687\n",
      "Epoch 2/10 | Batch 1760/2990 | Loss: 5396.0840\n",
      "Epoch 2/10 | Batch 1780/2990 | Loss: 3506.6562\n",
      "Epoch 2/10 | Batch 1800/2990 | Loss: 4892.7563\n",
      "Epoch 2/10 | Batch 1820/2990 | Loss: 2510.7471\n",
      "Epoch 2/10 | Batch 1840/2990 | Loss: 3305.1587\n",
      "Epoch 2/10 | Batch 1860/2990 | Loss: 2282.5557\n",
      "Epoch 2/10 | Batch 1880/2990 | Loss: 3897.4407\n",
      "Epoch 2/10 | Batch 1900/2990 | Loss: 4012.2307\n",
      "Epoch 2/10 | Batch 1920/2990 | Loss: 2869.8545\n",
      "Epoch 2/10 | Batch 1940/2990 | Loss: 3282.9849\n",
      "Epoch 2/10 | Batch 1960/2990 | Loss: 2009.0054\n",
      "Epoch 2/10 | Batch 1980/2990 | Loss: 2190.1062\n",
      "Epoch 2/10 | Batch 2000/2990 | Loss: 4912.7119\n",
      "Epoch 2/10 | Batch 2020/2990 | Loss: 1596.2705\n",
      "Epoch 2/10 | Batch 2040/2990 | Loss: 2006.6633\n",
      "Epoch 2/10 | Batch 2060/2990 | Loss: 2045.7408\n",
      "Epoch 2/10 | Batch 2080/2990 | Loss: 2022.1279\n",
      "Epoch 2/10 | Batch 2100/2990 | Loss: 5587.7998\n",
      "Epoch 2/10 | Batch 2120/2990 | Loss: 2245.2720\n",
      "Epoch 2/10 | Batch 2140/2990 | Loss: 4185.9473\n",
      "Epoch 2/10 | Batch 2160/2990 | Loss: 2829.0483\n",
      "Epoch 2/10 | Batch 2180/2990 | Loss: 2193.9641\n",
      "Epoch 2/10 | Batch 2200/2990 | Loss: 1396.2019\n",
      "Epoch 2/10 | Batch 2220/2990 | Loss: 5123.2305\n",
      "Epoch 2/10 | Batch 2240/2990 | Loss: 3443.7939\n",
      "Epoch 2/10 | Batch 2260/2990 | Loss: 2816.4614\n",
      "Epoch 2/10 | Batch 2280/2990 | Loss: 5826.8574\n",
      "Epoch 2/10 | Batch 2300/2990 | Loss: 1512.7021\n",
      "Epoch 2/10 | Batch 2320/2990 | Loss: 4350.1377\n",
      "Epoch 2/10 | Batch 2340/2990 | Loss: 3615.7104\n",
      "Epoch 2/10 | Batch 2360/2990 | Loss: 1493.0194\n",
      "Epoch 2/10 | Batch 2380/2990 | Loss: 2146.4878\n",
      "Epoch 2/10 | Batch 2400/2990 | Loss: 2156.7981\n",
      "Epoch 2/10 | Batch 2420/2990 | Loss: 1951.4221\n",
      "Epoch 2/10 | Batch 2440/2990 | Loss: 5390.8125\n",
      "Epoch 2/10 | Batch 2460/2990 | Loss: 5669.8740\n",
      "Epoch 2/10 | Batch 2480/2990 | Loss: 2797.2332\n",
      "Epoch 2/10 | Batch 2500/2990 | Loss: 3537.1062\n",
      "Epoch 2/10 | Batch 2520/2990 | Loss: 2623.4680\n",
      "Epoch 2/10 | Batch 2540/2990 | Loss: 2462.6328\n",
      "Epoch 2/10 | Batch 2560/2990 | Loss: 2219.5991\n",
      "Epoch 2/10 | Batch 2580/2990 | Loss: 2352.2866\n",
      "Epoch 2/10 | Batch 2600/2990 | Loss: 3598.4170\n",
      "Epoch 2/10 | Batch 2620/2990 | Loss: 1372.4395\n",
      "Epoch 2/10 | Batch 2640/2990 | Loss: 1402.9968\n",
      "Epoch 2/10 | Batch 2660/2990 | Loss: 4536.6353\n",
      "Epoch 2/10 | Batch 2680/2990 | Loss: 2432.8582\n",
      "Epoch 2/10 | Batch 2700/2990 | Loss: 4091.3872\n",
      "Epoch 2/10 | Batch 2720/2990 | Loss: 1980.9141\n",
      "Epoch 2/10 | Batch 2740/2990 | Loss: 2323.9771\n",
      "Epoch 2/10 | Batch 2760/2990 | Loss: 2388.8140\n",
      "Epoch 2/10 | Batch 2780/2990 | Loss: 2180.9209\n",
      "Epoch 2/10 | Batch 2800/2990 | Loss: 1344.3417\n",
      "Epoch 2/10 | Batch 2820/2990 | Loss: 3494.8154\n",
      "Epoch 2/10 | Batch 2840/2990 | Loss: 1232.6965\n",
      "Epoch 2/10 | Batch 2860/2990 | Loss: 1570.5002\n",
      "Epoch 2/10 | Batch 2880/2990 | Loss: 1707.3860\n",
      "Epoch 2/10 | Batch 2900/2990 | Loss: 3551.5928\n",
      "Epoch 2/10 | Batch 2920/2990 | Loss: 3553.6296\n",
      "Epoch 2/10 | Batch 2940/2990 | Loss: 5247.5298\n",
      "Epoch 2/10 | Batch 2960/2990 | Loss: 1008.5009\n",
      "Epoch 2/10 | Batch 2980/2990 | Loss: 3473.0027\n",
      "âœ… Epoch 2/10 | Train Loss: 4498.0785 | Val Loss: 2073.4962\n",
      "ðŸ’¾ Saved best model!\n",
      "Epoch 3/10 | Batch 0/2990 | Loss: 836.3409\n",
      "Epoch 3/10 | Batch 20/2990 | Loss: 2129.3909\n",
      "Epoch 3/10 | Batch 40/2990 | Loss: 2395.2173\n",
      "Epoch 3/10 | Batch 60/2990 | Loss: 1318.0645\n",
      "Epoch 3/10 | Batch 80/2990 | Loss: 3143.1648\n",
      "Epoch 3/10 | Batch 100/2990 | Loss: 1590.4323\n",
      "Epoch 3/10 | Batch 120/2990 | Loss: 2240.0288\n",
      "Epoch 3/10 | Batch 140/2990 | Loss: 1332.4200\n",
      "Epoch 3/10 | Batch 160/2990 | Loss: 1247.8254\n",
      "Epoch 3/10 | Batch 180/2990 | Loss: 2455.8647\n",
      "Epoch 3/10 | Batch 200/2990 | Loss: 2726.6455\n",
      "Epoch 3/10 | Batch 220/2990 | Loss: 1450.5544\n",
      "Epoch 3/10 | Batch 240/2990 | Loss: 1317.4512\n",
      "Epoch 3/10 | Batch 260/2990 | Loss: 2815.6084\n",
      "Epoch 3/10 | Batch 280/2990 | Loss: 1995.6202\n",
      "Epoch 3/10 | Batch 300/2990 | Loss: 2181.7158\n",
      "Epoch 3/10 | Batch 320/2990 | Loss: 3736.4258\n",
      "Epoch 3/10 | Batch 340/2990 | Loss: 2040.7037\n",
      "Epoch 3/10 | Batch 360/2990 | Loss: 1404.7964\n",
      "Epoch 3/10 | Batch 380/2990 | Loss: 1188.6296\n",
      "Epoch 3/10 | Batch 400/2990 | Loss: 2193.0801\n",
      "Epoch 3/10 | Batch 420/2990 | Loss: 1283.9495\n",
      "Epoch 3/10 | Batch 440/2990 | Loss: 2658.8608\n",
      "Epoch 3/10 | Batch 460/2990 | Loss: 2032.7169\n",
      "Epoch 3/10 | Batch 480/2990 | Loss: 1202.0208\n",
      "Epoch 3/10 | Batch 500/2990 | Loss: 1149.4750\n",
      "Epoch 3/10 | Batch 520/2990 | Loss: 986.4058\n",
      "Epoch 3/10 | Batch 540/2990 | Loss: 671.5232\n",
      "Epoch 3/10 | Batch 560/2990 | Loss: 748.7415\n",
      "Epoch 3/10 | Batch 580/2990 | Loss: 4587.9790\n",
      "Epoch 3/10 | Batch 600/2990 | Loss: 1246.9556\n",
      "Epoch 3/10 | Batch 620/2990 | Loss: 1775.9760\n",
      "Epoch 3/10 | Batch 640/2990 | Loss: 2167.2314\n",
      "Epoch 3/10 | Batch 660/2990 | Loss: 2245.5703\n",
      "Epoch 3/10 | Batch 680/2990 | Loss: 2294.6978\n",
      "Epoch 3/10 | Batch 700/2990 | Loss: 1835.2395\n",
      "Epoch 3/10 | Batch 720/2990 | Loss: 3970.1257\n",
      "Epoch 3/10 | Batch 740/2990 | Loss: 3728.7808\n",
      "Epoch 3/10 | Batch 760/2990 | Loss: 989.8212\n",
      "Epoch 3/10 | Batch 780/2990 | Loss: 1125.5903\n",
      "Epoch 3/10 | Batch 800/2990 | Loss: 2813.5537\n",
      "Epoch 3/10 | Batch 820/2990 | Loss: 2912.4292\n",
      "Epoch 3/10 | Batch 840/2990 | Loss: 1651.8982\n",
      "Epoch 3/10 | Batch 860/2990 | Loss: 3525.1108\n",
      "Epoch 3/10 | Batch 880/2990 | Loss: 1917.8430\n",
      "Epoch 3/10 | Batch 900/2990 | Loss: 3959.7473\n",
      "Epoch 3/10 | Batch 920/2990 | Loss: 2214.7080\n",
      "Epoch 3/10 | Batch 940/2990 | Loss: 998.1471\n",
      "Epoch 3/10 | Batch 960/2990 | Loss: 1877.9960\n",
      "Epoch 3/10 | Batch 980/2990 | Loss: 2524.1763\n",
      "Epoch 3/10 | Batch 1000/2990 | Loss: 1291.4209\n",
      "Epoch 3/10 | Batch 1020/2990 | Loss: 639.8741\n",
      "Epoch 3/10 | Batch 1040/2990 | Loss: 612.8657\n",
      "Epoch 3/10 | Batch 1060/2990 | Loss: 1557.1936\n",
      "Epoch 3/10 | Batch 1080/2990 | Loss: 1895.7778\n",
      "Epoch 3/10 | Batch 1100/2990 | Loss: 1407.3881\n",
      "Epoch 3/10 | Batch 1120/2990 | Loss: 1664.7131\n",
      "Epoch 3/10 | Batch 1140/2990 | Loss: 2480.6453\n",
      "Epoch 3/10 | Batch 1160/2990 | Loss: 2399.2729\n",
      "Epoch 3/10 | Batch 1180/2990 | Loss: 1001.0352\n",
      "Epoch 3/10 | Batch 1200/2990 | Loss: 1101.6881\n",
      "Epoch 3/10 | Batch 1220/2990 | Loss: 2298.9788\n",
      "Epoch 3/10 | Batch 1240/2990 | Loss: 1350.8770\n",
      "Epoch 3/10 | Batch 1260/2990 | Loss: 2194.8613\n",
      "Epoch 3/10 | Batch 1280/2990 | Loss: 3397.8911\n",
      "Epoch 3/10 | Batch 1300/2990 | Loss: 1596.9077\n",
      "Epoch 3/10 | Batch 1320/2990 | Loss: 1497.9310\n",
      "Epoch 3/10 | Batch 1340/2990 | Loss: 2344.8645\n",
      "Epoch 3/10 | Batch 1360/2990 | Loss: 2805.2183\n",
      "Epoch 3/10 | Batch 1380/2990 | Loss: 1070.2828\n",
      "Epoch 3/10 | Batch 1400/2990 | Loss: 1229.8802\n",
      "Epoch 3/10 | Batch 1420/2990 | Loss: 1011.1714\n",
      "Epoch 3/10 | Batch 1440/2990 | Loss: 1601.4309\n",
      "Epoch 3/10 | Batch 1460/2990 | Loss: 1658.8179\n",
      "Epoch 3/10 | Batch 1480/2990 | Loss: 1043.1488\n",
      "Epoch 3/10 | Batch 1500/2990 | Loss: 1288.1331\n",
      "Epoch 3/10 | Batch 1520/2990 | Loss: 1055.3182\n",
      "Epoch 3/10 | Batch 1540/2990 | Loss: 1516.3826\n",
      "Epoch 3/10 | Batch 1560/2990 | Loss: 883.1193\n",
      "Epoch 3/10 | Batch 1580/2990 | Loss: 1819.6832\n",
      "Epoch 3/10 | Batch 1600/2990 | Loss: 992.9998\n",
      "Epoch 3/10 | Batch 1620/2990 | Loss: 1286.7882\n",
      "Epoch 3/10 | Batch 1640/2990 | Loss: 1413.4565\n",
      "Epoch 3/10 | Batch 1660/2990 | Loss: 3536.2891\n",
      "Epoch 3/10 | Batch 1680/2990 | Loss: 3678.8477\n",
      "Epoch 3/10 | Batch 1700/2990 | Loss: 1454.6030\n",
      "Epoch 3/10 | Batch 1720/2990 | Loss: 920.4160\n",
      "Epoch 3/10 | Batch 1740/2990 | Loss: 1895.5010\n",
      "Epoch 3/10 | Batch 1760/2990 | Loss: 1374.9790\n",
      "Epoch 3/10 | Batch 1780/2990 | Loss: 1325.0737\n",
      "Epoch 3/10 | Batch 1800/2990 | Loss: 2400.8274\n",
      "Epoch 3/10 | Batch 1820/2990 | Loss: 3039.2339\n",
      "Epoch 3/10 | Batch 1840/2990 | Loss: 1199.7607\n",
      "Epoch 3/10 | Batch 1860/2990 | Loss: 1427.3197\n",
      "Epoch 3/10 | Batch 1880/2990 | Loss: 1195.8584\n",
      "Epoch 3/10 | Batch 1900/2990 | Loss: 1375.0071\n",
      "Epoch 3/10 | Batch 1920/2990 | Loss: 1709.4988\n",
      "Epoch 3/10 | Batch 1940/2990 | Loss: 816.8464\n",
      "Epoch 3/10 | Batch 1960/2990 | Loss: 1359.7139\n",
      "Epoch 3/10 | Batch 1980/2990 | Loss: 1229.1953\n",
      "Epoch 3/10 | Batch 2000/2990 | Loss: 1652.1858\n",
      "Epoch 3/10 | Batch 2020/2990 | Loss: 835.1294\n",
      "Epoch 3/10 | Batch 2040/2990 | Loss: 1689.8092\n",
      "Epoch 3/10 | Batch 2060/2990 | Loss: 1281.3357\n",
      "Epoch 3/10 | Batch 2080/2990 | Loss: 1382.6982\n",
      "Epoch 3/10 | Batch 2100/2990 | Loss: 981.9520\n",
      "Epoch 3/10 | Batch 2120/2990 | Loss: 1291.0204\n",
      "Epoch 3/10 | Batch 2140/2990 | Loss: 1264.9559\n",
      "Epoch 3/10 | Batch 2160/2990 | Loss: 1455.9492\n",
      "Epoch 3/10 | Batch 2180/2990 | Loss: 1396.0815\n",
      "Epoch 3/10 | Batch 2200/2990 | Loss: 934.3005\n",
      "Epoch 3/10 | Batch 2220/2990 | Loss: 838.8506\n",
      "Epoch 3/10 | Batch 2240/2990 | Loss: 568.5977\n",
      "Epoch 3/10 | Batch 2260/2990 | Loss: 804.3516\n",
      "Epoch 3/10 | Batch 2280/2990 | Loss: 1149.2957\n",
      "Epoch 3/10 | Batch 2300/2990 | Loss: 1336.5410\n",
      "Epoch 3/10 | Batch 2320/2990 | Loss: 1848.3894\n",
      "Epoch 3/10 | Batch 2340/2990 | Loss: 1145.2561\n",
      "Epoch 3/10 | Batch 2360/2990 | Loss: 1360.3936\n",
      "Epoch 3/10 | Batch 2380/2990 | Loss: 1246.3159\n",
      "Epoch 3/10 | Batch 2400/2990 | Loss: 562.6150\n",
      "Epoch 3/10 | Batch 2420/2990 | Loss: 1550.9219\n",
      "Epoch 3/10 | Batch 2440/2990 | Loss: 1134.1445\n",
      "Epoch 3/10 | Batch 2460/2990 | Loss: 688.0853\n",
      "Epoch 3/10 | Batch 2480/2990 | Loss: 801.5955\n",
      "Epoch 3/10 | Batch 2500/2990 | Loss: 1274.1890\n",
      "Epoch 3/10 | Batch 2520/2990 | Loss: 1024.4640\n",
      "Epoch 3/10 | Batch 2540/2990 | Loss: 1475.0680\n",
      "Epoch 3/10 | Batch 2560/2990 | Loss: 1066.5662\n",
      "Epoch 3/10 | Batch 2580/2990 | Loss: 1009.4354\n",
      "Epoch 3/10 | Batch 2600/2990 | Loss: 697.3671\n",
      "Epoch 3/10 | Batch 2620/2990 | Loss: 1193.2972\n",
      "Epoch 3/10 | Batch 2640/2990 | Loss: 1185.6895\n",
      "Epoch 3/10 | Batch 2660/2990 | Loss: 1009.0692\n",
      "Epoch 3/10 | Batch 2680/2990 | Loss: 1572.8804\n",
      "Epoch 3/10 | Batch 2700/2990 | Loss: 1270.5819\n",
      "Epoch 3/10 | Batch 2720/2990 | Loss: 1513.3910\n",
      "Epoch 3/10 | Batch 2740/2990 | Loss: 1214.5243\n",
      "Epoch 3/10 | Batch 2760/2990 | Loss: 441.1719\n",
      "Epoch 3/10 | Batch 2780/2990 | Loss: 506.3118\n",
      "Epoch 3/10 | Batch 2800/2990 | Loss: 1208.5806\n",
      "Epoch 3/10 | Batch 2820/2990 | Loss: 1367.5779\n",
      "Epoch 3/10 | Batch 2840/2990 | Loss: 700.9653\n",
      "Epoch 3/10 | Batch 2860/2990 | Loss: 634.8974\n",
      "Epoch 3/10 | Batch 2880/2990 | Loss: 514.3552\n",
      "Epoch 3/10 | Batch 2900/2990 | Loss: 777.7016\n",
      "Epoch 3/10 | Batch 2920/2990 | Loss: 1195.9521\n",
      "Epoch 3/10 | Batch 2940/2990 | Loss: 647.0076\n",
      "Epoch 3/10 | Batch 2960/2990 | Loss: 208.6841\n",
      "Epoch 3/10 | Batch 2980/2990 | Loss: 1004.5233\n",
      "âœ… Epoch 3/10 | Train Loss: 1588.3962 | Val Loss: 917.5961\n",
      "ðŸ’¾ Saved best model!\n",
      "Epoch 4/10 | Batch 0/2990 | Loss: 897.7021\n",
      "Epoch 4/10 | Batch 20/2990 | Loss: 391.9686\n",
      "Epoch 4/10 | Batch 40/2990 | Loss: 1231.2742\n",
      "Epoch 4/10 | Batch 60/2990 | Loss: 577.8128\n",
      "Epoch 4/10 | Batch 80/2990 | Loss: 326.8293\n",
      "Epoch 4/10 | Batch 100/2990 | Loss: 817.9972\n",
      "Epoch 4/10 | Batch 120/2990 | Loss: 231.2784\n",
      "Epoch 4/10 | Batch 140/2990 | Loss: 375.8555\n",
      "Epoch 4/10 | Batch 160/2990 | Loss: 715.9079\n",
      "Epoch 4/10 | Batch 180/2990 | Loss: 558.8429\n",
      "Epoch 4/10 | Batch 200/2990 | Loss: 777.3678\n",
      "Epoch 4/10 | Batch 220/2990 | Loss: 636.3983\n",
      "Epoch 4/10 | Batch 240/2990 | Loss: 1221.5813\n",
      "Epoch 4/10 | Batch 260/2990 | Loss: 950.9396\n",
      "Epoch 4/10 | Batch 280/2990 | Loss: 788.9507\n",
      "Epoch 4/10 | Batch 300/2990 | Loss: 502.6992\n",
      "Epoch 4/10 | Batch 320/2990 | Loss: 902.6282\n",
      "Epoch 4/10 | Batch 340/2990 | Loss: 960.7840\n",
      "Epoch 4/10 | Batch 360/2990 | Loss: 768.7046\n",
      "Epoch 4/10 | Batch 380/2990 | Loss: 867.0978\n",
      "Epoch 4/10 | Batch 400/2990 | Loss: 641.9903\n",
      "Epoch 4/10 | Batch 420/2990 | Loss: 771.0422\n",
      "Epoch 4/10 | Batch 440/2990 | Loss: 880.1306\n",
      "Epoch 4/10 | Batch 460/2990 | Loss: 1047.9285\n",
      "Epoch 4/10 | Batch 480/2990 | Loss: 868.1158\n",
      "Epoch 4/10 | Batch 500/2990 | Loss: 245.3123\n",
      "Epoch 4/10 | Batch 520/2990 | Loss: 864.7265\n",
      "Epoch 4/10 | Batch 540/2990 | Loss: 791.5477\n",
      "Epoch 4/10 | Batch 560/2990 | Loss: 505.1310\n",
      "Epoch 4/10 | Batch 580/2990 | Loss: 397.6375\n",
      "Epoch 4/10 | Batch 600/2990 | Loss: 2149.0928\n",
      "Epoch 4/10 | Batch 620/2990 | Loss: 1265.9386\n",
      "Epoch 4/10 | Batch 640/2990 | Loss: 972.6635\n",
      "Epoch 4/10 | Batch 660/2990 | Loss: 398.9659\n",
      "Epoch 4/10 | Batch 680/2990 | Loss: 462.4312\n",
      "Epoch 4/10 | Batch 700/2990 | Loss: 479.2133\n",
      "Epoch 4/10 | Batch 720/2990 | Loss: 567.0820\n",
      "Epoch 4/10 | Batch 740/2990 | Loss: 1164.1235\n",
      "Epoch 4/10 | Batch 760/2990 | Loss: 463.4707\n",
      "Epoch 4/10 | Batch 780/2990 | Loss: 733.5676\n",
      "Epoch 4/10 | Batch 800/2990 | Loss: 702.7830\n",
      "Epoch 4/10 | Batch 820/2990 | Loss: 930.2195\n",
      "Epoch 4/10 | Batch 840/2990 | Loss: 1301.0601\n",
      "Epoch 4/10 | Batch 860/2990 | Loss: 1555.6968\n",
      "Epoch 4/10 | Batch 880/2990 | Loss: 405.5799\n",
      "Epoch 4/10 | Batch 900/2990 | Loss: 619.9741\n",
      "Epoch 4/10 | Batch 920/2990 | Loss: 1081.6599\n",
      "Epoch 4/10 | Batch 940/2990 | Loss: 824.1405\n",
      "Epoch 4/10 | Batch 960/2990 | Loss: 689.3328\n",
      "Epoch 4/10 | Batch 980/2990 | Loss: 1374.3080\n",
      "Epoch 4/10 | Batch 1000/2990 | Loss: 978.1907\n",
      "Epoch 4/10 | Batch 1020/2990 | Loss: 789.4052\n",
      "Epoch 4/10 | Batch 1040/2990 | Loss: 694.0353\n",
      "Epoch 4/10 | Batch 1060/2990 | Loss: 946.6997\n",
      "Epoch 4/10 | Batch 1080/2990 | Loss: 827.6871\n",
      "Epoch 4/10 | Batch 1100/2990 | Loss: 803.4849\n",
      "Epoch 4/10 | Batch 1120/2990 | Loss: 417.0886\n",
      "Epoch 4/10 | Batch 1140/2990 | Loss: 1043.9165\n",
      "Epoch 4/10 | Batch 1160/2990 | Loss: 336.6516\n",
      "Epoch 4/10 | Batch 1180/2990 | Loss: 834.6587\n",
      "Epoch 4/10 | Batch 1200/2990 | Loss: 1107.7758\n",
      "Epoch 4/10 | Batch 1220/2990 | Loss: 324.4096\n",
      "Epoch 4/10 | Batch 1240/2990 | Loss: 655.5084\n",
      "Epoch 4/10 | Batch 1260/2990 | Loss: 785.9355\n",
      "Epoch 4/10 | Batch 1280/2990 | Loss: 225.1536\n",
      "Epoch 4/10 | Batch 1300/2990 | Loss: 497.9863\n",
      "Epoch 4/10 | Batch 1320/2990 | Loss: 1157.0916\n",
      "Epoch 4/10 | Batch 1340/2990 | Loss: 919.8517\n",
      "Epoch 4/10 | Batch 1360/2990 | Loss: 652.6598\n",
      "Epoch 4/10 | Batch 1380/2990 | Loss: 1284.1956\n",
      "Epoch 4/10 | Batch 1400/2990 | Loss: 570.1329\n",
      "Epoch 4/10 | Batch 1420/2990 | Loss: 519.0860\n",
      "Epoch 4/10 | Batch 1440/2990 | Loss: 1602.7872\n",
      "Epoch 4/10 | Batch 1460/2990 | Loss: 590.6007\n",
      "Epoch 4/10 | Batch 1480/2990 | Loss: 970.3027\n",
      "Epoch 4/10 | Batch 1500/2990 | Loss: 1016.7494\n",
      "Epoch 4/10 | Batch 1520/2990 | Loss: 655.1388\n",
      "Epoch 4/10 | Batch 1540/2990 | Loss: 740.6116\n",
      "Epoch 4/10 | Batch 1560/2990 | Loss: 498.5229\n",
      "Epoch 4/10 | Batch 1580/2990 | Loss: 891.2013\n",
      "Epoch 4/10 | Batch 1600/2990 | Loss: 626.3876\n",
      "Epoch 4/10 | Batch 1620/2990 | Loss: 498.6402\n",
      "Epoch 4/10 | Batch 1640/2990 | Loss: 592.1492\n",
      "Epoch 4/10 | Batch 1660/2990 | Loss: 509.0649\n",
      "Epoch 4/10 | Batch 1680/2990 | Loss: 918.0596\n",
      "Epoch 4/10 | Batch 1700/2990 | Loss: 269.2549\n",
      "Epoch 4/10 | Batch 1720/2990 | Loss: 683.2512\n",
      "Epoch 4/10 | Batch 1740/2990 | Loss: 402.0870\n",
      "Epoch 4/10 | Batch 1760/2990 | Loss: 550.0056\n",
      "Epoch 4/10 | Batch 1780/2990 | Loss: 539.4235\n",
      "Epoch 4/10 | Batch 1800/2990 | Loss: 1210.2227\n",
      "Epoch 4/10 | Batch 1820/2990 | Loss: 692.9347\n",
      "Epoch 4/10 | Batch 1840/2990 | Loss: 1491.6586\n",
      "Epoch 4/10 | Batch 1860/2990 | Loss: 631.3250\n",
      "Epoch 4/10 | Batch 1880/2990 | Loss: 206.0282\n",
      "Epoch 4/10 | Batch 1900/2990 | Loss: 645.7521\n",
      "Epoch 4/10 | Batch 1920/2990 | Loss: 521.9883\n",
      "Epoch 4/10 | Batch 1940/2990 | Loss: 268.5008\n",
      "Epoch 4/10 | Batch 1960/2990 | Loss: 346.7714\n",
      "Epoch 4/10 | Batch 1980/2990 | Loss: 892.2289\n",
      "Epoch 4/10 | Batch 2000/2990 | Loss: 1084.8044\n",
      "Epoch 4/10 | Batch 2020/2990 | Loss: 878.3738\n",
      "Epoch 4/10 | Batch 2040/2990 | Loss: 876.1519\n",
      "Epoch 4/10 | Batch 2060/2990 | Loss: 464.3084\n",
      "Epoch 4/10 | Batch 2080/2990 | Loss: 338.3252\n",
      "Epoch 4/10 | Batch 2100/2990 | Loss: 575.9559\n",
      "Epoch 4/10 | Batch 2120/2990 | Loss: 628.7345\n",
      "Epoch 4/10 | Batch 2140/2990 | Loss: 513.0395\n",
      "Epoch 4/10 | Batch 2160/2990 | Loss: 939.1918\n",
      "Epoch 4/10 | Batch 2180/2990 | Loss: 722.1188\n",
      "Epoch 4/10 | Batch 2200/2990 | Loss: 782.8168\n",
      "Epoch 4/10 | Batch 2220/2990 | Loss: 586.8736\n",
      "Epoch 4/10 | Batch 2240/2990 | Loss: 492.2120\n",
      "Epoch 4/10 | Batch 2260/2990 | Loss: 452.6307\n",
      "Epoch 4/10 | Batch 2280/2990 | Loss: 393.6863\n",
      "Epoch 4/10 | Batch 2300/2990 | Loss: 775.9167\n",
      "Epoch 4/10 | Batch 2320/2990 | Loss: 572.8965\n",
      "Epoch 4/10 | Batch 2340/2990 | Loss: 227.2794\n",
      "Epoch 4/10 | Batch 2360/2990 | Loss: 561.9514\n",
      "Epoch 4/10 | Batch 2380/2990 | Loss: 529.2199\n",
      "Epoch 4/10 | Batch 2400/2990 | Loss: 1071.4299\n",
      "Epoch 4/10 | Batch 2420/2990 | Loss: 449.8656\n",
      "Epoch 4/10 | Batch 2440/2990 | Loss: 1136.4650\n",
      "Epoch 4/10 | Batch 2460/2990 | Loss: 591.6935\n",
      "Epoch 4/10 | Batch 2480/2990 | Loss: 202.8525\n",
      "Epoch 4/10 | Batch 2500/2990 | Loss: 1017.6823\n",
      "Epoch 4/10 | Batch 2520/2990 | Loss: 503.0533\n",
      "Epoch 4/10 | Batch 2540/2990 | Loss: 550.9878\n",
      "Epoch 4/10 | Batch 2560/2990 | Loss: 556.7479\n",
      "Epoch 4/10 | Batch 2580/2990 | Loss: 1102.7354\n",
      "Epoch 4/10 | Batch 2600/2990 | Loss: 325.0471\n",
      "Epoch 4/10 | Batch 2620/2990 | Loss: 768.7039\n",
      "Epoch 4/10 | Batch 2640/2990 | Loss: 680.6357\n",
      "Epoch 4/10 | Batch 2660/2990 | Loss: 853.5876\n",
      "Epoch 4/10 | Batch 2680/2990 | Loss: 867.8067\n",
      "Epoch 4/10 | Batch 2700/2990 | Loss: 567.1552\n",
      "Epoch 4/10 | Batch 2720/2990 | Loss: 309.8878\n",
      "Epoch 4/10 | Batch 2740/2990 | Loss: 320.8347\n",
      "Epoch 4/10 | Batch 2760/2990 | Loss: 434.6442\n",
      "Epoch 4/10 | Batch 2780/2990 | Loss: 776.2073\n",
      "Epoch 4/10 | Batch 2800/2990 | Loss: 754.3888\n",
      "Epoch 4/10 | Batch 2820/2990 | Loss: 546.7172\n",
      "Epoch 4/10 | Batch 2840/2990 | Loss: 550.8296\n",
      "Epoch 4/10 | Batch 2860/2990 | Loss: 285.5743\n",
      "Epoch 4/10 | Batch 2880/2990 | Loss: 229.9419\n",
      "Epoch 4/10 | Batch 2900/2990 | Loss: 378.8079\n",
      "Epoch 4/10 | Batch 2920/2990 | Loss: 398.4131\n",
      "Epoch 4/10 | Batch 2940/2990 | Loss: 524.2285\n",
      "Epoch 4/10 | Batch 2960/2990 | Loss: 688.0916\n",
      "Epoch 4/10 | Batch 2980/2990 | Loss: 527.3607\n",
      "âœ… Epoch 4/10 | Train Loss: 679.5268 | Val Loss: 608.3486\n",
      "ðŸ’¾ Saved best model!\n",
      "Epoch 5/10 | Batch 0/2990 | Loss: 402.6736\n",
      "Epoch 5/10 | Batch 20/2990 | Loss: 298.0253\n",
      "Epoch 5/10 | Batch 40/2990 | Loss: 489.0470\n",
      "Epoch 5/10 | Batch 60/2990 | Loss: 304.9376\n",
      "Epoch 5/10 | Batch 80/2990 | Loss: 315.7946\n",
      "Epoch 5/10 | Batch 100/2990 | Loss: 651.4788\n",
      "Epoch 5/10 | Batch 120/2990 | Loss: 143.7247\n",
      "Epoch 5/10 | Batch 140/2990 | Loss: 283.7478\n",
      "Epoch 5/10 | Batch 160/2990 | Loss: 737.8235\n",
      "Epoch 5/10 | Batch 180/2990 | Loss: 358.9196\n",
      "Epoch 5/10 | Batch 200/2990 | Loss: 379.1500\n",
      "Epoch 5/10 | Batch 220/2990 | Loss: 669.9609\n",
      "Epoch 5/10 | Batch 240/2990 | Loss: 385.9511\n",
      "Epoch 5/10 | Batch 260/2990 | Loss: 599.8090\n",
      "Epoch 5/10 | Batch 280/2990 | Loss: 274.1366\n",
      "Epoch 5/10 | Batch 300/2990 | Loss: 628.3173\n",
      "Epoch 5/10 | Batch 320/2990 | Loss: 357.0262\n",
      "Epoch 5/10 | Batch 340/2990 | Loss: 487.2918\n",
      "Epoch 5/10 | Batch 360/2990 | Loss: 104.0937\n",
      "Epoch 5/10 | Batch 380/2990 | Loss: 320.5920\n",
      "Epoch 5/10 | Batch 400/2990 | Loss: 291.0217\n",
      "Epoch 5/10 | Batch 420/2990 | Loss: 467.4462\n",
      "Epoch 5/10 | Batch 440/2990 | Loss: 613.8173\n",
      "Epoch 5/10 | Batch 460/2990 | Loss: 798.4484\n",
      "Epoch 5/10 | Batch 480/2990 | Loss: 706.4930\n",
      "Epoch 5/10 | Batch 500/2990 | Loss: 245.4761\n",
      "Epoch 5/10 | Batch 520/2990 | Loss: 183.8813\n",
      "Epoch 5/10 | Batch 540/2990 | Loss: 385.2887\n",
      "Epoch 5/10 | Batch 560/2990 | Loss: 435.9880\n",
      "Epoch 5/10 | Batch 580/2990 | Loss: 200.1569\n",
      "Epoch 5/10 | Batch 600/2990 | Loss: 241.9293\n",
      "Epoch 5/10 | Batch 620/2990 | Loss: 281.3380\n",
      "Epoch 5/10 | Batch 640/2990 | Loss: 518.6808\n",
      "Epoch 5/10 | Batch 660/2990 | Loss: 302.0998\n",
      "Epoch 5/10 | Batch 680/2990 | Loss: 980.9395\n",
      "Epoch 5/10 | Batch 700/2990 | Loss: 212.5454\n",
      "Epoch 5/10 | Batch 720/2990 | Loss: 647.4100\n",
      "Epoch 5/10 | Batch 740/2990 | Loss: 530.7901\n",
      "Epoch 5/10 | Batch 760/2990 | Loss: 332.7569\n",
      "Epoch 5/10 | Batch 780/2990 | Loss: 179.9634\n",
      "Epoch 5/10 | Batch 800/2990 | Loss: 453.4477\n",
      "Epoch 5/10 | Batch 820/2990 | Loss: 492.1336\n",
      "Epoch 5/10 | Batch 840/2990 | Loss: 282.6555\n",
      "Epoch 5/10 | Batch 860/2990 | Loss: 350.9249\n",
      "Epoch 5/10 | Batch 880/2990 | Loss: 531.0913\n",
      "Epoch 5/10 | Batch 900/2990 | Loss: 635.8665\n",
      "Epoch 5/10 | Batch 920/2990 | Loss: 338.9026\n",
      "Epoch 5/10 | Batch 940/2990 | Loss: 509.8355\n",
      "Epoch 5/10 | Batch 960/2990 | Loss: 229.4512\n",
      "Epoch 5/10 | Batch 980/2990 | Loss: 198.3064\n",
      "Epoch 5/10 | Batch 1000/2990 | Loss: 306.4921\n",
      "Epoch 5/10 | Batch 1020/2990 | Loss: 137.3409\n",
      "Epoch 5/10 | Batch 1040/2990 | Loss: 185.2813\n",
      "Epoch 5/10 | Batch 1060/2990 | Loss: 443.6369\n",
      "Epoch 5/10 | Batch 1080/2990 | Loss: 204.5933\n",
      "Epoch 5/10 | Batch 1100/2990 | Loss: 221.6964\n",
      "Epoch 5/10 | Batch 1120/2990 | Loss: 387.9620\n",
      "Epoch 5/10 | Batch 1140/2990 | Loss: 253.3618\n",
      "Epoch 5/10 | Batch 1160/2990 | Loss: 420.3345\n",
      "Epoch 5/10 | Batch 1180/2990 | Loss: 381.0552\n",
      "Epoch 5/10 | Batch 1200/2990 | Loss: 425.2304\n",
      "Epoch 5/10 | Batch 1220/2990 | Loss: 288.6620\n",
      "Epoch 5/10 | Batch 1240/2990 | Loss: 155.8768\n",
      "Epoch 5/10 | Batch 1260/2990 | Loss: 424.9787\n",
      "Epoch 5/10 | Batch 1280/2990 | Loss: 450.7595\n",
      "Epoch 5/10 | Batch 1300/2990 | Loss: 405.2584\n",
      "Epoch 5/10 | Batch 1320/2990 | Loss: 410.1597\n",
      "Epoch 5/10 | Batch 1340/2990 | Loss: 423.2493\n",
      "Epoch 5/10 | Batch 1360/2990 | Loss: 361.3599\n",
      "Epoch 5/10 | Batch 1380/2990 | Loss: 436.9090\n",
      "Epoch 5/10 | Batch 1400/2990 | Loss: 230.5142\n",
      "Epoch 5/10 | Batch 1420/2990 | Loss: 478.3217\n",
      "Epoch 5/10 | Batch 1440/2990 | Loss: 565.1760\n",
      "Epoch 5/10 | Batch 1460/2990 | Loss: 833.8537\n",
      "Epoch 5/10 | Batch 1480/2990 | Loss: 330.3505\n",
      "Epoch 5/10 | Batch 1500/2990 | Loss: 431.2760\n",
      "Epoch 5/10 | Batch 1520/2990 | Loss: 132.6479\n",
      "Epoch 5/10 | Batch 1540/2990 | Loss: 601.2263\n",
      "Epoch 5/10 | Batch 1560/2990 | Loss: 407.7518\n",
      "Epoch 5/10 | Batch 1580/2990 | Loss: 125.9604\n",
      "Epoch 5/10 | Batch 1600/2990 | Loss: 611.4493\n",
      "Epoch 5/10 | Batch 1620/2990 | Loss: 385.7465\n",
      "Epoch 5/10 | Batch 1640/2990 | Loss: 444.2748\n",
      "Epoch 5/10 | Batch 1660/2990 | Loss: 879.1860\n",
      "Epoch 5/10 | Batch 1680/2990 | Loss: 385.9927\n",
      "Epoch 5/10 | Batch 1700/2990 | Loss: 285.3560\n",
      "Epoch 5/10 | Batch 1720/2990 | Loss: 293.8994\n",
      "Epoch 5/10 | Batch 1740/2990 | Loss: 303.1143\n",
      "Epoch 5/10 | Batch 1760/2990 | Loss: 401.6520\n",
      "Epoch 5/10 | Batch 1780/2990 | Loss: 241.3781\n",
      "Epoch 5/10 | Batch 1800/2990 | Loss: 116.2722\n",
      "Epoch 5/10 | Batch 1820/2990 | Loss: 550.4039\n",
      "Epoch 5/10 | Batch 1840/2990 | Loss: 512.4923\n",
      "Epoch 5/10 | Batch 1860/2990 | Loss: 261.3482\n",
      "Epoch 5/10 | Batch 1880/2990 | Loss: 215.6660\n",
      "Epoch 5/10 | Batch 1900/2990 | Loss: 152.5599\n",
      "Epoch 5/10 | Batch 1920/2990 | Loss: 230.2857\n",
      "Epoch 5/10 | Batch 1940/2990 | Loss: 397.0910\n",
      "Epoch 5/10 | Batch 1960/2990 | Loss: 258.9783\n",
      "Epoch 5/10 | Batch 1980/2990 | Loss: 524.8291\n",
      "Epoch 5/10 | Batch 2000/2990 | Loss: 431.7379\n",
      "Epoch 5/10 | Batch 2020/2990 | Loss: 286.1670\n",
      "Epoch 5/10 | Batch 2040/2990 | Loss: 272.2992\n",
      "Epoch 5/10 | Batch 2060/2990 | Loss: 470.1996\n",
      "Epoch 5/10 | Batch 2080/2990 | Loss: 741.3886\n",
      "Epoch 5/10 | Batch 2100/2990 | Loss: 230.3297\n",
      "Epoch 5/10 | Batch 2120/2990 | Loss: 360.6779\n",
      "Epoch 5/10 | Batch 2140/2990 | Loss: 381.8526\n",
      "Epoch 5/10 | Batch 2160/2990 | Loss: 415.0727\n",
      "Epoch 5/10 | Batch 2180/2990 | Loss: 142.1908\n",
      "Epoch 5/10 | Batch 2200/2990 | Loss: 286.7695\n",
      "Epoch 5/10 | Batch 2220/2990 | Loss: 741.7730\n",
      "Epoch 5/10 | Batch 2240/2990 | Loss: 463.2797\n",
      "Epoch 5/10 | Batch 2260/2990 | Loss: 535.0922\n",
      "Epoch 5/10 | Batch 2280/2990 | Loss: 785.1967\n",
      "Epoch 5/10 | Batch 2300/2990 | Loss: 443.5071\n",
      "Epoch 5/10 | Batch 2320/2990 | Loss: 410.2415\n",
      "Epoch 5/10 | Batch 2340/2990 | Loss: 316.2614\n",
      "Epoch 5/10 | Batch 2360/2990 | Loss: 265.6727\n",
      "Epoch 5/10 | Batch 2380/2990 | Loss: 467.8103\n",
      "Epoch 5/10 | Batch 2400/2990 | Loss: 240.6428\n",
      "Epoch 5/10 | Batch 2420/2990 | Loss: 249.4868\n",
      "Epoch 5/10 | Batch 2440/2990 | Loss: 353.8263\n",
      "Epoch 5/10 | Batch 2460/2990 | Loss: 417.0587\n",
      "Epoch 5/10 | Batch 2480/2990 | Loss: 515.8187\n",
      "Epoch 5/10 | Batch 2500/2990 | Loss: 364.8303\n",
      "Epoch 5/10 | Batch 2520/2990 | Loss: 684.1102\n",
      "Epoch 5/10 | Batch 2540/2990 | Loss: 760.0011\n",
      "Epoch 5/10 | Batch 2560/2990 | Loss: 478.3152\n",
      "Epoch 5/10 | Batch 2580/2990 | Loss: 189.6977\n",
      "Epoch 5/10 | Batch 2600/2990 | Loss: 497.5897\n",
      "Epoch 5/10 | Batch 2620/2990 | Loss: 572.1171\n",
      "Epoch 5/10 | Batch 2640/2990 | Loss: 434.4250\n",
      "Epoch 5/10 | Batch 2660/2990 | Loss: 219.9533\n",
      "Epoch 5/10 | Batch 2680/2990 | Loss: 310.2458\n",
      "Epoch 5/10 | Batch 2700/2990 | Loss: 404.8667\n",
      "Epoch 5/10 | Batch 2720/2990 | Loss: 375.1743\n",
      "Epoch 5/10 | Batch 2740/2990 | Loss: 378.2245\n",
      "Epoch 5/10 | Batch 2760/2990 | Loss: 212.1878\n",
      "Epoch 5/10 | Batch 2780/2990 | Loss: 435.3686\n",
      "Epoch 5/10 | Batch 2800/2990 | Loss: 254.8728\n",
      "Epoch 5/10 | Batch 2820/2990 | Loss: 417.9892\n",
      "Epoch 5/10 | Batch 2840/2990 | Loss: 638.0093\n",
      "Epoch 5/10 | Batch 2860/2990 | Loss: 303.4543\n",
      "Epoch 5/10 | Batch 2880/2990 | Loss: 446.1054\n",
      "Epoch 5/10 | Batch 2900/2990 | Loss: 194.7377\n",
      "Epoch 5/10 | Batch 2920/2990 | Loss: 520.9991\n",
      "Epoch 5/10 | Batch 2940/2990 | Loss: 265.4326\n",
      "Epoch 5/10 | Batch 2960/2990 | Loss: 236.8712\n",
      "Epoch 5/10 | Batch 2980/2990 | Loss: 444.3280\n",
      "âœ… Epoch 5/10 | Train Loss: 407.4934 | Val Loss: 417.6386\n",
      "ðŸ’¾ Saved best model!\n",
      "Epoch 6/10 | Batch 0/2990 | Loss: 249.2476\n",
      "Epoch 6/10 | Batch 20/2990 | Loss: 347.7168\n",
      "Epoch 6/10 | Batch 40/2990 | Loss: 661.7274\n",
      "Epoch 6/10 | Batch 60/2990 | Loss: 620.1995\n",
      "Epoch 6/10 | Batch 80/2990 | Loss: 253.8877\n",
      "Epoch 6/10 | Batch 100/2990 | Loss: 185.2334\n",
      "Epoch 6/10 | Batch 120/2990 | Loss: 276.7425\n",
      "Epoch 6/10 | Batch 140/2990 | Loss: 256.1608\n",
      "Epoch 6/10 | Batch 160/2990 | Loss: 243.0344\n",
      "Epoch 6/10 | Batch 180/2990 | Loss: 333.9590\n",
      "Epoch 6/10 | Batch 200/2990 | Loss: 941.5594\n",
      "Epoch 6/10 | Batch 220/2990 | Loss: 321.8771\n",
      "Epoch 6/10 | Batch 240/2990 | Loss: 165.2273\n",
      "Epoch 6/10 | Batch 260/2990 | Loss: 495.5422\n",
      "Epoch 6/10 | Batch 280/2990 | Loss: 280.1086\n",
      "Epoch 6/10 | Batch 300/2990 | Loss: 333.4848\n",
      "Epoch 6/10 | Batch 320/2990 | Loss: 136.8918\n",
      "Epoch 6/10 | Batch 340/2990 | Loss: 231.8792\n",
      "Epoch 6/10 | Batch 360/2990 | Loss: 341.6380\n",
      "Epoch 6/10 | Batch 380/2990 | Loss: 197.6713\n",
      "Epoch 6/10 | Batch 400/2990 | Loss: 393.6825\n",
      "Epoch 6/10 | Batch 420/2990 | Loss: 159.3923\n",
      "Epoch 6/10 | Batch 440/2990 | Loss: 156.2458\n",
      "Epoch 6/10 | Batch 460/2990 | Loss: 283.5041\n",
      "Epoch 6/10 | Batch 480/2990 | Loss: 333.0829\n",
      "Epoch 6/10 | Batch 500/2990 | Loss: 127.8447\n",
      "Epoch 6/10 | Batch 520/2990 | Loss: 579.0970\n",
      "Epoch 6/10 | Batch 540/2990 | Loss: 275.8276\n",
      "Epoch 6/10 | Batch 560/2990 | Loss: 229.8472\n",
      "Epoch 6/10 | Batch 580/2990 | Loss: 439.8198\n",
      "Epoch 6/10 | Batch 600/2990 | Loss: 409.5791\n",
      "Epoch 6/10 | Batch 620/2990 | Loss: 574.9246\n",
      "Epoch 6/10 | Batch 640/2990 | Loss: 387.1426\n",
      "Epoch 6/10 | Batch 660/2990 | Loss: 311.8353\n",
      "Epoch 6/10 | Batch 680/2990 | Loss: 583.5201\n",
      "Epoch 6/10 | Batch 700/2990 | Loss: 267.6664\n",
      "Epoch 6/10 | Batch 720/2990 | Loss: 142.6602\n",
      "Epoch 6/10 | Batch 740/2990 | Loss: 338.3861\n",
      "Epoch 6/10 | Batch 760/2990 | Loss: 225.8429\n",
      "Epoch 6/10 | Batch 780/2990 | Loss: 144.6998\n",
      "Epoch 6/10 | Batch 800/2990 | Loss: 396.0520\n",
      "Epoch 6/10 | Batch 820/2990 | Loss: 868.2235\n",
      "Epoch 6/10 | Batch 840/2990 | Loss: 291.3456\n",
      "Epoch 6/10 | Batch 860/2990 | Loss: 363.3365\n",
      "Epoch 6/10 | Batch 880/2990 | Loss: 910.2564\n",
      "Epoch 6/10 | Batch 900/2990 | Loss: 227.5594\n",
      "Epoch 6/10 | Batch 920/2990 | Loss: 165.2088\n",
      "Epoch 6/10 | Batch 940/2990 | Loss: 101.3052\n",
      "Epoch 6/10 | Batch 960/2990 | Loss: 323.9172\n",
      "Epoch 6/10 | Batch 980/2990 | Loss: 544.0016\n",
      "Epoch 6/10 | Batch 1000/2990 | Loss: 278.7770\n",
      "Epoch 6/10 | Batch 1020/2990 | Loss: 322.0200\n",
      "Epoch 6/10 | Batch 1040/2990 | Loss: 329.4708\n",
      "Epoch 6/10 | Batch 1060/2990 | Loss: 256.2910\n",
      "Epoch 6/10 | Batch 1080/2990 | Loss: 283.7633\n",
      "Epoch 6/10 | Batch 1100/2990 | Loss: 175.7725\n",
      "Epoch 6/10 | Batch 1120/2990 | Loss: 140.4393\n",
      "Epoch 6/10 | Batch 1140/2990 | Loss: 383.6439\n",
      "Epoch 6/10 | Batch 1160/2990 | Loss: 411.2516\n",
      "Epoch 6/10 | Batch 1180/2990 | Loss: 257.7749\n",
      "Epoch 6/10 | Batch 1200/2990 | Loss: 179.3686\n",
      "Epoch 6/10 | Batch 1220/2990 | Loss: 150.6656\n",
      "Epoch 6/10 | Batch 1240/2990 | Loss: 252.5750\n",
      "Epoch 6/10 | Batch 1260/2990 | Loss: 330.5247\n",
      "Epoch 6/10 | Batch 1280/2990 | Loss: 271.5004\n",
      "Epoch 6/10 | Batch 1300/2990 | Loss: 237.2562\n",
      "Epoch 6/10 | Batch 1320/2990 | Loss: 356.3711\n",
      "Epoch 6/10 | Batch 1340/2990 | Loss: 833.9644\n",
      "Epoch 6/10 | Batch 1360/2990 | Loss: 216.7900\n",
      "Epoch 6/10 | Batch 1380/2990 | Loss: 294.0475\n",
      "Epoch 6/10 | Batch 1400/2990 | Loss: 369.9156\n",
      "Epoch 6/10 | Batch 1420/2990 | Loss: 346.6881\n",
      "Epoch 6/10 | Batch 1440/2990 | Loss: 232.8196\n",
      "Epoch 6/10 | Batch 1460/2990 | Loss: 327.0436\n",
      "Epoch 6/10 | Batch 1480/2990 | Loss: 342.7315\n",
      "Epoch 6/10 | Batch 1500/2990 | Loss: 174.4119\n",
      "Epoch 6/10 | Batch 1520/2990 | Loss: 97.9909\n",
      "Epoch 6/10 | Batch 1540/2990 | Loss: 267.2865\n",
      "Epoch 6/10 | Batch 1560/2990 | Loss: 106.0435\n",
      "Epoch 6/10 | Batch 1580/2990 | Loss: 334.4207\n",
      "Epoch 6/10 | Batch 1600/2990 | Loss: 296.1526\n",
      "Epoch 6/10 | Batch 1620/2990 | Loss: 188.0789\n",
      "Epoch 6/10 | Batch 1640/2990 | Loss: 529.2052\n",
      "Epoch 6/10 | Batch 1660/2990 | Loss: 140.1861\n",
      "Epoch 6/10 | Batch 1680/2990 | Loss: 215.8053\n",
      "Epoch 6/10 | Batch 1700/2990 | Loss: 225.0931\n",
      "Epoch 6/10 | Batch 1720/2990 | Loss: 152.9632\n",
      "Epoch 6/10 | Batch 1740/2990 | Loss: 734.1486\n",
      "Epoch 6/10 | Batch 1760/2990 | Loss: 143.3387\n",
      "Epoch 6/10 | Batch 1780/2990 | Loss: 206.8167\n",
      "Epoch 6/10 | Batch 1800/2990 | Loss: 657.0681\n",
      "Epoch 6/10 | Batch 1820/2990 | Loss: 380.3404\n",
      "Epoch 6/10 | Batch 1840/2990 | Loss: 225.8184\n",
      "Epoch 6/10 | Batch 1860/2990 | Loss: 213.1857\n",
      "Epoch 6/10 | Batch 1880/2990 | Loss: 376.2221\n",
      "Epoch 6/10 | Batch 1900/2990 | Loss: 230.5853\n",
      "Epoch 6/10 | Batch 1920/2990 | Loss: 254.8328\n",
      "Epoch 6/10 | Batch 1940/2990 | Loss: 560.9384\n",
      "Epoch 6/10 | Batch 1960/2990 | Loss: 155.5085\n",
      "Epoch 6/10 | Batch 1980/2990 | Loss: 87.6255\n",
      "Epoch 6/10 | Batch 2000/2990 | Loss: 159.4045\n",
      "Epoch 6/10 | Batch 2020/2990 | Loss: 292.6767\n",
      "Epoch 6/10 | Batch 2040/2990 | Loss: 146.5748\n",
      "Epoch 6/10 | Batch 2060/2990 | Loss: 117.6131\n",
      "Epoch 6/10 | Batch 2080/2990 | Loss: 397.0179\n",
      "Epoch 6/10 | Batch 2100/2990 | Loss: 191.5329\n",
      "Epoch 6/10 | Batch 2120/2990 | Loss: 296.6300\n",
      "Epoch 6/10 | Batch 2140/2990 | Loss: 280.4971\n",
      "Epoch 6/10 | Batch 2160/2990 | Loss: 166.4876\n",
      "Epoch 6/10 | Batch 2180/2990 | Loss: 901.9200\n",
      "Epoch 6/10 | Batch 2200/2990 | Loss: 491.2999\n",
      "Epoch 6/10 | Batch 2220/2990 | Loss: 501.4138\n",
      "Epoch 6/10 | Batch 2240/2990 | Loss: 806.4976\n",
      "Epoch 6/10 | Batch 2260/2990 | Loss: 196.8488\n",
      "Epoch 6/10 | Batch 2280/2990 | Loss: 247.5837\n",
      "Epoch 6/10 | Batch 2300/2990 | Loss: 220.6021\n",
      "Epoch 6/10 | Batch 2320/2990 | Loss: 179.6471\n",
      "Epoch 6/10 | Batch 2340/2990 | Loss: 188.7823\n",
      "Epoch 6/10 | Batch 2360/2990 | Loss: 669.9406\n",
      "Epoch 6/10 | Batch 2380/2990 | Loss: 329.5136\n",
      "Epoch 6/10 | Batch 2400/2990 | Loss: 415.0867\n",
      "Epoch 6/10 | Batch 2420/2990 | Loss: 354.1750\n",
      "Epoch 6/10 | Batch 2440/2990 | Loss: 250.3606\n",
      "Epoch 6/10 | Batch 2460/2990 | Loss: 351.1704\n",
      "Epoch 6/10 | Batch 2480/2990 | Loss: 260.2064\n",
      "Epoch 6/10 | Batch 2500/2990 | Loss: 356.0322\n",
      "Epoch 6/10 | Batch 2520/2990 | Loss: 426.0430\n",
      "Epoch 6/10 | Batch 2540/2990 | Loss: 155.8355\n",
      "Epoch 6/10 | Batch 2560/2990 | Loss: 463.1918\n",
      "Epoch 6/10 | Batch 2580/2990 | Loss: 234.2854\n",
      "Epoch 6/10 | Batch 2600/2990 | Loss: 302.5867\n",
      "Epoch 6/10 | Batch 2620/2990 | Loss: 534.1156\n",
      "Epoch 6/10 | Batch 2640/2990 | Loss: 764.1313\n",
      "Epoch 6/10 | Batch 2660/2990 | Loss: 475.9933\n",
      "Epoch 6/10 | Batch 2680/2990 | Loss: 266.2433\n",
      "Epoch 6/10 | Batch 2700/2990 | Loss: 212.3942\n",
      "Epoch 6/10 | Batch 2720/2990 | Loss: 325.4151\n",
      "Epoch 6/10 | Batch 2740/2990 | Loss: 186.6947\n",
      "Epoch 6/10 | Batch 2760/2990 | Loss: 374.3291\n",
      "Epoch 6/10 | Batch 2780/2990 | Loss: 282.1988\n",
      "Epoch 6/10 | Batch 2800/2990 | Loss: 258.4569\n",
      "Epoch 6/10 | Batch 2820/2990 | Loss: 190.5027\n",
      "Epoch 6/10 | Batch 2840/2990 | Loss: 336.7281\n",
      "Epoch 6/10 | Batch 2860/2990 | Loss: 307.6629\n",
      "Epoch 6/10 | Batch 2880/2990 | Loss: 983.6908\n",
      "Epoch 6/10 | Batch 2900/2990 | Loss: 188.1314\n",
      "Epoch 6/10 | Batch 2920/2990 | Loss: 191.3634\n",
      "Epoch 6/10 | Batch 2940/2990 | Loss: 82.1376\n",
      "Epoch 6/10 | Batch 2960/2990 | Loss: 315.5830\n",
      "Epoch 6/10 | Batch 2980/2990 | Loss: 200.0007\n",
      "âœ… Epoch 6/10 | Train Loss: 296.4953 | Val Loss: 464.9834\n",
      "Epoch 7/10 | Batch 0/2990 | Loss: 320.6191\n",
      "Epoch 7/10 | Batch 20/2990 | Loss: 172.8427\n",
      "Epoch 7/10 | Batch 40/2990 | Loss: 116.9463\n",
      "Epoch 7/10 | Batch 60/2990 | Loss: 192.4112\n",
      "Epoch 7/10 | Batch 80/2990 | Loss: 191.7453\n",
      "Epoch 7/10 | Batch 100/2990 | Loss: 392.7856\n",
      "Epoch 7/10 | Batch 120/2990 | Loss: 450.5760\n",
      "Epoch 7/10 | Batch 140/2990 | Loss: 308.4771\n",
      "Epoch 7/10 | Batch 160/2990 | Loss: 164.1317\n",
      "Epoch 7/10 | Batch 180/2990 | Loss: 263.8066\n",
      "Epoch 7/10 | Batch 200/2990 | Loss: 137.1961\n",
      "Epoch 7/10 | Batch 220/2990 | Loss: 464.9353\n",
      "Epoch 7/10 | Batch 240/2990 | Loss: 128.1301\n",
      "Epoch 7/10 | Batch 260/2990 | Loss: 272.7120\n",
      "Epoch 7/10 | Batch 280/2990 | Loss: 277.3829\n",
      "Epoch 7/10 | Batch 300/2990 | Loss: 156.9184\n",
      "Epoch 7/10 | Batch 320/2990 | Loss: 211.2762\n",
      "Epoch 7/10 | Batch 340/2990 | Loss: 476.6037\n",
      "Epoch 7/10 | Batch 360/2990 | Loss: 261.1192\n",
      "Epoch 7/10 | Batch 380/2990 | Loss: 111.2475\n",
      "Epoch 7/10 | Batch 400/2990 | Loss: 206.0479\n",
      "Epoch 7/10 | Batch 420/2990 | Loss: 408.7047\n",
      "Epoch 7/10 | Batch 440/2990 | Loss: 228.3555\n",
      "Epoch 7/10 | Batch 460/2990 | Loss: 124.0296\n",
      "Epoch 7/10 | Batch 480/2990 | Loss: 157.8070\n",
      "Epoch 7/10 | Batch 500/2990 | Loss: 156.2097\n",
      "Epoch 7/10 | Batch 520/2990 | Loss: 142.9827\n",
      "Epoch 7/10 | Batch 540/2990 | Loss: 212.2080\n",
      "Epoch 7/10 | Batch 560/2990 | Loss: 289.4204\n",
      "Epoch 7/10 | Batch 580/2990 | Loss: 221.5988\n",
      "Epoch 7/10 | Batch 600/2990 | Loss: 106.7853\n",
      "Epoch 7/10 | Batch 620/2990 | Loss: 164.9201\n",
      "Epoch 7/10 | Batch 640/2990 | Loss: 289.7940\n",
      "Epoch 7/10 | Batch 660/2990 | Loss: 488.7139\n",
      "Epoch 7/10 | Batch 680/2990 | Loss: 140.8483\n",
      "Epoch 7/10 | Batch 700/2990 | Loss: 158.5424\n",
      "Epoch 7/10 | Batch 720/2990 | Loss: 334.7677\n",
      "Epoch 7/10 | Batch 740/2990 | Loss: 238.5762\n",
      "Epoch 7/10 | Batch 760/2990 | Loss: 100.9881\n",
      "Epoch 7/10 | Batch 780/2990 | Loss: 344.1670\n",
      "Epoch 7/10 | Batch 800/2990 | Loss: 385.2589\n",
      "Epoch 7/10 | Batch 820/2990 | Loss: 108.6955\n",
      "Epoch 7/10 | Batch 840/2990 | Loss: 181.4714\n",
      "Epoch 7/10 | Batch 860/2990 | Loss: 209.8936\n",
      "Epoch 7/10 | Batch 880/2990 | Loss: 155.2603\n",
      "Epoch 7/10 | Batch 900/2990 | Loss: 521.1254\n",
      "Epoch 7/10 | Batch 920/2990 | Loss: 222.5057\n",
      "Epoch 7/10 | Batch 940/2990 | Loss: 276.3753\n",
      "Epoch 7/10 | Batch 960/2990 | Loss: 368.5242\n",
      "Epoch 7/10 | Batch 980/2990 | Loss: 203.3915\n",
      "Epoch 7/10 | Batch 1000/2990 | Loss: 146.5802\n",
      "Epoch 7/10 | Batch 1020/2990 | Loss: 167.9678\n",
      "Epoch 7/10 | Batch 1040/2990 | Loss: 251.6523\n",
      "Epoch 7/10 | Batch 1060/2990 | Loss: 269.1017\n",
      "Epoch 7/10 | Batch 1080/2990 | Loss: 289.8992\n",
      "Epoch 7/10 | Batch 1100/2990 | Loss: 165.2698\n",
      "Epoch 7/10 | Batch 1120/2990 | Loss: 257.1154\n",
      "Epoch 7/10 | Batch 1140/2990 | Loss: 377.9993\n",
      "Epoch 7/10 | Batch 1160/2990 | Loss: 90.8729\n",
      "Epoch 7/10 | Batch 1180/2990 | Loss: 235.4915\n",
      "Epoch 7/10 | Batch 1200/2990 | Loss: 169.8834\n",
      "Epoch 7/10 | Batch 1220/2990 | Loss: 270.0116\n",
      "Epoch 7/10 | Batch 1240/2990 | Loss: 286.7889\n",
      "Epoch 7/10 | Batch 1260/2990 | Loss: 358.4540\n",
      "Epoch 7/10 | Batch 1280/2990 | Loss: 135.6900\n",
      "Epoch 7/10 | Batch 1300/2990 | Loss: 400.8371\n",
      "Epoch 7/10 | Batch 1320/2990 | Loss: 455.6894\n",
      "Epoch 7/10 | Batch 1340/2990 | Loss: 325.8233\n",
      "Epoch 7/10 | Batch 1360/2990 | Loss: 189.0719\n",
      "Epoch 7/10 | Batch 1380/2990 | Loss: 316.5126\n",
      "Epoch 7/10 | Batch 1400/2990 | Loss: 259.8023\n",
      "Epoch 7/10 | Batch 1420/2990 | Loss: 117.0588\n",
      "Epoch 7/10 | Batch 1440/2990 | Loss: 135.5012\n",
      "Epoch 7/10 | Batch 1460/2990 | Loss: 269.8996\n",
      "Epoch 7/10 | Batch 1480/2990 | Loss: 558.2047\n",
      "Epoch 7/10 | Batch 1500/2990 | Loss: 203.2647\n",
      "Epoch 7/10 | Batch 1520/2990 | Loss: 124.7685\n",
      "Epoch 7/10 | Batch 1540/2990 | Loss: 135.5576\n",
      "Epoch 7/10 | Batch 1560/2990 | Loss: 189.8333\n",
      "Epoch 7/10 | Batch 1580/2990 | Loss: 114.0027\n",
      "Epoch 7/10 | Batch 1600/2990 | Loss: 150.9888\n",
      "Epoch 7/10 | Batch 1620/2990 | Loss: 135.3043\n",
      "Epoch 7/10 | Batch 1640/2990 | Loss: 404.2791\n",
      "Epoch 7/10 | Batch 1660/2990 | Loss: 232.0096\n",
      "Epoch 7/10 | Batch 1680/2990 | Loss: 161.4471\n",
      "Epoch 7/10 | Batch 1700/2990 | Loss: 200.4748\n",
      "Epoch 7/10 | Batch 1720/2990 | Loss: 194.1721\n",
      "Epoch 7/10 | Batch 1740/2990 | Loss: 210.2851\n",
      "Epoch 7/10 | Batch 1760/2990 | Loss: 225.6717\n",
      "Epoch 7/10 | Batch 1780/2990 | Loss: 140.2472\n",
      "Epoch 7/10 | Batch 1800/2990 | Loss: 164.2012\n",
      "Epoch 7/10 | Batch 1820/2990 | Loss: 366.4282\n",
      "Epoch 7/10 | Batch 1840/2990 | Loss: 173.5372\n",
      "Epoch 7/10 | Batch 1860/2990 | Loss: 173.9945\n",
      "Epoch 7/10 | Batch 1880/2990 | Loss: 456.1180\n",
      "Epoch 7/10 | Batch 1900/2990 | Loss: 398.3209\n",
      "Epoch 7/10 | Batch 1920/2990 | Loss: 250.9723\n",
      "Epoch 7/10 | Batch 1940/2990 | Loss: 339.2105\n",
      "Epoch 7/10 | Batch 1960/2990 | Loss: 215.6637\n",
      "Epoch 7/10 | Batch 1980/2990 | Loss: 130.4117\n",
      "Epoch 7/10 | Batch 2000/2990 | Loss: 217.7857\n",
      "Epoch 7/10 | Batch 2020/2990 | Loss: 92.3956\n",
      "Epoch 7/10 | Batch 2040/2990 | Loss: 163.9476\n",
      "Epoch 7/10 | Batch 2060/2990 | Loss: 200.3371\n",
      "Epoch 7/10 | Batch 2080/2990 | Loss: 62.6500\n",
      "Epoch 7/10 | Batch 2100/2990 | Loss: 124.3394\n",
      "Epoch 7/10 | Batch 2120/2990 | Loss: 75.7826\n",
      "Epoch 7/10 | Batch 2140/2990 | Loss: 136.8511\n",
      "Epoch 7/10 | Batch 2160/2990 | Loss: 100.8048\n",
      "Epoch 7/10 | Batch 2180/2990 | Loss: 462.0737\n",
      "Epoch 7/10 | Batch 2200/2990 | Loss: 241.8299\n",
      "Epoch 7/10 | Batch 2220/2990 | Loss: 277.8008\n",
      "Epoch 7/10 | Batch 2240/2990 | Loss: 155.0302\n",
      "Epoch 7/10 | Batch 2260/2990 | Loss: 159.7918\n",
      "Epoch 7/10 | Batch 2280/2990 | Loss: 390.0203\n",
      "Epoch 7/10 | Batch 2300/2990 | Loss: 171.2058\n",
      "Epoch 7/10 | Batch 2320/2990 | Loss: 880.1359\n",
      "Epoch 7/10 | Batch 2340/2990 | Loss: 274.3425\n",
      "Epoch 7/10 | Batch 2360/2990 | Loss: 140.4227\n",
      "Epoch 7/10 | Batch 2380/2990 | Loss: 99.2963\n",
      "Epoch 7/10 | Batch 2400/2990 | Loss: 132.2419\n",
      "Epoch 7/10 | Batch 2420/2990 | Loss: 144.3735\n",
      "Epoch 7/10 | Batch 2440/2990 | Loss: 214.3029\n",
      "Epoch 7/10 | Batch 2460/2990 | Loss: 73.9632\n",
      "Epoch 7/10 | Batch 2480/2990 | Loss: 86.5245\n",
      "Epoch 7/10 | Batch 2500/2990 | Loss: 189.3109\n",
      "Epoch 7/10 | Batch 2520/2990 | Loss: 201.8952\n",
      "Epoch 7/10 | Batch 2540/2990 | Loss: 409.2925\n",
      "Epoch 7/10 | Batch 2560/2990 | Loss: 130.7786\n",
      "Epoch 7/10 | Batch 2580/2990 | Loss: 487.4898\n",
      "Epoch 7/10 | Batch 2600/2990 | Loss: 258.2281\n",
      "Epoch 7/10 | Batch 2620/2990 | Loss: 284.7566\n",
      "Epoch 7/10 | Batch 2640/2990 | Loss: 222.6590\n",
      "Epoch 7/10 | Batch 2660/2990 | Loss: 192.0489\n",
      "Epoch 7/10 | Batch 2680/2990 | Loss: 291.4432\n",
      "Epoch 7/10 | Batch 2700/2990 | Loss: 282.8104\n",
      "Epoch 7/10 | Batch 2720/2990 | Loss: 229.2943\n",
      "Epoch 7/10 | Batch 2740/2990 | Loss: 320.8957\n",
      "Epoch 7/10 | Batch 2760/2990 | Loss: 132.1535\n",
      "Epoch 7/10 | Batch 2780/2990 | Loss: 206.1319\n",
      "Epoch 7/10 | Batch 2800/2990 | Loss: 106.1604\n",
      "Epoch 7/10 | Batch 2820/2990 | Loss: 96.8368\n",
      "Epoch 7/10 | Batch 2840/2990 | Loss: 279.7726\n",
      "Epoch 7/10 | Batch 2860/2990 | Loss: 230.7671\n",
      "Epoch 7/10 | Batch 2880/2990 | Loss: 329.7411\n",
      "Epoch 7/10 | Batch 2900/2990 | Loss: 156.8942\n",
      "Epoch 7/10 | Batch 2920/2990 | Loss: 567.2995\n",
      "Epoch 7/10 | Batch 2940/2990 | Loss: 386.6216\n",
      "Epoch 7/10 | Batch 2960/2990 | Loss: 245.9332\n",
      "Epoch 7/10 | Batch 2980/2990 | Loss: 137.4547\n",
      "âœ… Epoch 7/10 | Train Loss: 232.4516 | Val Loss: 378.2707\n",
      "ðŸ’¾ Saved best model!\n",
      "Epoch 8/10 | Batch 0/2990 | Loss: 240.5845\n",
      "Epoch 8/10 | Batch 20/2990 | Loss: 230.6961\n",
      "Epoch 8/10 | Batch 40/2990 | Loss: 285.1525\n",
      "Epoch 8/10 | Batch 60/2990 | Loss: 141.8657\n",
      "Epoch 8/10 | Batch 80/2990 | Loss: 311.9484\n",
      "Epoch 8/10 | Batch 100/2990 | Loss: 127.5500\n",
      "Epoch 8/10 | Batch 120/2990 | Loss: 174.1849\n",
      "Epoch 8/10 | Batch 140/2990 | Loss: 226.0514\n",
      "Epoch 8/10 | Batch 160/2990 | Loss: 116.4416\n",
      "Epoch 8/10 | Batch 180/2990 | Loss: 149.4127\n",
      "Epoch 8/10 | Batch 200/2990 | Loss: 163.8292\n",
      "Epoch 8/10 | Batch 220/2990 | Loss: 101.0117\n",
      "Epoch 8/10 | Batch 240/2990 | Loss: 136.5048\n",
      "Epoch 8/10 | Batch 260/2990 | Loss: 204.3510\n",
      "Epoch 8/10 | Batch 280/2990 | Loss: 198.8174\n",
      "Epoch 8/10 | Batch 300/2990 | Loss: 85.8364\n",
      "Epoch 8/10 | Batch 320/2990 | Loss: 195.2213\n",
      "Epoch 8/10 | Batch 340/2990 | Loss: 134.1188\n",
      "Epoch 8/10 | Batch 360/2990 | Loss: 155.4678\n",
      "Epoch 8/10 | Batch 380/2990 | Loss: 240.0153\n",
      "Epoch 8/10 | Batch 400/2990 | Loss: 221.4630\n",
      "Epoch 8/10 | Batch 420/2990 | Loss: 740.9907\n",
      "Epoch 8/10 | Batch 440/2990 | Loss: 213.4859\n",
      "Epoch 8/10 | Batch 460/2990 | Loss: 160.9891\n",
      "Epoch 8/10 | Batch 480/2990 | Loss: 178.2982\n",
      "Epoch 8/10 | Batch 500/2990 | Loss: 108.0183\n",
      "Epoch 8/10 | Batch 520/2990 | Loss: 298.7023\n",
      "Epoch 8/10 | Batch 540/2990 | Loss: 154.0661\n",
      "Epoch 8/10 | Batch 560/2990 | Loss: 268.9004\n",
      "Epoch 8/10 | Batch 580/2990 | Loss: 386.4984\n",
      "Epoch 8/10 | Batch 600/2990 | Loss: 186.2926\n",
      "Epoch 8/10 | Batch 620/2990 | Loss: 172.5497\n",
      "Epoch 8/10 | Batch 640/2990 | Loss: 250.7512\n",
      "Epoch 8/10 | Batch 660/2990 | Loss: 210.6165\n",
      "Epoch 8/10 | Batch 680/2990 | Loss: 116.7574\n",
      "Epoch 8/10 | Batch 700/2990 | Loss: 104.2456\n",
      "Epoch 8/10 | Batch 720/2990 | Loss: 164.2018\n",
      "Epoch 8/10 | Batch 740/2990 | Loss: 118.1509\n",
      "Epoch 8/10 | Batch 760/2990 | Loss: 322.6217\n",
      "Epoch 8/10 | Batch 780/2990 | Loss: 540.6484\n",
      "Epoch 8/10 | Batch 800/2990 | Loss: 281.4690\n",
      "Epoch 8/10 | Batch 820/2990 | Loss: 124.4286\n",
      "Epoch 8/10 | Batch 840/2990 | Loss: 109.8008\n",
      "Epoch 8/10 | Batch 860/2990 | Loss: 289.7233\n",
      "Epoch 8/10 | Batch 880/2990 | Loss: 149.9614\n",
      "Epoch 8/10 | Batch 900/2990 | Loss: 381.8029\n",
      "Epoch 8/10 | Batch 920/2990 | Loss: 165.5285\n",
      "Epoch 8/10 | Batch 940/2990 | Loss: 136.9439\n",
      "Epoch 8/10 | Batch 960/2990 | Loss: 208.3361\n",
      "Epoch 8/10 | Batch 980/2990 | Loss: 214.1688\n",
      "Epoch 8/10 | Batch 1000/2990 | Loss: 219.8074\n",
      "Epoch 8/10 | Batch 1020/2990 | Loss: 147.3964\n",
      "Epoch 8/10 | Batch 1040/2990 | Loss: 269.7828\n",
      "Epoch 8/10 | Batch 1060/2990 | Loss: 163.1263\n",
      "Epoch 8/10 | Batch 1080/2990 | Loss: 166.2274\n",
      "Epoch 8/10 | Batch 1100/2990 | Loss: 513.5515\n",
      "Epoch 8/10 | Batch 1120/2990 | Loss: 76.6510\n",
      "Epoch 8/10 | Batch 1140/2990 | Loss: 112.5342\n",
      "Epoch 8/10 | Batch 1160/2990 | Loss: 142.7613\n",
      "Epoch 8/10 | Batch 1180/2990 | Loss: 149.7827\n",
      "Epoch 8/10 | Batch 1200/2990 | Loss: 95.4284\n",
      "Epoch 8/10 | Batch 1220/2990 | Loss: 183.9355\n",
      "Epoch 8/10 | Batch 1240/2990 | Loss: 149.7971\n",
      "Epoch 8/10 | Batch 1260/2990 | Loss: 107.3626\n",
      "Epoch 8/10 | Batch 1280/2990 | Loss: 359.2196\n",
      "Epoch 8/10 | Batch 1300/2990 | Loss: 210.3774\n",
      "Epoch 8/10 | Batch 1320/2990 | Loss: 351.4312\n",
      "Epoch 8/10 | Batch 1340/2990 | Loss: 114.2501\n",
      "Epoch 8/10 | Batch 1360/2990 | Loss: 335.1053\n",
      "Epoch 8/10 | Batch 1380/2990 | Loss: 131.2911\n",
      "Epoch 8/10 | Batch 1400/2990 | Loss: 183.2591\n",
      "Epoch 8/10 | Batch 1420/2990 | Loss: 135.5077\n",
      "Epoch 8/10 | Batch 1440/2990 | Loss: 244.1449\n",
      "Epoch 8/10 | Batch 1460/2990 | Loss: 186.0402\n",
      "Epoch 8/10 | Batch 1480/2990 | Loss: 241.1223\n",
      "Epoch 8/10 | Batch 1500/2990 | Loss: 465.5145\n",
      "Epoch 8/10 | Batch 1520/2990 | Loss: 239.1682\n",
      "Epoch 8/10 | Batch 1540/2990 | Loss: 171.2048\n",
      "Epoch 8/10 | Batch 1560/2990 | Loss: 271.6342\n",
      "Epoch 8/10 | Batch 1580/2990 | Loss: 174.9172\n",
      "Epoch 8/10 | Batch 1600/2990 | Loss: 101.8233\n",
      "Epoch 8/10 | Batch 1620/2990 | Loss: 152.0191\n",
      "Epoch 8/10 | Batch 1640/2990 | Loss: 226.8865\n",
      "Epoch 8/10 | Batch 1660/2990 | Loss: 210.1288\n",
      "Epoch 8/10 | Batch 1680/2990 | Loss: 302.0780\n",
      "Epoch 8/10 | Batch 1700/2990 | Loss: 239.7604\n",
      "Epoch 8/10 | Batch 1720/2990 | Loss: 173.6075\n",
      "Epoch 8/10 | Batch 1740/2990 | Loss: 183.2133\n",
      "Epoch 8/10 | Batch 1760/2990 | Loss: 205.0657\n",
      "Epoch 8/10 | Batch 1780/2990 | Loss: 99.7983\n",
      "Epoch 8/10 | Batch 1800/2990 | Loss: 133.6019\n",
      "Epoch 8/10 | Batch 1820/2990 | Loss: 167.7459\n",
      "Epoch 8/10 | Batch 1840/2990 | Loss: 554.6332\n",
      "Epoch 8/10 | Batch 1860/2990 | Loss: 158.9794\n",
      "Epoch 8/10 | Batch 1880/2990 | Loss: 151.5375\n",
      "Epoch 8/10 | Batch 1900/2990 | Loss: 144.4871\n",
      "Epoch 8/10 | Batch 1920/2990 | Loss: 358.5004\n",
      "Epoch 8/10 | Batch 1940/2990 | Loss: 273.3672\n",
      "Epoch 8/10 | Batch 1960/2990 | Loss: 155.7342\n",
      "Epoch 8/10 | Batch 1980/2990 | Loss: 81.6494\n",
      "Epoch 8/10 | Batch 2000/2990 | Loss: 193.0599\n",
      "Epoch 8/10 | Batch 2020/2990 | Loss: 661.4700\n",
      "Epoch 8/10 | Batch 2040/2990 | Loss: 104.5038\n",
      "Epoch 8/10 | Batch 2060/2990 | Loss: 196.9451\n",
      "Epoch 8/10 | Batch 2080/2990 | Loss: 148.7387\n",
      "Epoch 8/10 | Batch 2100/2990 | Loss: 107.7266\n",
      "Epoch 8/10 | Batch 2120/2990 | Loss: 243.2103\n",
      "Epoch 8/10 | Batch 2140/2990 | Loss: 257.0795\n",
      "Epoch 8/10 | Batch 2160/2990 | Loss: 138.1967\n",
      "Epoch 8/10 | Batch 2180/2990 | Loss: 126.4730\n",
      "Epoch 8/10 | Batch 2200/2990 | Loss: 216.9692\n",
      "Epoch 8/10 | Batch 2220/2990 | Loss: 158.0866\n",
      "Epoch 8/10 | Batch 2240/2990 | Loss: 135.0242\n",
      "Epoch 8/10 | Batch 2260/2990 | Loss: 148.0007\n",
      "Epoch 8/10 | Batch 2280/2990 | Loss: 139.6541\n",
      "Epoch 8/10 | Batch 2300/2990 | Loss: 201.5462\n",
      "Epoch 8/10 | Batch 2320/2990 | Loss: 349.0707\n",
      "Epoch 8/10 | Batch 2340/2990 | Loss: 267.6686\n",
      "Epoch 8/10 | Batch 2360/2990 | Loss: 228.4928\n",
      "Epoch 8/10 | Batch 2380/2990 | Loss: 160.2832\n",
      "Epoch 8/10 | Batch 2400/2990 | Loss: 170.7796\n",
      "Epoch 8/10 | Batch 2420/2990 | Loss: 207.9733\n",
      "Epoch 8/10 | Batch 2440/2990 | Loss: 273.7977\n",
      "Epoch 8/10 | Batch 2460/2990 | Loss: 225.5312\n",
      "Epoch 8/10 | Batch 2480/2990 | Loss: 184.8201\n",
      "Epoch 8/10 | Batch 2500/2990 | Loss: 153.7637\n",
      "Epoch 8/10 | Batch 2520/2990 | Loss: 118.4388\n",
      "Epoch 8/10 | Batch 2540/2990 | Loss: 54.2522\n",
      "Epoch 8/10 | Batch 2560/2990 | Loss: 324.5002\n",
      "Epoch 8/10 | Batch 2580/2990 | Loss: 164.4941\n",
      "Epoch 8/10 | Batch 2600/2990 | Loss: 226.9029\n",
      "Epoch 8/10 | Batch 2620/2990 | Loss: 157.3350\n",
      "Epoch 8/10 | Batch 2640/2990 | Loss: 200.1703\n",
      "Epoch 8/10 | Batch 2660/2990 | Loss: 102.2945\n",
      "Epoch 8/10 | Batch 2680/2990 | Loss: 164.2128\n",
      "Epoch 8/10 | Batch 2700/2990 | Loss: 244.1513\n",
      "Epoch 8/10 | Batch 2720/2990 | Loss: 177.0088\n",
      "Epoch 8/10 | Batch 2740/2990 | Loss: 234.0298\n",
      "Epoch 8/10 | Batch 2760/2990 | Loss: 64.7186\n",
      "Epoch 8/10 | Batch 2780/2990 | Loss: 252.1517\n",
      "Epoch 8/10 | Batch 2800/2990 | Loss: 140.3202\n",
      "Epoch 8/10 | Batch 2820/2990 | Loss: 291.4268\n",
      "Epoch 8/10 | Batch 2840/2990 | Loss: 280.4146\n",
      "Epoch 8/10 | Batch 2860/2990 | Loss: 250.4984\n",
      "Epoch 8/10 | Batch 2880/2990 | Loss: 139.6589\n",
      "Epoch 8/10 | Batch 2900/2990 | Loss: 173.0522\n",
      "Epoch 8/10 | Batch 2920/2990 | Loss: 87.4287\n",
      "Epoch 8/10 | Batch 2940/2990 | Loss: 191.4041\n",
      "Epoch 8/10 | Batch 2960/2990 | Loss: 217.7686\n",
      "Epoch 8/10 | Batch 2980/2990 | Loss: 78.6725\n",
      "âœ… Epoch 8/10 | Train Loss: 198.5355 | Val Loss: 612.9392\n",
      "Epoch 9/10 | Batch 0/2990 | Loss: 491.9436\n",
      "Epoch 9/10 | Batch 20/2990 | Loss: 187.7455\n",
      "Epoch 9/10 | Batch 40/2990 | Loss: 235.1471\n",
      "Epoch 9/10 | Batch 60/2990 | Loss: 160.7629\n",
      "Epoch 9/10 | Batch 80/2990 | Loss: 84.3720\n",
      "Epoch 9/10 | Batch 100/2990 | Loss: 282.0774\n",
      "Epoch 9/10 | Batch 120/2990 | Loss: 123.3359\n",
      "Epoch 9/10 | Batch 140/2990 | Loss: 110.1982\n",
      "Epoch 9/10 | Batch 160/2990 | Loss: 156.8330\n",
      "Epoch 9/10 | Batch 180/2990 | Loss: 57.2536\n",
      "Epoch 9/10 | Batch 200/2990 | Loss: 125.5290\n",
      "Epoch 9/10 | Batch 220/2990 | Loss: 122.2726\n",
      "Epoch 9/10 | Batch 240/2990 | Loss: 67.5919\n",
      "Epoch 9/10 | Batch 260/2990 | Loss: 96.2553\n",
      "Epoch 9/10 | Batch 280/2990 | Loss: 359.5083\n",
      "Epoch 9/10 | Batch 300/2990 | Loss: 113.5673\n",
      "Epoch 9/10 | Batch 320/2990 | Loss: 149.7296\n",
      "Epoch 9/10 | Batch 340/2990 | Loss: 347.2119\n",
      "Epoch 9/10 | Batch 360/2990 | Loss: 135.9617\n",
      "Epoch 9/10 | Batch 380/2990 | Loss: 239.8368\n",
      "Epoch 9/10 | Batch 400/2990 | Loss: 187.0436\n",
      "Epoch 9/10 | Batch 420/2990 | Loss: 126.3310\n",
      "Epoch 9/10 | Batch 440/2990 | Loss: 86.3293\n",
      "Epoch 9/10 | Batch 460/2990 | Loss: 352.4479\n",
      "Epoch 9/10 | Batch 480/2990 | Loss: 163.4222\n",
      "Epoch 9/10 | Batch 500/2990 | Loss: 75.9193\n",
      "Epoch 9/10 | Batch 520/2990 | Loss: 690.6569\n",
      "Epoch 9/10 | Batch 540/2990 | Loss: 81.9179\n",
      "Epoch 9/10 | Batch 560/2990 | Loss: 406.6700\n",
      "Epoch 9/10 | Batch 580/2990 | Loss: 454.0377\n",
      "Epoch 9/10 | Batch 600/2990 | Loss: 168.0384\n",
      "Epoch 9/10 | Batch 620/2990 | Loss: 139.4409\n",
      "Epoch 9/10 | Batch 640/2990 | Loss: 219.9440\n",
      "Epoch 9/10 | Batch 660/2990 | Loss: 154.7556\n",
      "Epoch 9/10 | Batch 680/2990 | Loss: 286.2996\n",
      "Epoch 9/10 | Batch 700/2990 | Loss: 129.2486\n",
      "Epoch 9/10 | Batch 720/2990 | Loss: 139.0221\n",
      "Epoch 9/10 | Batch 740/2990 | Loss: 144.2246\n",
      "Epoch 9/10 | Batch 760/2990 | Loss: 156.5513\n",
      "Epoch 9/10 | Batch 780/2990 | Loss: 271.7729\n",
      "Epoch 9/10 | Batch 800/2990 | Loss: 229.5969\n",
      "Epoch 9/10 | Batch 820/2990 | Loss: 457.5480\n",
      "Epoch 9/10 | Batch 840/2990 | Loss: 72.7772\n",
      "Epoch 9/10 | Batch 860/2990 | Loss: 57.7944\n",
      "Epoch 9/10 | Batch 880/2990 | Loss: 141.8725\n",
      "Epoch 9/10 | Batch 900/2990 | Loss: 252.8387\n",
      "Epoch 9/10 | Batch 920/2990 | Loss: 165.6636\n",
      "Epoch 9/10 | Batch 940/2990 | Loss: 115.3515\n",
      "Epoch 9/10 | Batch 960/2990 | Loss: 80.0153\n",
      "Epoch 9/10 | Batch 980/2990 | Loss: 259.9648\n",
      "Epoch 9/10 | Batch 1000/2990 | Loss: 191.8059\n",
      "Epoch 9/10 | Batch 1020/2990 | Loss: 137.7195\n",
      "Epoch 9/10 | Batch 1040/2990 | Loss: 176.3014\n",
      "Epoch 9/10 | Batch 1060/2990 | Loss: 233.2155\n",
      "Epoch 9/10 | Batch 1080/2990 | Loss: 174.8980\n",
      "Epoch 9/10 | Batch 1100/2990 | Loss: 148.6850\n",
      "Epoch 9/10 | Batch 1120/2990 | Loss: 174.5876\n",
      "Epoch 9/10 | Batch 1140/2990 | Loss: 96.3741\n",
      "Epoch 9/10 | Batch 1160/2990 | Loss: 200.6415\n",
      "Epoch 9/10 | Batch 1180/2990 | Loss: 279.8938\n",
      "Epoch 9/10 | Batch 1200/2990 | Loss: 64.9585\n",
      "Epoch 9/10 | Batch 1220/2990 | Loss: 90.9054\n",
      "Epoch 9/10 | Batch 1240/2990 | Loss: 191.5565\n",
      "Epoch 9/10 | Batch 1260/2990 | Loss: 253.0179\n",
      "Epoch 9/10 | Batch 1280/2990 | Loss: 97.0206\n",
      "Epoch 9/10 | Batch 1300/2990 | Loss: 56.8752\n",
      "Epoch 9/10 | Batch 1320/2990 | Loss: 81.8340\n",
      "Epoch 9/10 | Batch 1340/2990 | Loss: 215.1911\n",
      "Epoch 9/10 | Batch 1360/2990 | Loss: 183.0338\n",
      "Epoch 9/10 | Batch 1380/2990 | Loss: 268.0418\n",
      "Epoch 9/10 | Batch 1400/2990 | Loss: 187.3765\n",
      "Epoch 9/10 | Batch 1420/2990 | Loss: 220.8380\n",
      "Epoch 9/10 | Batch 1440/2990 | Loss: 71.4072\n",
      "Epoch 9/10 | Batch 1460/2990 | Loss: 45.5435\n",
      "Epoch 9/10 | Batch 1480/2990 | Loss: 90.7085\n",
      "Epoch 9/10 | Batch 1500/2990 | Loss: 116.9460\n",
      "Epoch 9/10 | Batch 1520/2990 | Loss: 85.7729\n",
      "Epoch 9/10 | Batch 1540/2990 | Loss: 127.3534\n",
      "Epoch 9/10 | Batch 1560/2990 | Loss: 225.2546\n",
      "Epoch 9/10 | Batch 1580/2990 | Loss: 179.4327\n",
      "Epoch 9/10 | Batch 1600/2990 | Loss: 229.5743\n",
      "Epoch 9/10 | Batch 1620/2990 | Loss: 168.9669\n",
      "Epoch 9/10 | Batch 1640/2990 | Loss: 130.9119\n",
      "Epoch 9/10 | Batch 1660/2990 | Loss: 120.8670\n",
      "Epoch 9/10 | Batch 1680/2990 | Loss: 238.0524\n",
      "Epoch 9/10 | Batch 1700/2990 | Loss: 122.7993\n",
      "Epoch 9/10 | Batch 1720/2990 | Loss: 150.5915\n",
      "Epoch 9/10 | Batch 1740/2990 | Loss: 76.4962\n",
      "Epoch 9/10 | Batch 1760/2990 | Loss: 108.8580\n",
      "Epoch 9/10 | Batch 1780/2990 | Loss: 98.2702\n",
      "Epoch 9/10 | Batch 1800/2990 | Loss: 139.4136\n",
      "Epoch 9/10 | Batch 1820/2990 | Loss: 115.6902\n",
      "Epoch 9/10 | Batch 1840/2990 | Loss: 131.0520\n",
      "Epoch 9/10 | Batch 1860/2990 | Loss: 110.1884\n",
      "Epoch 9/10 | Batch 1880/2990 | Loss: 143.2120\n",
      "Epoch 9/10 | Batch 1900/2990 | Loss: 131.8325\n",
      "Epoch 9/10 | Batch 1920/2990 | Loss: 197.6322\n",
      "Epoch 9/10 | Batch 1940/2990 | Loss: 100.4189\n",
      "Epoch 9/10 | Batch 1960/2990 | Loss: 199.5887\n",
      "Epoch 9/10 | Batch 1980/2990 | Loss: 248.8634\n",
      "Epoch 9/10 | Batch 2000/2990 | Loss: 110.0288\n",
      "Epoch 9/10 | Batch 2020/2990 | Loss: 193.1363\n",
      "Epoch 9/10 | Batch 2040/2990 | Loss: 107.1617\n",
      "Epoch 9/10 | Batch 2060/2990 | Loss: 132.2902\n",
      "Epoch 9/10 | Batch 2080/2990 | Loss: 307.1459\n",
      "Epoch 9/10 | Batch 2100/2990 | Loss: 107.8097\n",
      "Epoch 9/10 | Batch 2120/2990 | Loss: 140.5647\n",
      "Epoch 9/10 | Batch 2140/2990 | Loss: 295.4665\n",
      "Epoch 9/10 | Batch 2160/2990 | Loss: 83.5105\n",
      "Epoch 9/10 | Batch 2180/2990 | Loss: 194.3683\n",
      "Epoch 9/10 | Batch 2200/2990 | Loss: 216.5961\n",
      "Epoch 9/10 | Batch 2220/2990 | Loss: 262.0814\n",
      "Epoch 9/10 | Batch 2240/2990 | Loss: 288.6974\n",
      "Epoch 9/10 | Batch 2260/2990 | Loss: 147.2507\n",
      "Epoch 9/10 | Batch 2280/2990 | Loss: 75.7178\n",
      "Epoch 9/10 | Batch 2300/2990 | Loss: 96.7511\n",
      "Epoch 9/10 | Batch 2320/2990 | Loss: 244.8954\n",
      "Epoch 9/10 | Batch 2340/2990 | Loss: 136.7112\n",
      "Epoch 9/10 | Batch 2360/2990 | Loss: 165.2265\n",
      "Epoch 9/10 | Batch 2380/2990 | Loss: 172.2588\n",
      "Epoch 9/10 | Batch 2400/2990 | Loss: 123.4158\n",
      "Epoch 9/10 | Batch 2420/2990 | Loss: 197.4907\n",
      "Epoch 9/10 | Batch 2440/2990 | Loss: 131.0321\n",
      "Epoch 9/10 | Batch 2460/2990 | Loss: 195.6680\n",
      "Epoch 9/10 | Batch 2480/2990 | Loss: 165.6511\n",
      "Epoch 9/10 | Batch 2500/2990 | Loss: 123.8086\n",
      "Epoch 9/10 | Batch 2520/2990 | Loss: 40.7334\n",
      "Epoch 9/10 | Batch 2540/2990 | Loss: 96.6528\n",
      "Epoch 9/10 | Batch 2560/2990 | Loss: 93.9187\n",
      "Epoch 9/10 | Batch 2580/2990 | Loss: 84.0807\n",
      "Epoch 9/10 | Batch 2600/2990 | Loss: 55.1046\n",
      "Epoch 9/10 | Batch 2620/2990 | Loss: 129.7132\n",
      "Epoch 9/10 | Batch 2640/2990 | Loss: 146.1829\n",
      "Epoch 9/10 | Batch 2660/2990 | Loss: 176.5558\n",
      "Epoch 9/10 | Batch 2680/2990 | Loss: 167.4764\n",
      "Epoch 9/10 | Batch 2700/2990 | Loss: 270.2701\n",
      "Epoch 9/10 | Batch 2720/2990 | Loss: 206.9309\n",
      "Epoch 9/10 | Batch 2740/2990 | Loss: 285.8310\n",
      "Epoch 9/10 | Batch 2760/2990 | Loss: 226.7202\n",
      "Epoch 9/10 | Batch 2780/2990 | Loss: 122.8922\n",
      "Epoch 9/10 | Batch 2800/2990 | Loss: 174.4716\n",
      "Epoch 9/10 | Batch 2820/2990 | Loss: 172.7322\n",
      "Epoch 9/10 | Batch 2840/2990 | Loss: 145.8938\n",
      "Epoch 9/10 | Batch 2860/2990 | Loss: 91.3571\n",
      "Epoch 9/10 | Batch 2880/2990 | Loss: 198.1178\n",
      "Epoch 9/10 | Batch 2900/2990 | Loss: 241.4709\n",
      "Epoch 9/10 | Batch 2920/2990 | Loss: 184.5287\n",
      "Epoch 9/10 | Batch 2940/2990 | Loss: 119.5713\n",
      "Epoch 9/10 | Batch 2960/2990 | Loss: 80.8297\n",
      "Epoch 9/10 | Batch 2980/2990 | Loss: 143.8899\n",
      "âœ… Epoch 9/10 | Train Loss: 169.7131 | Val Loss: 324.1174\n",
      "ðŸ’¾ Saved best model!\n",
      "Epoch 10/10 | Batch 0/2990 | Loss: 44.5775\n",
      "Epoch 10/10 | Batch 20/2990 | Loss: 62.7227\n",
      "Epoch 10/10 | Batch 40/2990 | Loss: 38.8745\n",
      "Epoch 10/10 | Batch 60/2990 | Loss: 104.5822\n",
      "Epoch 10/10 | Batch 80/2990 | Loss: 173.8024\n",
      "Epoch 10/10 | Batch 100/2990 | Loss: 186.1167\n",
      "Epoch 10/10 | Batch 120/2990 | Loss: 126.6436\n",
      "Epoch 10/10 | Batch 140/2990 | Loss: 166.6606\n",
      "Epoch 10/10 | Batch 160/2990 | Loss: 154.2231\n",
      "Epoch 10/10 | Batch 180/2990 | Loss: 29.3300\n",
      "Epoch 10/10 | Batch 200/2990 | Loss: 49.4170\n",
      "Epoch 10/10 | Batch 220/2990 | Loss: 170.1847\n",
      "Epoch 10/10 | Batch 240/2990 | Loss: 116.2079\n",
      "Epoch 10/10 | Batch 260/2990 | Loss: 183.6252\n",
      "Epoch 10/10 | Batch 280/2990 | Loss: 145.8474\n",
      "Epoch 10/10 | Batch 300/2990 | Loss: 116.7123\n",
      "Epoch 10/10 | Batch 320/2990 | Loss: 76.6884\n",
      "Epoch 10/10 | Batch 340/2990 | Loss: 95.1273\n",
      "Epoch 10/10 | Batch 360/2990 | Loss: 138.1779\n",
      "Epoch 10/10 | Batch 380/2990 | Loss: 140.5454\n",
      "Epoch 10/10 | Batch 400/2990 | Loss: 109.4122\n",
      "Epoch 10/10 | Batch 420/2990 | Loss: 89.4080\n",
      "Epoch 10/10 | Batch 440/2990 | Loss: 88.6225\n",
      "Epoch 10/10 | Batch 460/2990 | Loss: 63.5028\n",
      "Epoch 10/10 | Batch 480/2990 | Loss: 126.7307\n",
      "Epoch 10/10 | Batch 500/2990 | Loss: 74.5098\n",
      "Epoch 10/10 | Batch 520/2990 | Loss: 127.7066\n",
      "Epoch 10/10 | Batch 540/2990 | Loss: 132.5697\n",
      "Epoch 10/10 | Batch 560/2990 | Loss: 40.5967\n",
      "Epoch 10/10 | Batch 580/2990 | Loss: 160.3796\n",
      "Epoch 10/10 | Batch 600/2990 | Loss: 66.6834\n",
      "Epoch 10/10 | Batch 620/2990 | Loss: 99.9405\n",
      "Epoch 10/10 | Batch 640/2990 | Loss: 295.5267\n",
      "Epoch 10/10 | Batch 660/2990 | Loss: 186.8598\n",
      "Epoch 10/10 | Batch 680/2990 | Loss: 161.3198\n",
      "Epoch 10/10 | Batch 700/2990 | Loss: 68.9663\n",
      "Epoch 10/10 | Batch 720/2990 | Loss: 134.3412\n",
      "Epoch 10/10 | Batch 740/2990 | Loss: 81.6574\n",
      "Epoch 10/10 | Batch 760/2990 | Loss: 142.8060\n",
      "Epoch 10/10 | Batch 780/2990 | Loss: 212.6674\n",
      "Epoch 10/10 | Batch 800/2990 | Loss: 217.9865\n",
      "Epoch 10/10 | Batch 820/2990 | Loss: 292.1054\n",
      "Epoch 10/10 | Batch 840/2990 | Loss: 65.7633\n",
      "Epoch 10/10 | Batch 860/2990 | Loss: 175.5508\n",
      "Epoch 10/10 | Batch 880/2990 | Loss: 227.3318\n",
      "Epoch 10/10 | Batch 900/2990 | Loss: 122.1179\n",
      "Epoch 10/10 | Batch 920/2990 | Loss: 106.9605\n",
      "Epoch 10/10 | Batch 940/2990 | Loss: 287.9200\n",
      "Epoch 10/10 | Batch 960/2990 | Loss: 251.1214\n",
      "Epoch 10/10 | Batch 980/2990 | Loss: 95.5522\n",
      "Epoch 10/10 | Batch 1000/2990 | Loss: 57.0512\n",
      "Epoch 10/10 | Batch 1020/2990 | Loss: 152.1406\n",
      "Epoch 10/10 | Batch 1040/2990 | Loss: 178.9748\n",
      "Epoch 10/10 | Batch 1060/2990 | Loss: 160.9434\n",
      "Epoch 10/10 | Batch 1080/2990 | Loss: 143.7922\n",
      "Epoch 10/10 | Batch 1100/2990 | Loss: 148.7775\n",
      "Epoch 10/10 | Batch 1120/2990 | Loss: 77.3892\n",
      "Epoch 10/10 | Batch 1140/2990 | Loss: 143.0831\n",
      "Epoch 10/10 | Batch 1160/2990 | Loss: 113.0877\n",
      "Epoch 10/10 | Batch 1180/2990 | Loss: 123.0057\n",
      "Epoch 10/10 | Batch 1200/2990 | Loss: 78.2705\n",
      "Epoch 10/10 | Batch 1220/2990 | Loss: 206.4587\n",
      "Epoch 10/10 | Batch 1240/2990 | Loss: 113.9266\n",
      "Epoch 10/10 | Batch 1260/2990 | Loss: 111.6601\n",
      "Epoch 10/10 | Batch 1280/2990 | Loss: 304.6398\n",
      "Epoch 10/10 | Batch 1300/2990 | Loss: 282.6745\n",
      "Epoch 10/10 | Batch 1320/2990 | Loss: 102.0730\n",
      "Epoch 10/10 | Batch 1340/2990 | Loss: 125.2643\n",
      "Epoch 10/10 | Batch 1360/2990 | Loss: 113.7240\n",
      "Epoch 10/10 | Batch 1380/2990 | Loss: 81.3375\n",
      "Epoch 10/10 | Batch 1400/2990 | Loss: 186.6137\n",
      "Epoch 10/10 | Batch 1420/2990 | Loss: 182.5398\n",
      "Epoch 10/10 | Batch 1440/2990 | Loss: 136.8020\n",
      "Epoch 10/10 | Batch 1460/2990 | Loss: 42.6031\n",
      "Epoch 10/10 | Batch 1480/2990 | Loss: 149.1795\n",
      "Epoch 10/10 | Batch 1500/2990 | Loss: 117.8968\n",
      "Epoch 10/10 | Batch 1520/2990 | Loss: 73.8418\n",
      "Epoch 10/10 | Batch 1540/2990 | Loss: 197.0098\n",
      "Epoch 10/10 | Batch 1560/2990 | Loss: 293.5874\n",
      "Epoch 10/10 | Batch 1580/2990 | Loss: 87.2797\n",
      "Epoch 10/10 | Batch 1600/2990 | Loss: 126.4442\n",
      "Epoch 10/10 | Batch 1620/2990 | Loss: 147.8124\n",
      "Epoch 10/10 | Batch 1640/2990 | Loss: 149.7304\n",
      "Epoch 10/10 | Batch 1660/2990 | Loss: 285.5035\n",
      "Epoch 10/10 | Batch 1680/2990 | Loss: 135.2150\n",
      "Epoch 10/10 | Batch 1700/2990 | Loss: 173.3907\n",
      "Epoch 10/10 | Batch 1720/2990 | Loss: 186.9739\n",
      "Epoch 10/10 | Batch 1740/2990 | Loss: 172.4338\n",
      "Epoch 10/10 | Batch 1760/2990 | Loss: 91.9105\n",
      "Epoch 10/10 | Batch 1780/2990 | Loss: 73.9495\n",
      "Epoch 10/10 | Batch 1800/2990 | Loss: 320.7701\n",
      "Epoch 10/10 | Batch 1820/2990 | Loss: 230.3938\n",
      "Epoch 10/10 | Batch 1840/2990 | Loss: 238.5215\n",
      "Epoch 10/10 | Batch 1860/2990 | Loss: 173.3540\n",
      "Epoch 10/10 | Batch 1880/2990 | Loss: 138.7461\n",
      "Epoch 10/10 | Batch 1900/2990 | Loss: 266.0767\n",
      "Epoch 10/10 | Batch 1920/2990 | Loss: 142.6760\n",
      "Epoch 10/10 | Batch 1940/2990 | Loss: 113.3040\n",
      "Epoch 10/10 | Batch 1960/2990 | Loss: 272.5120\n",
      "Epoch 10/10 | Batch 1980/2990 | Loss: 174.1689\n",
      "Epoch 10/10 | Batch 2000/2990 | Loss: 158.8595\n",
      "Epoch 10/10 | Batch 2020/2990 | Loss: 139.3927\n",
      "Epoch 10/10 | Batch 2040/2990 | Loss: 47.8394\n",
      "Epoch 10/10 | Batch 2060/2990 | Loss: 86.3395\n",
      "Epoch 10/10 | Batch 2080/2990 | Loss: 68.9624\n",
      "Epoch 10/10 | Batch 2100/2990 | Loss: 232.6501\n",
      "Epoch 10/10 | Batch 2120/2990 | Loss: 145.6079\n",
      "Epoch 10/10 | Batch 2140/2990 | Loss: 108.1378\n",
      "Epoch 10/10 | Batch 2160/2990 | Loss: 181.2942\n",
      "Epoch 10/10 | Batch 2180/2990 | Loss: 66.6811\n",
      "Epoch 10/10 | Batch 2200/2990 | Loss: 158.0367\n",
      "Epoch 10/10 | Batch 2220/2990 | Loss: 443.1463\n",
      "Epoch 10/10 | Batch 2240/2990 | Loss: 118.9412\n",
      "Epoch 10/10 | Batch 2260/2990 | Loss: 113.6079\n",
      "Epoch 10/10 | Batch 2280/2990 | Loss: 339.6186\n",
      "Epoch 10/10 | Batch 2300/2990 | Loss: 108.4827\n",
      "Epoch 10/10 | Batch 2320/2990 | Loss: 252.9702\n",
      "Epoch 10/10 | Batch 2340/2990 | Loss: 44.0995\n",
      "Epoch 10/10 | Batch 2360/2990 | Loss: 121.3311\n",
      "Epoch 10/10 | Batch 2380/2990 | Loss: 169.3453\n",
      "Epoch 10/10 | Batch 2400/2990 | Loss: 127.8069\n",
      "Epoch 10/10 | Batch 2420/2990 | Loss: 109.7032\n",
      "Epoch 10/10 | Batch 2440/2990 | Loss: 53.9696\n",
      "Epoch 10/10 | Batch 2460/2990 | Loss: 140.4651\n",
      "Epoch 10/10 | Batch 2480/2990 | Loss: 71.1265\n",
      "Epoch 10/10 | Batch 2500/2990 | Loss: 244.1313\n",
      "Epoch 10/10 | Batch 2520/2990 | Loss: 121.2316\n",
      "Epoch 10/10 | Batch 2540/2990 | Loss: 106.7605\n",
      "Epoch 10/10 | Batch 2560/2990 | Loss: 132.4854\n",
      "Epoch 10/10 | Batch 2580/2990 | Loss: 207.9428\n",
      "Epoch 10/10 | Batch 2600/2990 | Loss: 153.5782\n",
      "Epoch 10/10 | Batch 2620/2990 | Loss: 222.0493\n",
      "Epoch 10/10 | Batch 2640/2990 | Loss: 306.6087\n",
      "Epoch 10/10 | Batch 2660/2990 | Loss: 136.4322\n",
      "Epoch 10/10 | Batch 2680/2990 | Loss: 102.0125\n",
      "Epoch 10/10 | Batch 2700/2990 | Loss: 121.0359\n",
      "Epoch 10/10 | Batch 2720/2990 | Loss: 158.4948\n",
      "Epoch 10/10 | Batch 2740/2990 | Loss: 103.3947\n",
      "Epoch 10/10 | Batch 2760/2990 | Loss: 206.4266\n",
      "Epoch 10/10 | Batch 2780/2990 | Loss: 78.6709\n",
      "Epoch 10/10 | Batch 2800/2990 | Loss: 117.9979\n",
      "Epoch 10/10 | Batch 2820/2990 | Loss: 101.3150\n",
      "Epoch 10/10 | Batch 2840/2990 | Loss: 365.8819\n",
      "Epoch 10/10 | Batch 2860/2990 | Loss: 49.8537\n",
      "Epoch 10/10 | Batch 2880/2990 | Loss: 52.5068\n",
      "Epoch 10/10 | Batch 2900/2990 | Loss: 99.1452\n",
      "Epoch 10/10 | Batch 2920/2990 | Loss: 124.8180\n",
      "Epoch 10/10 | Batch 2940/2990 | Loss: 101.9109\n",
      "Epoch 10/10 | Batch 2960/2990 | Loss: 115.4901\n",
      "Epoch 10/10 | Batch 2980/2990 | Loss: 50.0708\n",
      "âœ… Epoch 10/10 | Train Loss: 153.4688 | Val Loss: 324.2639\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
