{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/.pyenv/versions/3.10.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from simpleCNNModel import WheatEarModel\n",
    "from dataLoaderFunc import loadSplitData, createLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, save_path=\"wheat_ear_model.pth\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float(\"inf\")  # Track the best validation loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Training mode\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Compute validation loss (without gradient updates)\n",
    "        model.eval()  # Switch to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                outputs = model(rgb_batch, dsm_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✅ Model saved with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.load_state_dict(torch.load(\"best_wheat_ear_model.pth\"))\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb_batch, dsm_batch, label_batch in test_loader:\n",
    "            rgb_batch, dsm_batch = rgb_batch.to(\"cuda\"), dsm_batch.to(\"cuda\")\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            actuals.extend(label_batch.cpu().numpy().flatten())\n",
    "\n",
    "    return predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ✅ Universal device selection\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"✅ Using device: {device}\")\n",
    "# Initialize model\n",
    "model = WheatEarModel().to(device)\n",
    "\n",
    "# Loss function (MSE for regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer (Adam works well for deep learning)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 0/2990 | Loss: 101077.9844\n",
      "Epoch 1/10 | Batch 100/2990 | Loss: 7128.2104\n",
      "Epoch 1/10 | Batch 200/2990 | Loss: 10878.1709\n",
      "Epoch 1/10 | Batch 300/2990 | Loss: 11422.9902\n",
      "Epoch 1/10 | Batch 400/2990 | Loss: 9385.7588\n",
      "Epoch 1/10 | Batch 500/2990 | Loss: 8159.3750\n",
      "Epoch 1/10 | Batch 600/2990 | Loss: 9748.4355\n",
      "Epoch 1/10 | Batch 700/2990 | Loss: 7014.3569\n",
      "Epoch 1/10 | Batch 800/2990 | Loss: 11091.7275\n",
      "Epoch 1/10 | Batch 900/2990 | Loss: 3227.2292\n",
      "Epoch 1/10 | Batch 1000/2990 | Loss: 8621.7129\n",
      "Epoch 1/10 | Batch 1100/2990 | Loss: 7272.8960\n",
      "Epoch 1/10 | Batch 1200/2990 | Loss: 9088.9443\n",
      "Epoch 1/10 | Batch 1300/2990 | Loss: 7125.2275\n",
      "Epoch 1/10 | Batch 1400/2990 | Loss: 10909.8594\n",
      "Epoch 1/10 | Batch 1500/2990 | Loss: 3400.9009\n",
      "Epoch 1/10 | Batch 1600/2990 | Loss: 3269.1514\n",
      "Epoch 1/10 | Batch 1700/2990 | Loss: 4018.1504\n",
      "Epoch 1/10 | Batch 1800/2990 | Loss: 6750.3384\n",
      "Epoch 1/10 | Batch 1900/2990 | Loss: 6059.2178\n",
      "Epoch 1/10 | Batch 2000/2990 | Loss: 4347.7017\n",
      "Epoch 1/10 | Batch 2100/2990 | Loss: 2312.0786\n",
      "Epoch 1/10 | Batch 2200/2990 | Loss: 5995.2188\n",
      "Epoch 1/10 | Batch 2300/2990 | Loss: 7120.8696\n",
      "Epoch 1/10 | Batch 2400/2990 | Loss: 5026.3462\n",
      "Epoch 1/10 | Batch 2500/2990 | Loss: 1612.8933\n",
      "Epoch 1/10 | Batch 2600/2990 | Loss: 4193.9844\n",
      "Epoch 1/10 | Batch 2700/2990 | Loss: 3800.9966\n",
      "Epoch 1/10 | Batch 2800/2990 | Loss: 2442.5381\n",
      "Epoch 1/10 | Batch 2900/2990 | Loss: 3311.5095\n",
      "Epoch 1/10, Train Loss: 6616.7379, Val Loss: 4255.6484\n",
      "✅ Model saved with Val Loss: 4255.6484\n",
      "Epoch 2/10 | Batch 0/2990 | Loss: 4020.5732\n",
      "Epoch 2/10 | Batch 100/2990 | Loss: 3509.9268\n",
      "Epoch 2/10 | Batch 200/2990 | Loss: 2536.1399\n",
      "Epoch 2/10 | Batch 300/2990 | Loss: 2483.6057\n",
      "Epoch 2/10 | Batch 400/2990 | Loss: 1725.0981\n",
      "Epoch 2/10 | Batch 500/2990 | Loss: 2853.0269\n",
      "Epoch 2/10 | Batch 600/2990 | Loss: 4329.7710\n",
      "Epoch 2/10 | Batch 700/2990 | Loss: 3919.5293\n",
      "Epoch 2/10 | Batch 800/2990 | Loss: 5304.0796\n",
      "Epoch 2/10 | Batch 900/2990 | Loss: 1647.6487\n",
      "Epoch 2/10 | Batch 1000/2990 | Loss: 4387.5493\n",
      "Epoch 2/10 | Batch 1100/2990 | Loss: 5099.1865\n",
      "Epoch 2/10 | Batch 1200/2990 | Loss: 3353.7891\n",
      "Epoch 2/10 | Batch 1300/2990 | Loss: 4641.9097\n",
      "Epoch 2/10 | Batch 1400/2990 | Loss: 2136.1445\n",
      "Epoch 2/10 | Batch 1500/2990 | Loss: 2017.7537\n",
      "Epoch 2/10 | Batch 1600/2990 | Loss: 1368.0132\n",
      "Epoch 2/10 | Batch 1700/2990 | Loss: 2196.5112\n",
      "Epoch 2/10 | Batch 1800/2990 | Loss: 2284.3342\n",
      "Epoch 2/10 | Batch 1900/2990 | Loss: 2122.2295\n",
      "Epoch 2/10 | Batch 2000/2990 | Loss: 2548.0415\n",
      "Epoch 2/10 | Batch 2100/2990 | Loss: 1926.5033\n",
      "Epoch 2/10 | Batch 2200/2990 | Loss: 1233.2440\n",
      "Epoch 2/10 | Batch 2300/2990 | Loss: 1620.3503\n",
      "Epoch 2/10 | Batch 2400/2990 | Loss: 1046.3933\n",
      "Epoch 2/10 | Batch 2500/2990 | Loss: 1852.6844\n",
      "Epoch 2/10 | Batch 2600/2990 | Loss: 1777.4111\n",
      "Epoch 2/10 | Batch 2700/2990 | Loss: 1884.4910\n",
      "Epoch 2/10 | Batch 2800/2990 | Loss: 1980.1249\n",
      "Epoch 2/10 | Batch 2900/2990 | Loss: 2152.8054\n",
      "Epoch 2/10, Train Loss: 2544.1815, Val Loss: 1573.2219\n",
      "✅ Model saved with Val Loss: 1573.2219\n",
      "Epoch 3/10 | Batch 0/2990 | Loss: 1281.4196\n",
      "Epoch 3/10 | Batch 100/2990 | Loss: 1166.6122\n",
      "Epoch 3/10 | Batch 200/2990 | Loss: 659.7497\n",
      "Epoch 3/10 | Batch 300/2990 | Loss: 671.2203\n",
      "Epoch 3/10 | Batch 400/2990 | Loss: 982.0623\n",
      "Epoch 3/10 | Batch 500/2990 | Loss: 928.2261\n",
      "Epoch 3/10 | Batch 600/2990 | Loss: 1907.9656\n",
      "Epoch 3/10 | Batch 700/2990 | Loss: 1153.7042\n",
      "Epoch 3/10 | Batch 800/2990 | Loss: 370.3026\n",
      "Epoch 3/10 | Batch 900/2990 | Loss: 1985.3003\n",
      "Epoch 3/10 | Batch 1000/2990 | Loss: 434.8699\n",
      "Epoch 3/10 | Batch 1100/2990 | Loss: 924.1313\n",
      "Epoch 3/10 | Batch 1200/2990 | Loss: 1029.2357\n",
      "Epoch 3/10 | Batch 1300/2990 | Loss: 825.3358\n",
      "Epoch 3/10 | Batch 1400/2990 | Loss: 586.9672\n",
      "Epoch 3/10 | Batch 1500/2990 | Loss: 1183.5431\n",
      "Epoch 3/10 | Batch 1600/2990 | Loss: 915.5453\n",
      "Epoch 3/10 | Batch 1700/2990 | Loss: 266.9910\n",
      "Epoch 3/10 | Batch 1800/2990 | Loss: 445.8814\n",
      "Epoch 3/10 | Batch 1900/2990 | Loss: 780.7390\n",
      "Epoch 3/10 | Batch 2000/2990 | Loss: 1487.4688\n",
      "Epoch 3/10 | Batch 2100/2990 | Loss: 582.4788\n",
      "Epoch 3/10 | Batch 2200/2990 | Loss: 1640.0836\n",
      "Epoch 3/10 | Batch 2300/2990 | Loss: 414.0766\n",
      "Epoch 3/10 | Batch 2400/2990 | Loss: 2621.8069\n",
      "Epoch 3/10 | Batch 2500/2990 | Loss: 1827.0015\n",
      "Epoch 3/10 | Batch 2600/2990 | Loss: 625.5935\n",
      "Epoch 3/10 | Batch 2700/2990 | Loss: 863.3749\n",
      "Epoch 3/10 | Batch 2800/2990 | Loss: 870.0927\n",
      "Epoch 3/10 | Batch 2900/2990 | Loss: 313.8038\n",
      "Epoch 3/10, Train Loss: 1004.0880, Val Loss: 1163.0047\n",
      "✅ Model saved with Val Loss: 1163.0047\n",
      "Epoch 4/10 | Batch 0/2990 | Loss: 525.4752\n",
      "Epoch 4/10 | Batch 100/2990 | Loss: 522.6354\n",
      "Epoch 4/10 | Batch 200/2990 | Loss: 365.3838\n",
      "Epoch 4/10 | Batch 300/2990 | Loss: 483.8430\n",
      "Epoch 4/10 | Batch 400/2990 | Loss: 399.5916\n",
      "Epoch 4/10 | Batch 500/2990 | Loss: 400.2017\n",
      "Epoch 4/10 | Batch 600/2990 | Loss: 783.4220\n",
      "Epoch 4/10 | Batch 700/2990 | Loss: 430.8103\n",
      "Epoch 4/10 | Batch 800/2990 | Loss: 399.0799\n",
      "Epoch 4/10 | Batch 900/2990 | Loss: 971.0962\n",
      "Epoch 4/10 | Batch 1000/2990 | Loss: 430.6990\n",
      "Epoch 4/10 | Batch 1100/2990 | Loss: 483.0777\n",
      "Epoch 4/10 | Batch 1200/2990 | Loss: 369.5195\n",
      "Epoch 4/10 | Batch 1300/2990 | Loss: 1169.3214\n",
      "Epoch 4/10 | Batch 1400/2990 | Loss: 442.6172\n",
      "Epoch 4/10 | Batch 1500/2990 | Loss: 1102.8748\n",
      "Epoch 4/10 | Batch 1600/2990 | Loss: 739.2319\n",
      "Epoch 4/10 | Batch 1700/2990 | Loss: 332.1716\n",
      "Epoch 4/10 | Batch 1800/2990 | Loss: 443.9738\n",
      "Epoch 4/10 | Batch 1900/2990 | Loss: 541.4133\n",
      "Epoch 4/10 | Batch 2000/2990 | Loss: 516.7344\n",
      "Epoch 4/10 | Batch 2100/2990 | Loss: 581.8328\n",
      "Epoch 4/10 | Batch 2200/2990 | Loss: 344.6884\n",
      "Epoch 4/10 | Batch 2300/2990 | Loss: 985.8412\n",
      "Epoch 4/10 | Batch 2400/2990 | Loss: 1039.6910\n",
      "Epoch 4/10 | Batch 2500/2990 | Loss: 277.0337\n",
      "Epoch 4/10 | Batch 2600/2990 | Loss: 498.3598\n",
      "Epoch 4/10 | Batch 2700/2990 | Loss: 958.3086\n",
      "Epoch 4/10 | Batch 2800/2990 | Loss: 514.3482\n",
      "Epoch 4/10 | Batch 2900/2990 | Loss: 822.1461\n",
      "Epoch 4/10, Train Loss: 536.7064, Val Loss: 905.4490\n",
      "✅ Model saved with Val Loss: 905.4490\n",
      "Epoch 5/10 | Batch 0/2990 | Loss: 322.0064\n",
      "Epoch 5/10 | Batch 100/2990 | Loss: 690.7670\n",
      "Epoch 5/10 | Batch 200/2990 | Loss: 320.2061\n",
      "Epoch 5/10 | Batch 300/2990 | Loss: 253.1091\n",
      "Epoch 5/10 | Batch 400/2990 | Loss: 254.1369\n",
      "Epoch 5/10 | Batch 500/2990 | Loss: 491.2404\n",
      "Epoch 5/10 | Batch 600/2990 | Loss: 158.0179\n",
      "Epoch 5/10 | Batch 700/2990 | Loss: 261.8548\n",
      "Epoch 5/10 | Batch 800/2990 | Loss: 178.0730\n",
      "Epoch 5/10 | Batch 900/2990 | Loss: 254.4244\n",
      "Epoch 5/10 | Batch 1000/2990 | Loss: 430.8305\n",
      "Epoch 5/10 | Batch 1100/2990 | Loss: 368.8533\n",
      "Epoch 5/10 | Batch 1200/2990 | Loss: 517.6174\n",
      "Epoch 5/10 | Batch 1300/2990 | Loss: 266.1598\n",
      "Epoch 5/10 | Batch 1400/2990 | Loss: 361.6006\n",
      "Epoch 5/10 | Batch 1500/2990 | Loss: 256.5739\n",
      "Epoch 5/10 | Batch 1600/2990 | Loss: 319.0796\n",
      "Epoch 5/10 | Batch 1700/2990 | Loss: 270.6073\n",
      "Epoch 5/10 | Batch 1800/2990 | Loss: 475.0476\n",
      "Epoch 5/10 | Batch 1900/2990 | Loss: 593.4240\n",
      "Epoch 5/10 | Batch 2000/2990 | Loss: 247.8766\n",
      "Epoch 5/10 | Batch 2100/2990 | Loss: 483.8670\n",
      "Epoch 5/10 | Batch 2200/2990 | Loss: 200.0217\n",
      "Epoch 5/10 | Batch 2300/2990 | Loss: 196.6866\n",
      "Epoch 5/10 | Batch 2400/2990 | Loss: 249.0284\n",
      "Epoch 5/10 | Batch 2500/2990 | Loss: 281.3642\n",
      "Epoch 5/10 | Batch 2600/2990 | Loss: 493.7944\n",
      "Epoch 5/10 | Batch 2700/2990 | Loss: 694.2603\n",
      "Epoch 5/10 | Batch 2800/2990 | Loss: 448.3577\n",
      "Epoch 5/10 | Batch 2900/2990 | Loss: 628.5269\n",
      "Epoch 5/10, Train Loss: 388.1176, Val Loss: 887.4559\n",
      "✅ Model saved with Val Loss: 887.4559\n",
      "Epoch 6/10 | Batch 0/2990 | Loss: 488.1366\n",
      "Epoch 6/10 | Batch 100/2990 | Loss: 429.8842\n",
      "Epoch 6/10 | Batch 200/2990 | Loss: 248.3019\n",
      "Epoch 6/10 | Batch 300/2990 | Loss: 278.9099\n",
      "Epoch 6/10 | Batch 400/2990 | Loss: 239.0246\n",
      "Epoch 6/10 | Batch 500/2990 | Loss: 255.9787\n",
      "Epoch 6/10 | Batch 600/2990 | Loss: 361.0475\n",
      "Epoch 6/10 | Batch 700/2990 | Loss: 314.5969\n",
      "Epoch 6/10 | Batch 800/2990 | Loss: 233.9906\n",
      "Epoch 6/10 | Batch 900/2990 | Loss: 184.9099\n",
      "Epoch 6/10 | Batch 1000/2990 | Loss: 236.8827\n",
      "Epoch 6/10 | Batch 1100/2990 | Loss: 195.6446\n",
      "Epoch 6/10 | Batch 1200/2990 | Loss: 393.6817\n",
      "Epoch 6/10 | Batch 1300/2990 | Loss: 233.2350\n",
      "Epoch 6/10 | Batch 1400/2990 | Loss: 170.2099\n",
      "Epoch 6/10 | Batch 1500/2990 | Loss: 198.1705\n",
      "Epoch 6/10 | Batch 1600/2990 | Loss: 276.7511\n",
      "Epoch 6/10 | Batch 1700/2990 | Loss: 523.3761\n",
      "Epoch 6/10 | Batch 1800/2990 | Loss: 351.3213\n",
      "Epoch 6/10 | Batch 1900/2990 | Loss: 130.0719\n",
      "Epoch 6/10 | Batch 2000/2990 | Loss: 186.2021\n",
      "Epoch 6/10 | Batch 2100/2990 | Loss: 466.7786\n",
      "Epoch 6/10 | Batch 2200/2990 | Loss: 376.3624\n",
      "Epoch 6/10 | Batch 2300/2990 | Loss: 443.7784\n",
      "Epoch 6/10 | Batch 2400/2990 | Loss: 312.2454\n",
      "Epoch 6/10 | Batch 2500/2990 | Loss: 298.1110\n",
      "Epoch 6/10 | Batch 2600/2990 | Loss: 174.6817\n",
      "Epoch 6/10 | Batch 2700/2990 | Loss: 183.8737\n",
      "Epoch 6/10 | Batch 2800/2990 | Loss: 501.1671\n",
      "Epoch 6/10 | Batch 2900/2990 | Loss: 826.1605\n",
      "Epoch 6/10, Train Loss: 315.6209, Val Loss: 856.0938\n",
      "✅ Model saved with Val Loss: 856.0938\n",
      "Epoch 7/10 | Batch 0/2990 | Loss: 283.6823\n",
      "Epoch 7/10 | Batch 100/2990 | Loss: 455.9553\n",
      "Epoch 7/10 | Batch 200/2990 | Loss: 203.1804\n",
      "Epoch 7/10 | Batch 300/2990 | Loss: 369.3484\n",
      "Epoch 7/10 | Batch 400/2990 | Loss: 126.0282\n",
      "Epoch 7/10 | Batch 500/2990 | Loss: 262.7594\n",
      "Epoch 7/10 | Batch 600/2990 | Loss: 484.6221\n",
      "Epoch 7/10 | Batch 700/2990 | Loss: 253.2416\n",
      "Epoch 7/10 | Batch 800/2990 | Loss: 128.8651\n",
      "Epoch 7/10 | Batch 900/2990 | Loss: 452.9868\n",
      "Epoch 7/10 | Batch 1000/2990 | Loss: 333.6966\n",
      "Epoch 7/10 | Batch 1100/2990 | Loss: 212.5756\n",
      "Epoch 7/10 | Batch 1200/2990 | Loss: 263.6493\n",
      "Epoch 7/10 | Batch 1300/2990 | Loss: 128.3965\n",
      "Epoch 7/10 | Batch 1400/2990 | Loss: 88.9333\n",
      "Epoch 7/10 | Batch 1500/2990 | Loss: 306.9243\n",
      "Epoch 7/10 | Batch 1600/2990 | Loss: 246.0181\n",
      "Epoch 7/10 | Batch 1700/2990 | Loss: 519.0643\n",
      "Epoch 7/10 | Batch 1800/2990 | Loss: 195.8495\n",
      "Epoch 7/10 | Batch 1900/2990 | Loss: 353.6935\n",
      "Epoch 7/10 | Batch 2000/2990 | Loss: 659.9279\n",
      "Epoch 7/10 | Batch 2100/2990 | Loss: 204.1812\n",
      "Epoch 7/10 | Batch 2200/2990 | Loss: 211.3921\n",
      "Epoch 7/10 | Batch 2300/2990 | Loss: 178.2866\n",
      "Epoch 7/10 | Batch 2400/2990 | Loss: 273.3442\n",
      "Epoch 7/10 | Batch 2500/2990 | Loss: 179.3813\n",
      "Epoch 7/10 | Batch 2600/2990 | Loss: 234.3874\n",
      "Epoch 7/10 | Batch 2700/2990 | Loss: 105.9371\n",
      "Epoch 7/10 | Batch 2800/2990 | Loss: 185.7987\n",
      "Epoch 7/10 | Batch 2900/2990 | Loss: 367.0721\n",
      "Epoch 7/10, Train Loss: 269.9840, Val Loss: 942.3368\n",
      "Epoch 8/10 | Batch 0/2990 | Loss: 288.9628\n",
      "Epoch 8/10 | Batch 100/2990 | Loss: 242.2646\n",
      "Epoch 8/10 | Batch 200/2990 | Loss: 483.0420\n",
      "Epoch 8/10 | Batch 300/2990 | Loss: 106.7866\n",
      "Epoch 8/10 | Batch 400/2990 | Loss: 120.0538\n",
      "Epoch 8/10 | Batch 500/2990 | Loss: 89.9623\n",
      "Epoch 8/10 | Batch 600/2990 | Loss: 137.2583\n",
      "Epoch 8/10 | Batch 700/2990 | Loss: 390.1033\n",
      "Epoch 8/10 | Batch 800/2990 | Loss: 93.2146\n",
      "Epoch 8/10 | Batch 900/2990 | Loss: 93.6022\n",
      "Epoch 8/10 | Batch 1000/2990 | Loss: 378.8604\n",
      "Epoch 8/10 | Batch 1100/2990 | Loss: 126.8535\n",
      "Epoch 8/10 | Batch 1200/2990 | Loss: 182.4520\n",
      "Epoch 8/10 | Batch 1300/2990 | Loss: 245.1673\n",
      "Epoch 8/10 | Batch 1400/2990 | Loss: 132.9582\n",
      "Epoch 8/10 | Batch 1500/2990 | Loss: 85.9515\n",
      "Epoch 8/10 | Batch 1600/2990 | Loss: 201.0362\n",
      "Epoch 8/10 | Batch 1700/2990 | Loss: 196.6859\n",
      "Epoch 8/10 | Batch 1800/2990 | Loss: 191.8874\n",
      "Epoch 8/10 | Batch 1900/2990 | Loss: 232.0837\n",
      "Epoch 8/10 | Batch 2000/2990 | Loss: 467.2514\n",
      "Epoch 8/10 | Batch 2100/2990 | Loss: 295.0596\n",
      "Epoch 8/10 | Batch 2200/2990 | Loss: 162.0002\n",
      "Epoch 8/10 | Batch 2300/2990 | Loss: 106.9699\n",
      "Epoch 8/10 | Batch 2400/2990 | Loss: 263.7710\n",
      "Epoch 8/10 | Batch 2500/2990 | Loss: 191.4602\n",
      "Epoch 8/10 | Batch 2600/2990 | Loss: 196.6415\n",
      "Epoch 8/10 | Batch 2700/2990 | Loss: 258.4058\n",
      "Epoch 8/10 | Batch 2800/2990 | Loss: 247.7944\n",
      "Epoch 8/10 | Batch 2900/2990 | Loss: 233.2332\n",
      "Epoch 8/10, Train Loss: 220.7649, Val Loss: 697.3949\n",
      "✅ Model saved with Val Loss: 697.3949\n",
      "Epoch 9/10 | Batch 0/2990 | Loss: 151.1798\n",
      "Epoch 9/10 | Batch 100/2990 | Loss: 235.0411\n",
      "Epoch 9/10 | Batch 200/2990 | Loss: 168.8302\n",
      "Epoch 9/10 | Batch 300/2990 | Loss: 146.8530\n",
      "Epoch 9/10 | Batch 400/2990 | Loss: 169.0089\n",
      "Epoch 9/10 | Batch 500/2990 | Loss: 358.3914\n",
      "Epoch 9/10 | Batch 600/2990 | Loss: 148.0531\n",
      "Epoch 9/10 | Batch 700/2990 | Loss: 223.9664\n",
      "Epoch 9/10 | Batch 800/2990 | Loss: 166.1080\n",
      "Epoch 9/10 | Batch 900/2990 | Loss: 127.1443\n",
      "Epoch 9/10 | Batch 1000/2990 | Loss: 149.6735\n",
      "Epoch 9/10 | Batch 1100/2990 | Loss: 159.0338\n",
      "Epoch 9/10 | Batch 1200/2990 | Loss: 150.8585\n",
      "Epoch 9/10 | Batch 1300/2990 | Loss: 132.1210\n",
      "Epoch 9/10 | Batch 1400/2990 | Loss: 188.8012\n",
      "Epoch 9/10 | Batch 1500/2990 | Loss: 117.8144\n",
      "Epoch 9/10 | Batch 1600/2990 | Loss: 387.6439\n",
      "Epoch 9/10 | Batch 1700/2990 | Loss: 115.6815\n",
      "Epoch 9/10 | Batch 1800/2990 | Loss: 329.6195\n",
      "Epoch 9/10 | Batch 1900/2990 | Loss: 193.7906\n",
      "Epoch 9/10 | Batch 2000/2990 | Loss: 69.1858\n",
      "Epoch 9/10 | Batch 2100/2990 | Loss: 174.3749\n",
      "Epoch 9/10 | Batch 2200/2990 | Loss: 123.0685\n",
      "Epoch 9/10 | Batch 2300/2990 | Loss: 113.3496\n",
      "Epoch 9/10 | Batch 2400/2990 | Loss: 228.4135\n",
      "Epoch 9/10 | Batch 2500/2990 | Loss: 149.6992\n",
      "Epoch 9/10 | Batch 2600/2990 | Loss: 120.3314\n",
      "Epoch 9/10 | Batch 2700/2990 | Loss: 111.5951\n",
      "Epoch 9/10 | Batch 2800/2990 | Loss: 214.3069\n",
      "Epoch 9/10 | Batch 2900/2990 | Loss: 325.2505\n",
      "Epoch 9/10, Train Loss: 200.4579, Val Loss: 860.6419\n",
      "Epoch 10/10 | Batch 0/2990 | Loss: 215.4512\n",
      "Epoch 10/10 | Batch 100/2990 | Loss: 246.9686\n",
      "Epoch 10/10 | Batch 200/2990 | Loss: 148.9147\n",
      "Epoch 10/10 | Batch 300/2990 | Loss: 61.1591\n",
      "Epoch 10/10 | Batch 400/2990 | Loss: 195.6671\n",
      "Epoch 10/10 | Batch 500/2990 | Loss: 165.4685\n",
      "Epoch 10/10 | Batch 600/2990 | Loss: 60.1699\n",
      "Epoch 10/10 | Batch 700/2990 | Loss: 99.9207\n",
      "Epoch 10/10 | Batch 800/2990 | Loss: 108.9331\n",
      "Epoch 10/10 | Batch 900/2990 | Loss: 107.5192\n",
      "Epoch 10/10 | Batch 1000/2990 | Loss: 92.6772\n",
      "Epoch 10/10 | Batch 1100/2990 | Loss: 118.8771\n",
      "Epoch 10/10 | Batch 1200/2990 | Loss: 43.1298\n",
      "Epoch 10/10 | Batch 1300/2990 | Loss: 320.6250\n",
      "Epoch 10/10 | Batch 1400/2990 | Loss: 107.2357\n",
      "Epoch 10/10 | Batch 1500/2990 | Loss: 302.6795\n",
      "Epoch 10/10 | Batch 1600/2990 | Loss: 118.5787\n",
      "Epoch 10/10 | Batch 1700/2990 | Loss: 372.5610\n",
      "Epoch 10/10 | Batch 1800/2990 | Loss: 246.9081\n",
      "Epoch 10/10 | Batch 1900/2990 | Loss: 221.6056\n",
      "Epoch 10/10 | Batch 2000/2990 | Loss: 128.6761\n",
      "Epoch 10/10 | Batch 2100/2990 | Loss: 73.0106\n",
      "Epoch 10/10 | Batch 2200/2990 | Loss: 116.0381\n",
      "Epoch 10/10 | Batch 2300/2990 | Loss: 379.9060\n",
      "Epoch 10/10 | Batch 2400/2990 | Loss: 528.4377\n",
      "Epoch 10/10 | Batch 2500/2990 | Loss: 95.4047\n",
      "Epoch 10/10 | Batch 2600/2990 | Loss: 105.8801\n",
      "Epoch 10/10 | Batch 2700/2990 | Loss: 127.6939\n",
      "Epoch 10/10 | Batch 2800/2990 | Loss: 65.2167\n",
      "Epoch 10/10 | Batch 2900/2990 | Loss: 168.6721\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, save_path=\"best_wheat_ear_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "preds, actuals = test_model(model, test_loader)\n",
    "\n",
    "# Print some predictions vs actual values\n",
    "for p, a in zip(preds[:10], actuals[:10]):\n",
    "    print(f\"Predicted: {p:.2f}, Actual: {a:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
