{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from simpleCNNModel import WheatEarModel\n",
    "from dataLoaderFunc import loadSplitData, createLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, save_path=\"wheat_ear_model.pth\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float(\"inf\")  # Track the best validation loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Training mode\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Compute validation loss (without gradient updates)\n",
    "        model.eval()  # Switch to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                outputs = model(rgb_batch, dsm_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✅ Model saved with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.load_state_dict(torch.load(\"best_wheat_ear_model.pth\"))\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb_batch, dsm_batch, label_batch in test_loader:\n",
    "            rgb_batch, dsm_batch = rgb_batch.to(\"cuda\"), dsm_batch.to(\"cuda\")\n",
    "            outputs = model(rgb_batch, dsm_batch)\n",
    "            predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            actuals.extend(label_batch.cpu().numpy().flatten())\n",
    "\n",
    "    return predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ✅ Universal device selection\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"✅ Using device: {device}\")\n",
    "# Initialize model\n",
    "model = WheatEarModel().to(device)\n",
    "\n",
    "# Loss function (MSE for regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer (Adam works well for deep learning)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 0/2990 | Loss: 111478.0156\n",
      "Epoch 1/10 | Batch 100/2990 | Loss: 6957.0142\n",
      "Epoch 1/10 | Batch 200/2990 | Loss: 8915.6318\n",
      "Epoch 1/10 | Batch 300/2990 | Loss: 9131.6328\n",
      "Epoch 1/10 | Batch 400/2990 | Loss: 6565.3145\n",
      "Epoch 1/10 | Batch 500/2990 | Loss: 6786.3848\n",
      "Epoch 1/10 | Batch 600/2990 | Loss: 4269.6709\n",
      "Epoch 1/10 | Batch 700/2990 | Loss: 5123.5640\n",
      "Epoch 1/10 | Batch 800/2990 | Loss: 4177.0840\n",
      "Epoch 1/10 | Batch 900/2990 | Loss: 8313.9141\n",
      "Epoch 1/10 | Batch 1000/2990 | Loss: 13006.9844\n",
      "Epoch 1/10 | Batch 1100/2990 | Loss: 5856.0137\n",
      "Epoch 1/10 | Batch 1200/2990 | Loss: 4568.6504\n",
      "Epoch 1/10 | Batch 1300/2990 | Loss: 18484.0332\n",
      "Epoch 1/10 | Batch 1400/2990 | Loss: 6287.9297\n",
      "Epoch 1/10 | Batch 1500/2990 | Loss: 5185.4282\n",
      "Epoch 1/10 | Batch 1600/2990 | Loss: 8028.2178\n",
      "Epoch 1/10 | Batch 1700/2990 | Loss: 4412.6914\n",
      "Epoch 1/10 | Batch 1800/2990 | Loss: 6282.4507\n",
      "Epoch 1/10 | Batch 1900/2990 | Loss: 4541.4067\n",
      "Epoch 1/10 | Batch 2000/2990 | Loss: 5378.6934\n",
      "Epoch 1/10 | Batch 2100/2990 | Loss: 5326.3691\n",
      "Epoch 1/10 | Batch 2200/2990 | Loss: 4746.7637\n",
      "Epoch 1/10 | Batch 2300/2990 | Loss: 4179.4912\n",
      "Epoch 1/10 | Batch 2400/2990 | Loss: 9467.5732\n",
      "Epoch 1/10 | Batch 2500/2990 | Loss: 2645.7693\n",
      "Epoch 1/10 | Batch 2600/2990 | Loss: 5316.1816\n",
      "Epoch 1/10 | Batch 2700/2990 | Loss: 3221.2476\n",
      "Epoch 1/10 | Batch 2800/2990 | Loss: 5436.8271\n",
      "Epoch 1/10 | Batch 2900/2990 | Loss: 3316.7124\n",
      "Epoch 1/10, Train Loss: 6659.1816, Val Loss: 4768.6264\n",
      "✅ Model saved with Val Loss: 4768.6264\n",
      "Epoch 2/10 | Batch 0/2990 | Loss: 2558.4419\n",
      "Epoch 2/10 | Batch 100/2990 | Loss: 2234.1416\n",
      "Epoch 2/10 | Batch 200/2990 | Loss: 3186.8669\n",
      "Epoch 2/10 | Batch 300/2990 | Loss: 5654.5083\n",
      "Epoch 2/10 | Batch 400/2990 | Loss: 4867.5312\n",
      "Epoch 2/10 | Batch 500/2990 | Loss: 5204.8301\n",
      "Epoch 2/10 | Batch 600/2990 | Loss: 1301.7090\n",
      "Epoch 2/10 | Batch 700/2990 | Loss: 1723.9288\n",
      "Epoch 2/10 | Batch 800/2990 | Loss: 1070.6807\n",
      "Epoch 2/10 | Batch 900/2990 | Loss: 2666.8579\n",
      "Epoch 2/10 | Batch 1000/2990 | Loss: 4679.9453\n",
      "Epoch 2/10 | Batch 1100/2990 | Loss: 3706.7246\n",
      "Epoch 2/10 | Batch 1200/2990 | Loss: 2640.0369\n",
      "Epoch 2/10 | Batch 1300/2990 | Loss: 3880.2358\n",
      "Epoch 2/10 | Batch 1400/2990 | Loss: 5919.3174\n",
      "Epoch 2/10 | Batch 1500/2990 | Loss: 2025.0271\n",
      "Epoch 2/10 | Batch 1600/2990 | Loss: 1601.0198\n",
      "Epoch 2/10 | Batch 1700/2990 | Loss: 4316.3916\n",
      "Epoch 2/10 | Batch 1800/2990 | Loss: 963.7047\n",
      "Epoch 2/10 | Batch 1900/2990 | Loss: 2828.6028\n",
      "Epoch 2/10 | Batch 2000/2990 | Loss: 2664.0942\n",
      "Epoch 2/10 | Batch 2100/2990 | Loss: 1094.3579\n",
      "Epoch 2/10 | Batch 2200/2990 | Loss: 1517.5691\n",
      "Epoch 2/10 | Batch 2300/2990 | Loss: 2103.2061\n",
      "Epoch 2/10 | Batch 2400/2990 | Loss: 1608.0978\n",
      "Epoch 2/10 | Batch 2500/2990 | Loss: 1657.3290\n",
      "Epoch 2/10 | Batch 2600/2990 | Loss: 1204.3207\n",
      "Epoch 2/10 | Batch 2700/2990 | Loss: 2178.3906\n",
      "Epoch 2/10 | Batch 2800/2990 | Loss: 2250.2329\n",
      "Epoch 2/10 | Batch 2900/2990 | Loss: 2806.3550\n",
      "Epoch 2/10, Train Loss: 2761.6143, Val Loss: 2064.1559\n",
      "✅ Model saved with Val Loss: 2064.1559\n",
      "Epoch 3/10 | Batch 0/2990 | Loss: 913.5973\n",
      "Epoch 3/10 | Batch 100/2990 | Loss: 1796.9768\n",
      "Epoch 3/10 | Batch 200/2990 | Loss: 506.8434\n",
      "Epoch 3/10 | Batch 300/2990 | Loss: 431.3339\n",
      "Epoch 3/10 | Batch 400/2990 | Loss: 1137.1307\n",
      "Epoch 3/10 | Batch 500/2990 | Loss: 2224.5166\n",
      "Epoch 3/10 | Batch 600/2990 | Loss: 504.6846\n",
      "Epoch 3/10 | Batch 700/2990 | Loss: 1167.6470\n",
      "Epoch 3/10 | Batch 800/2990 | Loss: 766.6447\n",
      "Epoch 3/10 | Batch 900/2990 | Loss: 2724.1709\n",
      "Epoch 3/10 | Batch 1000/2990 | Loss: 751.4180\n",
      "Epoch 3/10 | Batch 1100/2990 | Loss: 467.6898\n",
      "Epoch 3/10 | Batch 1200/2990 | Loss: 640.8584\n",
      "Epoch 3/10 | Batch 1300/2990 | Loss: 809.4268\n",
      "Epoch 3/10 | Batch 1400/2990 | Loss: 837.6614\n",
      "Epoch 3/10 | Batch 1500/2990 | Loss: 957.5701\n",
      "Epoch 3/10 | Batch 1600/2990 | Loss: 625.9979\n",
      "Epoch 3/10 | Batch 1700/2990 | Loss: 303.4287\n",
      "Epoch 3/10 | Batch 1800/2990 | Loss: 135.9995\n",
      "Epoch 3/10 | Batch 1900/2990 | Loss: 1108.7803\n",
      "Epoch 3/10 | Batch 2000/2990 | Loss: 766.8923\n",
      "Epoch 3/10 | Batch 2100/2990 | Loss: 909.3657\n",
      "Epoch 3/10 | Batch 2200/2990 | Loss: 887.1118\n",
      "Epoch 3/10 | Batch 2300/2990 | Loss: 1645.8059\n",
      "Epoch 3/10 | Batch 2400/2990 | Loss: 1161.9413\n",
      "Epoch 3/10 | Batch 2500/2990 | Loss: 1136.8735\n",
      "Epoch 3/10 | Batch 2600/2990 | Loss: 604.6337\n",
      "Epoch 3/10 | Batch 2700/2990 | Loss: 826.9810\n",
      "Epoch 3/10 | Batch 2800/2990 | Loss: 977.2377\n",
      "Epoch 3/10 | Batch 2900/2990 | Loss: 1069.9185\n",
      "Epoch 3/10, Train Loss: 1026.7247, Val Loss: 1110.8025\n",
      "✅ Model saved with Val Loss: 1110.8025\n",
      "Epoch 4/10 | Batch 0/2990 | Loss: 582.0943\n",
      "Epoch 4/10 | Batch 100/2990 | Loss: 481.2315\n",
      "Epoch 4/10 | Batch 200/2990 | Loss: 360.5275\n",
      "Epoch 4/10 | Batch 300/2990 | Loss: 507.5464\n",
      "Epoch 4/10 | Batch 400/2990 | Loss: 887.5785\n",
      "Epoch 4/10 | Batch 500/2990 | Loss: 709.1726\n",
      "Epoch 4/10 | Batch 600/2990 | Loss: 504.2027\n",
      "Epoch 4/10 | Batch 700/2990 | Loss: 575.8560\n",
      "Epoch 4/10 | Batch 800/2990 | Loss: 494.7362\n",
      "Epoch 4/10 | Batch 900/2990 | Loss: 638.4042\n",
      "Epoch 4/10 | Batch 1000/2990 | Loss: 369.5778\n",
      "Epoch 4/10 | Batch 1100/2990 | Loss: 1108.1265\n",
      "Epoch 4/10 | Batch 1200/2990 | Loss: 583.3761\n",
      "Epoch 4/10 | Batch 1300/2990 | Loss: 361.4437\n",
      "Epoch 4/10 | Batch 1400/2990 | Loss: 261.3684\n",
      "Epoch 4/10 | Batch 1500/2990 | Loss: 940.4771\n",
      "Epoch 4/10 | Batch 1600/2990 | Loss: 747.6609\n",
      "Epoch 4/10 | Batch 1700/2990 | Loss: 1175.4353\n",
      "Epoch 4/10 | Batch 1800/2990 | Loss: 349.6220\n",
      "Epoch 4/10 | Batch 1900/2990 | Loss: 371.7998\n",
      "Epoch 4/10 | Batch 2000/2990 | Loss: 422.8004\n",
      "Epoch 4/10 | Batch 2100/2990 | Loss: 320.2796\n",
      "Epoch 4/10 | Batch 2200/2990 | Loss: 432.8257\n",
      "Epoch 4/10 | Batch 2300/2990 | Loss: 250.3931\n",
      "Epoch 4/10 | Batch 2400/2990 | Loss: 578.0771\n",
      "Epoch 4/10 | Batch 2500/2990 | Loss: 232.8600\n",
      "Epoch 4/10 | Batch 2600/2990 | Loss: 236.0805\n",
      "Epoch 4/10 | Batch 2700/2990 | Loss: 824.6318\n",
      "Epoch 4/10 | Batch 2800/2990 | Loss: 295.7275\n",
      "Epoch 4/10 | Batch 2900/2990 | Loss: 373.7186\n",
      "Epoch 4/10, Train Loss: 522.0299, Val Loss: 967.2725\n",
      "✅ Model saved with Val Loss: 967.2725\n",
      "Epoch 5/10 | Batch 0/2990 | Loss: 310.9212\n",
      "Epoch 5/10 | Batch 100/2990 | Loss: 560.3440\n",
      "Epoch 5/10 | Batch 200/2990 | Loss: 269.7477\n",
      "Epoch 5/10 | Batch 300/2990 | Loss: 348.6680\n",
      "Epoch 5/10 | Batch 400/2990 | Loss: 204.6157\n",
      "Epoch 5/10 | Batch 500/2990 | Loss: 406.1708\n",
      "Epoch 5/10 | Batch 600/2990 | Loss: 273.8770\n",
      "Epoch 5/10 | Batch 700/2990 | Loss: 263.0606\n",
      "Epoch 5/10 | Batch 800/2990 | Loss: 332.5155\n",
      "Epoch 5/10 | Batch 900/2990 | Loss: 154.2673\n",
      "Epoch 5/10 | Batch 1000/2990 | Loss: 215.0534\n",
      "Epoch 5/10 | Batch 1100/2990 | Loss: 524.3658\n",
      "Epoch 5/10 | Batch 1200/2990 | Loss: 256.5250\n",
      "Epoch 5/10 | Batch 1300/2990 | Loss: 474.5123\n",
      "Epoch 5/10 | Batch 1400/2990 | Loss: 510.3115\n",
      "Epoch 5/10 | Batch 1500/2990 | Loss: 491.8055\n",
      "Epoch 5/10 | Batch 1600/2990 | Loss: 774.1954\n",
      "Epoch 5/10 | Batch 1700/2990 | Loss: 642.1289\n",
      "Epoch 5/10 | Batch 1800/2990 | Loss: 190.6865\n",
      "Epoch 5/10 | Batch 1900/2990 | Loss: 175.5989\n",
      "Epoch 5/10 | Batch 2000/2990 | Loss: 358.3683\n",
      "Epoch 5/10 | Batch 2100/2990 | Loss: 423.8096\n",
      "Epoch 5/10 | Batch 2200/2990 | Loss: 257.8449\n",
      "Epoch 5/10 | Batch 2300/2990 | Loss: 533.9695\n",
      "Epoch 5/10 | Batch 2400/2990 | Loss: 245.5422\n",
      "Epoch 5/10 | Batch 2500/2990 | Loss: 159.2966\n",
      "Epoch 5/10 | Batch 2600/2990 | Loss: 363.2132\n",
      "Epoch 5/10 | Batch 2700/2990 | Loss: 422.2632\n",
      "Epoch 5/10 | Batch 2800/2990 | Loss: 388.3977\n",
      "Epoch 5/10 | Batch 2900/2990 | Loss: 192.7195\n",
      "Epoch 5/10, Train Loss: 371.4293, Val Loss: 908.6816\n",
      "✅ Model saved with Val Loss: 908.6816\n",
      "Epoch 6/10 | Batch 0/2990 | Loss: 271.5622\n",
      "Epoch 6/10 | Batch 100/2990 | Loss: 419.4976\n",
      "Epoch 6/10 | Batch 200/2990 | Loss: 264.3593\n",
      "Epoch 6/10 | Batch 300/2990 | Loss: 300.5679\n",
      "Epoch 6/10 | Batch 400/2990 | Loss: 210.5355\n",
      "Epoch 6/10 | Batch 500/2990 | Loss: 259.7484\n",
      "Epoch 6/10 | Batch 600/2990 | Loss: 151.9060\n",
      "Epoch 6/10 | Batch 700/2990 | Loss: 587.7802\n",
      "Epoch 6/10 | Batch 800/2990 | Loss: 279.6006\n",
      "Epoch 6/10 | Batch 900/2990 | Loss: 249.6177\n",
      "Epoch 6/10 | Batch 1000/2990 | Loss: 172.0697\n",
      "Epoch 6/10 | Batch 1100/2990 | Loss: 313.8504\n",
      "Epoch 6/10 | Batch 1200/2990 | Loss: 406.0178\n",
      "Epoch 6/10 | Batch 1300/2990 | Loss: 628.7367\n",
      "Epoch 6/10 | Batch 1400/2990 | Loss: 221.7155\n",
      "Epoch 6/10 | Batch 1500/2990 | Loss: 366.7825\n",
      "Epoch 6/10 | Batch 1600/2990 | Loss: 344.2495\n",
      "Epoch 6/10 | Batch 1700/2990 | Loss: 370.3049\n",
      "Epoch 6/10 | Batch 1800/2990 | Loss: 568.2936\n",
      "Epoch 6/10 | Batch 1900/2990 | Loss: 213.7012\n",
      "Epoch 6/10 | Batch 2000/2990 | Loss: 144.7785\n",
      "Epoch 6/10 | Batch 2100/2990 | Loss: 130.1788\n",
      "Epoch 6/10 | Batch 2200/2990 | Loss: 135.8637\n",
      "Epoch 6/10 | Batch 2300/2990 | Loss: 184.6873\n",
      "Epoch 6/10 | Batch 2400/2990 | Loss: 307.9920\n",
      "Epoch 6/10 | Batch 2500/2990 | Loss: 297.8883\n",
      "Epoch 6/10 | Batch 2600/2990 | Loss: 345.1640\n",
      "Epoch 6/10 | Batch 2700/2990 | Loss: 459.6320\n",
      "Epoch 6/10 | Batch 2800/2990 | Loss: 298.4789\n",
      "Epoch 6/10 | Batch 2900/2990 | Loss: 213.3020\n",
      "Epoch 6/10, Train Loss: 306.4926, Val Loss: 769.3583\n",
      "✅ Model saved with Val Loss: 769.3583\n",
      "Epoch 7/10 | Batch 0/2990 | Loss: 163.5663\n",
      "Epoch 7/10 | Batch 100/2990 | Loss: 150.4970\n",
      "Epoch 7/10 | Batch 200/2990 | Loss: 193.1730\n",
      "Epoch 7/10 | Batch 300/2990 | Loss: 286.7129\n",
      "Epoch 7/10 | Batch 400/2990 | Loss: 370.6700\n",
      "Epoch 7/10 | Batch 500/2990 | Loss: 184.5398\n",
      "Epoch 7/10 | Batch 600/2990 | Loss: 162.0640\n",
      "Epoch 7/10 | Batch 700/2990 | Loss: 176.0243\n",
      "Epoch 7/10 | Batch 800/2990 | Loss: 147.7686\n",
      "Epoch 7/10 | Batch 900/2990 | Loss: 467.4687\n",
      "Epoch 7/10 | Batch 1000/2990 | Loss: 228.8695\n",
      "Epoch 7/10 | Batch 1100/2990 | Loss: 200.0483\n",
      "Epoch 7/10 | Batch 1200/2990 | Loss: 131.8173\n",
      "Epoch 7/10 | Batch 1300/2990 | Loss: 226.4389\n",
      "Epoch 7/10 | Batch 1400/2990 | Loss: 373.9070\n",
      "Epoch 7/10 | Batch 1500/2990 | Loss: 257.9155\n",
      "Epoch 7/10 | Batch 1600/2990 | Loss: 761.9493\n",
      "Epoch 7/10 | Batch 1700/2990 | Loss: 232.8112\n",
      "Epoch 7/10 | Batch 1800/2990 | Loss: 131.1189\n",
      "Epoch 7/10 | Batch 1900/2990 | Loss: 306.1892\n",
      "Epoch 7/10 | Batch 2000/2990 | Loss: 403.0005\n",
      "Epoch 7/10 | Batch 2100/2990 | Loss: 228.5226\n",
      "Epoch 7/10 | Batch 2200/2990 | Loss: 219.3107\n",
      "Epoch 7/10 | Batch 2300/2990 | Loss: 296.8199\n",
      "Epoch 7/10 | Batch 2400/2990 | Loss: 100.6435\n",
      "Epoch 7/10 | Batch 2500/2990 | Loss: 85.7267\n",
      "Epoch 7/10 | Batch 2600/2990 | Loss: 158.8555\n",
      "Epoch 7/10 | Batch 2700/2990 | Loss: 237.8325\n",
      "Epoch 7/10 | Batch 2800/2990 | Loss: 337.1147\n",
      "Epoch 7/10 | Batch 2900/2990 | Loss: 149.6482\n",
      "Epoch 7/10, Train Loss: 254.9278, Val Loss: 771.2217\n",
      "Epoch 8/10 | Batch 0/2990 | Loss: 219.7807\n",
      "Epoch 8/10 | Batch 100/2990 | Loss: 253.1400\n",
      "Epoch 8/10 | Batch 200/2990 | Loss: 251.8898\n",
      "Epoch 8/10 | Batch 300/2990 | Loss: 113.7840\n",
      "Epoch 8/10 | Batch 400/2990 | Loss: 139.4237\n",
      "Epoch 8/10 | Batch 500/2990 | Loss: 147.6483\n",
      "Epoch 8/10 | Batch 600/2990 | Loss: 239.5849\n",
      "Epoch 8/10 | Batch 700/2990 | Loss: 215.5266\n",
      "Epoch 8/10 | Batch 800/2990 | Loss: 201.7649\n",
      "Epoch 8/10 | Batch 900/2990 | Loss: 158.7803\n",
      "Epoch 8/10 | Batch 1000/2990 | Loss: 286.6686\n",
      "Epoch 8/10 | Batch 1100/2990 | Loss: 201.7987\n",
      "Epoch 8/10 | Batch 1200/2990 | Loss: 269.1907\n",
      "Epoch 8/10 | Batch 1300/2990 | Loss: 100.7579\n",
      "Epoch 8/10 | Batch 1400/2990 | Loss: 155.1617\n",
      "Epoch 8/10 | Batch 1500/2990 | Loss: 166.7720\n",
      "Epoch 8/10 | Batch 1600/2990 | Loss: 131.0817\n",
      "Epoch 8/10 | Batch 1700/2990 | Loss: 233.0793\n",
      "Epoch 8/10 | Batch 1800/2990 | Loss: 168.3642\n",
      "Epoch 8/10 | Batch 1900/2990 | Loss: 108.0216\n",
      "Epoch 8/10 | Batch 2000/2990 | Loss: 167.4133\n",
      "Epoch 8/10 | Batch 2100/2990 | Loss: 325.2119\n",
      "Epoch 8/10 | Batch 2200/2990 | Loss: 216.8132\n",
      "Epoch 8/10 | Batch 2300/2990 | Loss: 168.4142\n",
      "Epoch 8/10 | Batch 2400/2990 | Loss: 409.6468\n",
      "Epoch 8/10 | Batch 2500/2990 | Loss: 329.2867\n",
      "Epoch 8/10 | Batch 2600/2990 | Loss: 404.3298\n",
      "Epoch 8/10 | Batch 2700/2990 | Loss: 253.8688\n",
      "Epoch 8/10 | Batch 2800/2990 | Loss: 148.1226\n",
      "Epoch 8/10 | Batch 2900/2990 | Loss: 346.2734\n",
      "Epoch 8/10, Train Loss: 216.5667, Val Loss: 729.1245\n",
      "✅ Model saved with Val Loss: 729.1245\n",
      "Epoch 9/10 | Batch 0/2990 | Loss: 278.0291\n",
      "Epoch 9/10 | Batch 100/2990 | Loss: 98.7571\n",
      "Epoch 9/10 | Batch 200/2990 | Loss: 316.8943\n",
      "Epoch 9/10 | Batch 300/2990 | Loss: 113.3227\n",
      "Epoch 9/10 | Batch 400/2990 | Loss: 347.9255\n",
      "Epoch 9/10 | Batch 500/2990 | Loss: 159.7641\n",
      "Epoch 9/10 | Batch 600/2990 | Loss: 130.5043\n",
      "Epoch 9/10 | Batch 700/2990 | Loss: 67.5803\n",
      "Epoch 9/10 | Batch 800/2990 | Loss: 265.6542\n",
      "Epoch 9/10 | Batch 900/2990 | Loss: 117.7928\n",
      "Epoch 9/10 | Batch 1000/2990 | Loss: 161.9475\n",
      "Epoch 9/10 | Batch 1100/2990 | Loss: 158.1772\n",
      "Epoch 9/10 | Batch 1200/2990 | Loss: 186.2094\n",
      "Epoch 9/10 | Batch 1300/2990 | Loss: 145.2817\n",
      "Epoch 9/10 | Batch 1400/2990 | Loss: 72.8263\n",
      "Epoch 9/10 | Batch 1500/2990 | Loss: 58.5419\n",
      "Epoch 9/10 | Batch 1600/2990 | Loss: 180.0612\n",
      "Epoch 9/10 | Batch 1700/2990 | Loss: 240.3497\n",
      "Epoch 9/10 | Batch 1800/2990 | Loss: 128.4792\n",
      "Epoch 9/10 | Batch 1900/2990 | Loss: 97.5502\n",
      "Epoch 9/10 | Batch 2000/2990 | Loss: 324.8415\n",
      "Epoch 9/10 | Batch 2100/2990 | Loss: 148.4589\n",
      "Epoch 9/10 | Batch 2200/2990 | Loss: 122.6822\n",
      "Epoch 9/10 | Batch 2300/2990 | Loss: 101.6167\n",
      "Epoch 9/10 | Batch 2400/2990 | Loss: 401.0779\n",
      "Epoch 9/10 | Batch 2500/2990 | Loss: 235.1980\n",
      "Epoch 9/10 | Batch 2600/2990 | Loss: 169.0553\n",
      "Epoch 9/10 | Batch 2700/2990 | Loss: 291.3139\n",
      "Epoch 9/10 | Batch 2800/2990 | Loss: 171.6002\n",
      "Epoch 9/10 | Batch 2900/2990 | Loss: 287.9632\n",
      "Epoch 9/10, Train Loss: 188.9132, Val Loss: 694.2190\n",
      "✅ Model saved with Val Loss: 694.2190\n",
      "Epoch 10/10 | Batch 0/2990 | Loss: 111.0410\n",
      "Epoch 10/10 | Batch 100/2990 | Loss: 245.6718\n",
      "Epoch 10/10 | Batch 200/2990 | Loss: 111.4051\n",
      "Epoch 10/10 | Batch 300/2990 | Loss: 129.7409\n",
      "Epoch 10/10 | Batch 400/2990 | Loss: 223.8860\n",
      "Epoch 10/10 | Batch 500/2990 | Loss: 138.3648\n",
      "Epoch 10/10 | Batch 600/2990 | Loss: 186.9392\n",
      "Epoch 10/10 | Batch 700/2990 | Loss: 223.1234\n",
      "Epoch 10/10 | Batch 800/2990 | Loss: 267.5268\n",
      "Epoch 10/10 | Batch 900/2990 | Loss: 398.2620\n",
      "Epoch 10/10 | Batch 1000/2990 | Loss: 132.0276\n",
      "Epoch 10/10 | Batch 1100/2990 | Loss: 185.8024\n",
      "Epoch 10/10 | Batch 1200/2990 | Loss: 193.4748\n",
      "Epoch 10/10 | Batch 1300/2990 | Loss: 117.2915\n",
      "Epoch 10/10 | Batch 1400/2990 | Loss: 95.1669\n",
      "Epoch 10/10 | Batch 1500/2990 | Loss: 212.2415\n",
      "Epoch 10/10 | Batch 1600/2990 | Loss: 178.6072\n",
      "Epoch 10/10 | Batch 1700/2990 | Loss: 289.6610\n",
      "Epoch 10/10 | Batch 1800/2990 | Loss: 171.4948\n",
      "Epoch 10/10 | Batch 1900/2990 | Loss: 129.5418\n",
      "Epoch 10/10 | Batch 2000/2990 | Loss: 346.2373\n",
      "Epoch 10/10 | Batch 2100/2990 | Loss: 160.9752\n",
      "Epoch 10/10 | Batch 2200/2990 | Loss: 125.3653\n",
      "Epoch 10/10 | Batch 2300/2990 | Loss: 68.6426\n",
      "Epoch 10/10 | Batch 2400/2990 | Loss: 83.3698\n",
      "Epoch 10/10 | Batch 2500/2990 | Loss: 148.9943\n",
      "Epoch 10/10 | Batch 2600/2990 | Loss: 226.5615\n",
      "Epoch 10/10 | Batch 2700/2990 | Loss: 66.9933\n",
      "Epoch 10/10 | Batch 2800/2990 | Loss: 202.4667\n",
      "Epoch 10/10 | Batch 2900/2990 | Loss: 309.8188\n",
      "Epoch 10/10, Train Loss: 166.1345, Val Loss: 668.3145\n",
      "✅ Model saved with Val Loss: 668.3145\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, save_path=\"cnn_best_wheat_ear_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 373.59, Actual: 368.00\n",
      "Predicted: 535.93, Actual: 552.00\n",
      "Predicted: 363.43, Actual: 454.00\n",
      "Predicted: 307.25, Actual: 281.00\n",
      "Predicted: 468.25, Actual: 472.00\n",
      "Predicted: 438.65, Actual: 425.00\n",
      "Predicted: 292.20, Actual: 295.00\n",
      "Predicted: 453.73, Actual: 430.00\n",
      "Predicted: 417.03, Actual: 392.00\n",
      "Predicted: 126.40, Actual: 129.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "preds, actuals = test_model(model, test_loader)\n",
    "\n",
    "# Print some predictions vs actual values\n",
    "for p, a in zip(preds[:10], actuals[:10]):\n",
    "    print(f\"Predicted: {p:.2f}, Actual: {a:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
