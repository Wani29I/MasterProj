{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from EfficientNetV2.Model import EfficientNetV2SWheatCountWithConfidence\n",
    "from dataLoaderFunc import loadSplitData, createLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Custom Gaussian NLL Loss\n",
    "def gaussian_nll_loss(pred_mean, pred_logvar, target):\n",
    "    precision = torch.exp(-pred_logvar)\n",
    "    return torch.mean(precision * (target - pred_mean)**2 + pred_logvar)\n",
    "\n",
    "# ✅ Training Function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_batch, dsm_batch)  # output: [B, 2]\n",
    "            pred_mean = output[:, 0]\n",
    "            pred_logvar = output[:, 1]\n",
    "            loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                output = model(rgb_batch, dsm_batch)\n",
    "                pred_mean = output[:, 0]\n",
    "                pred_logvar = output[:, 1]\n",
    "                loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save model\n",
    "        torch.save(model.state_dict(), f\"EffNetV2S_EarCount_Conf_E{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ✅ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ✅ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ✅ Default to CPU if no GPU is available\n",
    "print(f\"✅ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 0/2990 | Loss: 132156.5625\n",
      "Epoch 1/10 | Batch 20/2990 | Loss: 18.4873\n",
      "Epoch 1/10 | Batch 40/2990 | Loss: 19.9267\n",
      "Epoch 1/10 | Batch 60/2990 | Loss: 17.3949\n",
      "Epoch 1/10 | Batch 80/2990 | Loss: 15.0106\n",
      "Epoch 1/10 | Batch 100/2990 | Loss: 15.9969\n",
      "Epoch 1/10 | Batch 120/2990 | Loss: 15.4573\n",
      "Epoch 1/10 | Batch 140/2990 | Loss: 15.4008\n",
      "Epoch 1/10 | Batch 160/2990 | Loss: 15.2395\n",
      "Epoch 1/10 | Batch 180/2990 | Loss: 14.2859\n",
      "Epoch 1/10 | Batch 200/2990 | Loss: 13.9945\n",
      "Epoch 1/10 | Batch 220/2990 | Loss: 13.8882\n",
      "Epoch 1/10 | Batch 240/2990 | Loss: 14.1885\n",
      "Epoch 1/10 | Batch 260/2990 | Loss: 13.6968\n",
      "Epoch 1/10 | Batch 280/2990 | Loss: 13.7722\n",
      "Epoch 1/10 | Batch 300/2990 | Loss: 13.7549\n",
      "Epoch 1/10 | Batch 320/2990 | Loss: 14.4435\n",
      "Epoch 1/10 | Batch 340/2990 | Loss: 13.9169\n",
      "Epoch 1/10 | Batch 360/2990 | Loss: 13.6319\n",
      "Epoch 1/10 | Batch 380/2990 | Loss: 14.0031\n",
      "Epoch 1/10 | Batch 400/2990 | Loss: 13.5268\n",
      "Epoch 1/10 | Batch 420/2990 | Loss: 14.6741\n",
      "Epoch 1/10 | Batch 440/2990 | Loss: 15.5363\n",
      "Epoch 1/10 | Batch 460/2990 | Loss: 13.5993\n",
      "Epoch 1/10 | Batch 480/2990 | Loss: 13.5425\n",
      "Epoch 1/10 | Batch 500/2990 | Loss: 13.6308\n",
      "Epoch 1/10 | Batch 520/2990 | Loss: 14.2184\n",
      "Epoch 1/10 | Batch 540/2990 | Loss: 13.5400\n",
      "Epoch 1/10 | Batch 560/2990 | Loss: 17.2311\n",
      "Epoch 1/10 | Batch 580/2990 | Loss: 13.7900\n",
      "Epoch 1/10 | Batch 600/2990 | Loss: 13.5755\n",
      "Epoch 1/10 | Batch 620/2990 | Loss: 13.8422\n",
      "Epoch 1/10 | Batch 640/2990 | Loss: 13.6528\n",
      "Epoch 1/10 | Batch 660/2990 | Loss: 13.4625\n",
      "Epoch 1/10 | Batch 680/2990 | Loss: 13.2665\n",
      "Epoch 1/10 | Batch 700/2990 | Loss: 13.2975\n",
      "Epoch 1/10 | Batch 720/2990 | Loss: 13.6474\n",
      "Epoch 1/10 | Batch 740/2990 | Loss: 13.6056\n",
      "Epoch 1/10 | Batch 760/2990 | Loss: 14.2320\n",
      "Epoch 1/10 | Batch 780/2990 | Loss: 14.1885\n",
      "Epoch 1/10 | Batch 800/2990 | Loss: 13.2238\n",
      "Epoch 1/10 | Batch 820/2990 | Loss: 14.1773\n",
      "Epoch 1/10 | Batch 840/2990 | Loss: 13.7037\n",
      "Epoch 1/10 | Batch 860/2990 | Loss: 13.7325\n",
      "Epoch 1/10 | Batch 880/2990 | Loss: 13.4091\n",
      "Epoch 1/10 | Batch 900/2990 | Loss: 13.2840\n",
      "Epoch 1/10 | Batch 920/2990 | Loss: 13.5559\n",
      "Epoch 1/10 | Batch 940/2990 | Loss: 13.1837\n",
      "Epoch 1/10 | Batch 960/2990 | Loss: 13.8137\n",
      "Epoch 1/10 | Batch 980/2990 | Loss: 13.0492\n",
      "Epoch 1/10 | Batch 1000/2990 | Loss: 14.3709\n",
      "Epoch 1/10 | Batch 1020/2990 | Loss: 13.3076\n",
      "Epoch 1/10 | Batch 1040/2990 | Loss: 13.1528\n",
      "Epoch 1/10 | Batch 1060/2990 | Loss: 13.9823\n",
      "Epoch 1/10 | Batch 1080/2990 | Loss: 13.9198\n",
      "Epoch 1/10 | Batch 1100/2990 | Loss: 13.2496\n",
      "Epoch 1/10 | Batch 1120/2990 | Loss: 13.2607\n",
      "Epoch 1/10 | Batch 1140/2990 | Loss: 13.4046\n",
      "Epoch 1/10 | Batch 1160/2990 | Loss: 14.9269\n",
      "Epoch 1/10 | Batch 1180/2990 | Loss: 16.2456\n",
      "Epoch 1/10 | Batch 1200/2990 | Loss: 16.3017\n",
      "Epoch 1/10 | Batch 1220/2990 | Loss: 13.1117\n",
      "Epoch 1/10 | Batch 1240/2990 | Loss: 13.9926\n",
      "Epoch 1/10 | Batch 1260/2990 | Loss: 13.3101\n",
      "Epoch 1/10 | Batch 1280/2990 | Loss: 14.0426\n",
      "Epoch 1/10 | Batch 1300/2990 | Loss: 13.4914\n",
      "Epoch 1/10 | Batch 1320/2990 | Loss: 13.2114\n",
      "Epoch 1/10 | Batch 1340/2990 | Loss: 13.5682\n",
      "Epoch 1/10 | Batch 1360/2990 | Loss: 13.4316\n",
      "Epoch 1/10 | Batch 1380/2990 | Loss: 13.1125\n",
      "Epoch 1/10 | Batch 1400/2990 | Loss: 13.5493\n",
      "Epoch 1/10 | Batch 1420/2990 | Loss: 13.4282\n",
      "Epoch 1/10 | Batch 1440/2990 | Loss: 13.8469\n",
      "Epoch 1/10 | Batch 1460/2990 | Loss: 13.7282\n",
      "Epoch 1/10 | Batch 1480/2990 | Loss: 13.2604\n",
      "Epoch 1/10 | Batch 1500/2990 | Loss: 13.8875\n",
      "Epoch 1/10 | Batch 1520/2990 | Loss: 13.8640\n",
      "Epoch 1/10 | Batch 1540/2990 | Loss: 13.2233\n",
      "Epoch 1/10 | Batch 1560/2990 | Loss: 14.2024\n",
      "Epoch 1/10 | Batch 1580/2990 | Loss: 14.8310\n",
      "Epoch 1/10 | Batch 1600/2990 | Loss: 13.1473\n",
      "Epoch 1/10 | Batch 1620/2990 | Loss: 13.1495\n",
      "Epoch 1/10 | Batch 1640/2990 | Loss: 13.3772\n",
      "Epoch 1/10 | Batch 1660/2990 | Loss: 13.4542\n",
      "Epoch 1/10 | Batch 1680/2990 | Loss: 13.1496\n",
      "Epoch 1/10 | Batch 1700/2990 | Loss: 13.3694\n",
      "Epoch 1/10 | Batch 1720/2990 | Loss: 13.1821\n",
      "Epoch 1/10 | Batch 1740/2990 | Loss: 13.3735\n",
      "Epoch 1/10 | Batch 1760/2990 | Loss: 12.9805\n",
      "Epoch 1/10 | Batch 1780/2990 | Loss: 13.5023\n",
      "Epoch 1/10 | Batch 1800/2990 | Loss: 14.2545\n",
      "Epoch 1/10 | Batch 1820/2990 | Loss: 12.8189\n",
      "Epoch 1/10 | Batch 1840/2990 | Loss: 13.0963\n",
      "Epoch 1/10 | Batch 1860/2990 | Loss: 13.6942\n",
      "Epoch 1/10 | Batch 1880/2990 | Loss: 13.6209\n",
      "Epoch 1/10 | Batch 1900/2990 | Loss: 12.9589\n",
      "Epoch 1/10 | Batch 1920/2990 | Loss: 13.3055\n",
      "Epoch 1/10 | Batch 1940/2990 | Loss: 14.3916\n",
      "Epoch 1/10 | Batch 1960/2990 | Loss: 13.4009\n",
      "Epoch 1/10 | Batch 1980/2990 | Loss: 13.2982\n",
      "Epoch 1/10 | Batch 2000/2990 | Loss: 13.2341\n",
      "Epoch 1/10 | Batch 2020/2990 | Loss: 12.9840\n",
      "Epoch 1/10 | Batch 2040/2990 | Loss: 13.5892\n",
      "Epoch 1/10 | Batch 2060/2990 | Loss: 13.5366\n",
      "Epoch 1/10 | Batch 2080/2990 | Loss: 13.9196\n",
      "Epoch 1/10 | Batch 2100/2990 | Loss: 13.8559\n",
      "Epoch 1/10 | Batch 2120/2990 | Loss: 13.4452\n",
      "Epoch 1/10 | Batch 2140/2990 | Loss: 13.2262\n",
      "Epoch 1/10 | Batch 2160/2990 | Loss: 13.6881\n",
      "Epoch 1/10 | Batch 2180/2990 | Loss: 13.3537\n",
      "Epoch 1/10 | Batch 2200/2990 | Loss: 13.4495\n",
      "Epoch 1/10 | Batch 2220/2990 | Loss: 13.2797\n",
      "Epoch 1/10 | Batch 2240/2990 | Loss: 13.1944\n",
      "Epoch 1/10 | Batch 2260/2990 | Loss: 13.8656\n",
      "Epoch 1/10 | Batch 2280/2990 | Loss: 13.1860\n",
      "Epoch 1/10 | Batch 2300/2990 | Loss: 13.5061\n",
      "Epoch 1/10 | Batch 2320/2990 | Loss: 13.2671\n",
      "Epoch 1/10 | Batch 2340/2990 | Loss: 13.2053\n",
      "Epoch 1/10 | Batch 2360/2990 | Loss: 13.3011\n",
      "Epoch 1/10 | Batch 2380/2990 | Loss: 13.4964\n",
      "Epoch 1/10 | Batch 2400/2990 | Loss: 13.4020\n",
      "Epoch 1/10 | Batch 2420/2990 | Loss: 13.5620\n",
      "Epoch 1/10 | Batch 2440/2990 | Loss: 13.0705\n",
      "Epoch 1/10 | Batch 2460/2990 | Loss: 13.3072\n",
      "Epoch 1/10 | Batch 2480/2990 | Loss: 13.4072\n",
      "Epoch 1/10 | Batch 2500/2990 | Loss: 13.9169\n",
      "Epoch 1/10 | Batch 2520/2990 | Loss: 14.1614\n",
      "Epoch 1/10 | Batch 2540/2990 | Loss: 13.0851\n",
      "Epoch 1/10 | Batch 2560/2990 | Loss: 13.7115\n",
      "Epoch 1/10 | Batch 2580/2990 | Loss: 12.9022\n",
      "Epoch 1/10 | Batch 2600/2990 | Loss: 13.2758\n",
      "Epoch 1/10 | Batch 2620/2990 | Loss: 14.0135\n",
      "Epoch 1/10 | Batch 2640/2990 | Loss: 13.4082\n",
      "Epoch 1/10 | Batch 2660/2990 | Loss: 13.8198\n",
      "Epoch 1/10 | Batch 2680/2990 | Loss: 13.1093\n",
      "Epoch 1/10 | Batch 2700/2990 | Loss: 12.8194\n",
      "Epoch 1/10 | Batch 2720/2990 | Loss: 13.3529\n",
      "Epoch 1/10 | Batch 2740/2990 | Loss: 13.2807\n",
      "Epoch 1/10 | Batch 2760/2990 | Loss: 13.0516\n",
      "Epoch 1/10 | Batch 2780/2990 | Loss: 13.9618\n",
      "Epoch 1/10 | Batch 2800/2990 | Loss: 13.4864\n",
      "Epoch 1/10 | Batch 2820/2990 | Loss: 13.9691\n",
      "Epoch 1/10 | Batch 2840/2990 | Loss: 13.2946\n",
      "Epoch 1/10 | Batch 2860/2990 | Loss: 12.6736\n",
      "Epoch 1/10 | Batch 2880/2990 | Loss: 12.5073\n",
      "Epoch 1/10 | Batch 2900/2990 | Loss: 13.1153\n",
      "Epoch 1/10 | Batch 2920/2990 | Loss: 12.7288\n",
      "Epoch 1/10 | Batch 2940/2990 | Loss: 13.0092\n",
      "Epoch 1/10 | Batch 2960/2990 | Loss: 12.9143\n",
      "Epoch 1/10 | Batch 2980/2990 | Loss: 13.1060\n",
      "✅ Epoch 1/10 | Train Loss: 90.2853 | Val Loss: 12.4431\n",
      "Epoch 2/10 | Batch 0/2990 | Loss: 13.3392\n",
      "Epoch 2/10 | Batch 20/2990 | Loss: 12.8340\n",
      "Epoch 2/10 | Batch 40/2990 | Loss: 14.1376\n",
      "Epoch 2/10 | Batch 60/2990 | Loss: 12.8277\n",
      "Epoch 2/10 | Batch 80/2990 | Loss: 12.9444\n",
      "Epoch 2/10 | Batch 100/2990 | Loss: 12.4572\n",
      "Epoch 2/10 | Batch 120/2990 | Loss: 12.8714\n",
      "Epoch 2/10 | Batch 140/2990 | Loss: 13.0086\n",
      "Epoch 2/10 | Batch 160/2990 | Loss: 13.2299\n",
      "Epoch 2/10 | Batch 180/2990 | Loss: 13.3628\n",
      "Epoch 2/10 | Batch 200/2990 | Loss: 12.9950\n",
      "Epoch 2/10 | Batch 220/2990 | Loss: 12.8518\n",
      "Epoch 2/10 | Batch 240/2990 | Loss: 13.3188\n",
      "Epoch 2/10 | Batch 260/2990 | Loss: 13.3181\n",
      "Epoch 2/10 | Batch 280/2990 | Loss: 13.1570\n",
      "Epoch 2/10 | Batch 300/2990 | Loss: 13.0469\n",
      "Epoch 2/10 | Batch 320/2990 | Loss: 12.9013\n",
      "Epoch 2/10 | Batch 340/2990 | Loss: 12.7787\n",
      "Epoch 2/10 | Batch 360/2990 | Loss: 13.4106\n",
      "Epoch 2/10 | Batch 380/2990 | Loss: 12.4711\n",
      "Epoch 2/10 | Batch 400/2990 | Loss: 12.3309\n",
      "Epoch 2/10 | Batch 420/2990 | Loss: 13.7165\n",
      "Epoch 2/10 | Batch 440/2990 | Loss: 13.3394\n",
      "Epoch 2/10 | Batch 460/2990 | Loss: 13.2942\n",
      "Epoch 2/10 | Batch 480/2990 | Loss: 12.7706\n",
      "Epoch 2/10 | Batch 500/2990 | Loss: 12.6572\n",
      "Epoch 2/10 | Batch 520/2990 | Loss: 13.6515\n",
      "Epoch 2/10 | Batch 540/2990 | Loss: 12.3658\n",
      "Epoch 2/10 | Batch 560/2990 | Loss: 12.6668\n",
      "Epoch 2/10 | Batch 580/2990 | Loss: 13.6975\n",
      "Epoch 2/10 | Batch 600/2990 | Loss: 12.8198\n",
      "Epoch 2/10 | Batch 620/2990 | Loss: 12.5843\n",
      "Epoch 2/10 | Batch 640/2990 | Loss: 12.1515\n",
      "Epoch 2/10 | Batch 660/2990 | Loss: 12.4429\n",
      "Epoch 2/10 | Batch 680/2990 | Loss: 12.4467\n",
      "Epoch 2/10 | Batch 700/2990 | Loss: 12.3088\n",
      "Epoch 2/10 | Batch 720/2990 | Loss: 12.3225\n",
      "Epoch 2/10 | Batch 740/2990 | Loss: 11.9331\n",
      "Epoch 2/10 | Batch 760/2990 | Loss: 12.7127\n",
      "Epoch 2/10 | Batch 780/2990 | Loss: 12.0314\n",
      "Epoch 2/10 | Batch 800/2990 | Loss: 12.1958\n",
      "Epoch 2/10 | Batch 820/2990 | Loss: 13.6973\n",
      "Epoch 2/10 | Batch 840/2990 | Loss: 12.4255\n",
      "Epoch 2/10 | Batch 860/2990 | Loss: 12.7304\n",
      "Epoch 2/10 | Batch 880/2990 | Loss: 12.1065\n",
      "Epoch 2/10 | Batch 900/2990 | Loss: 15.5881\n",
      "Epoch 2/10 | Batch 920/2990 | Loss: 11.9236\n",
      "Epoch 2/10 | Batch 940/2990 | Loss: 11.2025\n",
      "Epoch 2/10 | Batch 960/2990 | Loss: 11.6526\n",
      "Epoch 2/10 | Batch 980/2990 | Loss: 11.6859\n",
      "Epoch 2/10 | Batch 1000/2990 | Loss: 12.6201\n",
      "Epoch 2/10 | Batch 1020/2990 | Loss: 12.4236\n",
      "Epoch 2/10 | Batch 1040/2990 | Loss: 12.5718\n",
      "Epoch 2/10 | Batch 1060/2990 | Loss: 10.7188\n",
      "Epoch 2/10 | Batch 1080/2990 | Loss: 11.4541\n",
      "Epoch 2/10 | Batch 1100/2990 | Loss: 10.9404\n",
      "Epoch 2/10 | Batch 1120/2990 | Loss: 10.5294\n",
      "Epoch 2/10 | Batch 1140/2990 | Loss: 10.2291\n",
      "Epoch 2/10 | Batch 1160/2990 | Loss: 11.2445\n",
      "Epoch 2/10 | Batch 1180/2990 | Loss: 11.6950\n",
      "Epoch 2/10 | Batch 1200/2990 | Loss: 11.6111\n",
      "Epoch 2/10 | Batch 1220/2990 | Loss: 11.3334\n",
      "Epoch 2/10 | Batch 1240/2990 | Loss: 11.0402\n",
      "Epoch 2/10 | Batch 1260/2990 | Loss: 11.6211\n",
      "Epoch 2/10 | Batch 1280/2990 | Loss: 10.8164\n",
      "Epoch 2/10 | Batch 1300/2990 | Loss: 10.6592\n",
      "Epoch 2/10 | Batch 1320/2990 | Loss: 11.0211\n",
      "Epoch 2/10 | Batch 1340/2990 | Loss: 11.6533\n",
      "Epoch 2/10 | Batch 1360/2990 | Loss: 10.8394\n",
      "Epoch 2/10 | Batch 1380/2990 | Loss: 11.6902\n",
      "Epoch 2/10 | Batch 1400/2990 | Loss: 10.9139\n",
      "Epoch 2/10 | Batch 1420/2990 | Loss: 11.4193\n",
      "Epoch 2/10 | Batch 1440/2990 | Loss: 10.6977\n",
      "Epoch 2/10 | Batch 1460/2990 | Loss: 11.3448\n",
      "Epoch 2/10 | Batch 1480/2990 | Loss: 11.3546\n",
      "Epoch 2/10 | Batch 1500/2990 | Loss: 10.9436\n",
      "Epoch 2/10 | Batch 1520/2990 | Loss: 10.9792\n",
      "Epoch 2/10 | Batch 1540/2990 | Loss: 10.2346\n",
      "Epoch 2/10 | Batch 1560/2990 | Loss: 11.1868\n",
      "Epoch 2/10 | Batch 1580/2990 | Loss: 11.0346\n",
      "Epoch 2/10 | Batch 1600/2990 | Loss: 10.7857\n",
      "Epoch 2/10 | Batch 1620/2990 | Loss: 10.4180\n",
      "Epoch 2/10 | Batch 1640/2990 | Loss: 11.3507\n",
      "Epoch 2/10 | Batch 1660/2990 | Loss: 11.1674\n",
      "Epoch 2/10 | Batch 1680/2990 | Loss: 11.5921\n",
      "Epoch 2/10 | Batch 1700/2990 | Loss: 10.5474\n",
      "Epoch 2/10 | Batch 1720/2990 | Loss: 11.1506\n",
      "Epoch 2/10 | Batch 1740/2990 | Loss: 11.9936\n",
      "Epoch 2/10 | Batch 1760/2990 | Loss: 11.4107\n",
      "Epoch 2/10 | Batch 1780/2990 | Loss: 11.0522\n",
      "Epoch 2/10 | Batch 1800/2990 | Loss: 10.9930\n",
      "Epoch 2/10 | Batch 1820/2990 | Loss: 11.2554\n",
      "Epoch 2/10 | Batch 1840/2990 | Loss: 11.9248\n",
      "Epoch 2/10 | Batch 1860/2990 | Loss: 10.2644\n",
      "Epoch 2/10 | Batch 1880/2990 | Loss: 11.0417\n",
      "Epoch 2/10 | Batch 1900/2990 | Loss: 10.4809\n",
      "Epoch 2/10 | Batch 1920/2990 | Loss: 11.0735\n",
      "Epoch 2/10 | Batch 1940/2990 | Loss: 10.0561\n",
      "Epoch 2/10 | Batch 1960/2990 | Loss: 10.5072\n",
      "Epoch 2/10 | Batch 1980/2990 | Loss: 11.7408\n",
      "Epoch 2/10 | Batch 2000/2990 | Loss: 10.7811\n",
      "Epoch 2/10 | Batch 2020/2990 | Loss: 11.0327\n",
      "Epoch 2/10 | Batch 2040/2990 | Loss: 11.2109\n",
      "Epoch 2/10 | Batch 2060/2990 | Loss: 10.4627\n",
      "Epoch 2/10 | Batch 2080/2990 | Loss: 11.1078\n",
      "Epoch 2/10 | Batch 2100/2990 | Loss: 10.2906\n",
      "Epoch 2/10 | Batch 2120/2990 | Loss: 10.4660\n",
      "Epoch 2/10 | Batch 2140/2990 | Loss: 10.2763\n",
      "Epoch 2/10 | Batch 2160/2990 | Loss: 10.5775\n",
      "Epoch 2/10 | Batch 2180/2990 | Loss: 10.4968\n",
      "Epoch 2/10 | Batch 2200/2990 | Loss: 11.0590\n",
      "Epoch 2/10 | Batch 2220/2990 | Loss: 9.8756\n",
      "Epoch 2/10 | Batch 2240/2990 | Loss: 10.8339\n",
      "Epoch 2/10 | Batch 2260/2990 | Loss: 12.1792\n",
      "Epoch 2/10 | Batch 2280/2990 | Loss: 10.2659\n",
      "Epoch 2/10 | Batch 2300/2990 | Loss: 10.6764\n",
      "Epoch 2/10 | Batch 2320/2990 | Loss: 10.4333\n",
      "Epoch 2/10 | Batch 2340/2990 | Loss: 11.8949\n",
      "Epoch 2/10 | Batch 2360/2990 | Loss: 10.5068\n",
      "Epoch 2/10 | Batch 2380/2990 | Loss: 10.2695\n",
      "Epoch 2/10 | Batch 2400/2990 | Loss: 11.0560\n",
      "Epoch 2/10 | Batch 2420/2990 | Loss: 10.4048\n",
      "Epoch 2/10 | Batch 2440/2990 | Loss: 10.6494\n",
      "Epoch 2/10 | Batch 2460/2990 | Loss: 12.2478\n",
      "Epoch 2/10 | Batch 2480/2990 | Loss: 10.9007\n",
      "Epoch 2/10 | Batch 2500/2990 | Loss: 10.8598\n",
      "Epoch 2/10 | Batch 2520/2990 | Loss: 10.2932\n",
      "Epoch 2/10 | Batch 2540/2990 | Loss: 11.2419\n",
      "Epoch 2/10 | Batch 2560/2990 | Loss: 10.2202\n",
      "Epoch 2/10 | Batch 2580/2990 | Loss: 10.6268\n",
      "Epoch 2/10 | Batch 2600/2990 | Loss: 10.4658\n",
      "Epoch 2/10 | Batch 2620/2990 | Loss: 11.4627\n",
      "Epoch 2/10 | Batch 2640/2990 | Loss: 9.8200\n",
      "Epoch 2/10 | Batch 2660/2990 | Loss: 10.2991\n",
      "Epoch 2/10 | Batch 2680/2990 | Loss: 14.4413\n",
      "Epoch 2/10 | Batch 2700/2990 | Loss: 12.4936\n",
      "Epoch 2/10 | Batch 2720/2990 | Loss: 11.5269\n",
      "Epoch 2/10 | Batch 2740/2990 | Loss: 10.4854\n",
      "Epoch 2/10 | Batch 2760/2990 | Loss: 9.9653\n",
      "Epoch 2/10 | Batch 2780/2990 | Loss: 10.6318\n",
      "Epoch 2/10 | Batch 2800/2990 | Loss: 11.3085\n",
      "Epoch 2/10 | Batch 2820/2990 | Loss: 10.8869\n",
      "Epoch 2/10 | Batch 2840/2990 | Loss: 10.9618\n",
      "Epoch 2/10 | Batch 2860/2990 | Loss: 12.3022\n",
      "Epoch 2/10 | Batch 2880/2990 | Loss: 10.8248\n",
      "Epoch 2/10 | Batch 2900/2990 | Loss: 10.3335\n",
      "Epoch 2/10 | Batch 2920/2990 | Loss: 10.7356\n",
      "Epoch 2/10 | Batch 2940/2990 | Loss: 11.4396\n",
      "Epoch 2/10 | Batch 2960/2990 | Loss: 10.7050\n",
      "Epoch 2/10 | Batch 2980/2990 | Loss: 10.2470\n",
      "✅ Epoch 2/10 | Train Loss: 11.6075 | Val Loss: 10.2396\n",
      "Epoch 3/10 | Batch 0/2990 | Loss: 12.2666\n",
      "Epoch 3/10 | Batch 20/2990 | Loss: 10.4672\n",
      "Epoch 3/10 | Batch 40/2990 | Loss: 11.2479\n",
      "Epoch 3/10 | Batch 60/2990 | Loss: 10.6188\n",
      "Epoch 3/10 | Batch 80/2990 | Loss: 13.2650\n",
      "Epoch 3/10 | Batch 100/2990 | Loss: 9.9398\n",
      "Epoch 3/10 | Batch 120/2990 | Loss: 12.6829\n",
      "Epoch 3/10 | Batch 140/2990 | Loss: 10.3260\n",
      "Epoch 3/10 | Batch 160/2990 | Loss: 10.3978\n",
      "Epoch 3/10 | Batch 180/2990 | Loss: 10.1295\n",
      "Epoch 3/10 | Batch 200/2990 | Loss: 11.3414\n",
      "Epoch 3/10 | Batch 220/2990 | Loss: 10.7885\n",
      "Epoch 3/10 | Batch 240/2990 | Loss: 11.9011\n",
      "Epoch 3/10 | Batch 260/2990 | Loss: 11.1013\n",
      "Epoch 3/10 | Batch 280/2990 | Loss: 10.3880\n",
      "Epoch 3/10 | Batch 300/2990 | Loss: 11.0824\n",
      "Epoch 3/10 | Batch 320/2990 | Loss: 10.0363\n",
      "Epoch 3/10 | Batch 340/2990 | Loss: 10.0833\n",
      "Epoch 3/10 | Batch 360/2990 | Loss: 10.3373\n",
      "Epoch 3/10 | Batch 380/2990 | Loss: 10.5253\n",
      "Epoch 3/10 | Batch 400/2990 | Loss: 10.3681\n",
      "Epoch 3/10 | Batch 420/2990 | Loss: 10.2309\n",
      "Epoch 3/10 | Batch 440/2990 | Loss: 11.2214\n",
      "Epoch 3/10 | Batch 460/2990 | Loss: 10.7958\n",
      "Epoch 3/10 | Batch 480/2990 | Loss: 11.7740\n",
      "Epoch 3/10 | Batch 500/2990 | Loss: 10.6803\n",
      "Epoch 3/10 | Batch 520/2990 | Loss: 11.7277\n",
      "Epoch 3/10 | Batch 540/2990 | Loss: 10.4227\n",
      "Epoch 3/10 | Batch 560/2990 | Loss: 10.2269\n",
      "Epoch 3/10 | Batch 580/2990 | Loss: 10.1360\n",
      "Epoch 3/10 | Batch 600/2990 | Loss: 9.8798\n",
      "Epoch 3/10 | Batch 620/2990 | Loss: 11.5429\n",
      "Epoch 3/10 | Batch 640/2990 | Loss: 10.0624\n",
      "Epoch 3/10 | Batch 660/2990 | Loss: 10.8224\n",
      "Epoch 3/10 | Batch 680/2990 | Loss: 11.1572\n",
      "Epoch 3/10 | Batch 700/2990 | Loss: 9.8963\n",
      "Epoch 3/10 | Batch 720/2990 | Loss: 9.6411\n",
      "Epoch 3/10 | Batch 740/2990 | Loss: 9.8804\n",
      "Epoch 3/10 | Batch 760/2990 | Loss: 10.7350\n",
      "Epoch 3/10 | Batch 780/2990 | Loss: 10.6955\n",
      "Epoch 3/10 | Batch 800/2990 | Loss: 10.5285\n",
      "Epoch 3/10 | Batch 820/2990 | Loss: 10.5246\n",
      "Epoch 3/10 | Batch 840/2990 | Loss: 10.9190\n",
      "Epoch 3/10 | Batch 860/2990 | Loss: 10.4937\n",
      "Epoch 3/10 | Batch 880/2990 | Loss: 10.9804\n",
      "Epoch 3/10 | Batch 900/2990 | Loss: 10.6796\n",
      "Epoch 3/10 | Batch 920/2990 | Loss: 10.0199\n",
      "Epoch 3/10 | Batch 940/2990 | Loss: 10.6329\n",
      "Epoch 3/10 | Batch 960/2990 | Loss: 10.4796\n",
      "Epoch 3/10 | Batch 980/2990 | Loss: 10.1871\n",
      "Epoch 3/10 | Batch 1000/2990 | Loss: 10.7009\n",
      "Epoch 3/10 | Batch 1020/2990 | Loss: 9.9502\n",
      "Epoch 3/10 | Batch 1040/2990 | Loss: 10.4945\n",
      "Epoch 3/10 | Batch 1060/2990 | Loss: 10.0084\n",
      "Epoch 3/10 | Batch 1080/2990 | Loss: 10.3308\n",
      "Epoch 3/10 | Batch 1100/2990 | Loss: 10.2329\n",
      "Epoch 3/10 | Batch 1120/2990 | Loss: 10.4279\n",
      "Epoch 3/10 | Batch 1140/2990 | Loss: 10.1194\n",
      "Epoch 3/10 | Batch 1160/2990 | Loss: 10.4723\n",
      "Epoch 3/10 | Batch 1180/2990 | Loss: 10.5962\n",
      "Epoch 3/10 | Batch 1200/2990 | Loss: 10.1215\n",
      "Epoch 3/10 | Batch 1220/2990 | Loss: 10.0823\n",
      "Epoch 3/10 | Batch 1240/2990 | Loss: 10.2409\n",
      "Epoch 3/10 | Batch 1260/2990 | Loss: 10.7457\n",
      "Epoch 3/10 | Batch 1280/2990 | Loss: 12.4090\n",
      "Epoch 3/10 | Batch 1300/2990 | Loss: 10.2988\n",
      "Epoch 3/10 | Batch 1320/2990 | Loss: 10.3829\n",
      "Epoch 3/10 | Batch 1340/2990 | Loss: 11.3375\n",
      "Epoch 3/10 | Batch 1360/2990 | Loss: 10.3949\n",
      "Epoch 3/10 | Batch 1380/2990 | Loss: 9.7328\n",
      "Epoch 3/10 | Batch 1400/2990 | Loss: 10.2179\n",
      "Epoch 3/10 | Batch 1420/2990 | Loss: 10.7101\n",
      "Epoch 3/10 | Batch 1440/2990 | Loss: 9.4679\n",
      "Epoch 3/10 | Batch 1460/2990 | Loss: 10.5980\n",
      "Epoch 3/10 | Batch 1480/2990 | Loss: 11.8204\n",
      "Epoch 3/10 | Batch 1500/2990 | Loss: 10.2321\n",
      "Epoch 3/10 | Batch 1520/2990 | Loss: 10.2986\n",
      "Epoch 3/10 | Batch 1540/2990 | Loss: 10.1037\n",
      "Epoch 3/10 | Batch 1560/2990 | Loss: 9.6065\n",
      "Epoch 3/10 | Batch 1580/2990 | Loss: 10.7764\n",
      "Epoch 3/10 | Batch 1600/2990 | Loss: 10.8309\n",
      "Epoch 3/10 | Batch 1620/2990 | Loss: 10.1409\n",
      "Epoch 3/10 | Batch 1640/2990 | Loss: 10.1997\n",
      "Epoch 3/10 | Batch 1660/2990 | Loss: 9.7728\n",
      "Epoch 3/10 | Batch 1680/2990 | Loss: 11.9623\n",
      "Epoch 3/10 | Batch 1700/2990 | Loss: 10.0904\n",
      "Epoch 3/10 | Batch 1720/2990 | Loss: 10.3952\n",
      "Epoch 3/10 | Batch 1740/2990 | Loss: 10.1825\n",
      "Epoch 3/10 | Batch 1760/2990 | Loss: 9.9423\n",
      "Epoch 3/10 | Batch 1780/2990 | Loss: 12.4863\n",
      "Epoch 3/10 | Batch 1800/2990 | Loss: 10.3707\n",
      "Epoch 3/10 | Batch 1820/2990 | Loss: 9.5933\n",
      "Epoch 3/10 | Batch 1840/2990 | Loss: 10.3190\n",
      "Epoch 3/10 | Batch 1860/2990 | Loss: 10.3806\n",
      "Epoch 3/10 | Batch 1880/2990 | Loss: 10.6187\n",
      "Epoch 3/10 | Batch 1900/2990 | Loss: 9.5564\n",
      "Epoch 3/10 | Batch 1920/2990 | Loss: 10.2958\n",
      "Epoch 3/10 | Batch 1940/2990 | Loss: 10.3627\n",
      "Epoch 3/10 | Batch 1960/2990 | Loss: 9.5206\n",
      "Epoch 3/10 | Batch 1980/2990 | Loss: 9.6853\n",
      "Epoch 3/10 | Batch 2000/2990 | Loss: 9.9218\n",
      "Epoch 3/10 | Batch 2020/2990 | Loss: 10.3856\n",
      "Epoch 3/10 | Batch 2040/2990 | Loss: 10.2416\n",
      "Epoch 3/10 | Batch 2060/2990 | Loss: 9.9827\n",
      "Epoch 3/10 | Batch 2080/2990 | Loss: 10.2480\n",
      "Epoch 3/10 | Batch 2100/2990 | Loss: 10.1233\n",
      "Epoch 3/10 | Batch 2120/2990 | Loss: 9.9509\n",
      "Epoch 3/10 | Batch 2140/2990 | Loss: 10.0604\n",
      "Epoch 3/10 | Batch 2160/2990 | Loss: 10.1348\n",
      "Epoch 3/10 | Batch 2180/2990 | Loss: 11.1372\n",
      "Epoch 3/10 | Batch 2200/2990 | Loss: 12.8087\n",
      "Epoch 3/10 | Batch 2220/2990 | Loss: 10.2499\n",
      "Epoch 3/10 | Batch 2240/2990 | Loss: 10.2413\n",
      "Epoch 3/10 | Batch 2260/2990 | Loss: 10.8435\n",
      "Epoch 3/10 | Batch 2280/2990 | Loss: 10.3152\n",
      "Epoch 3/10 | Batch 2300/2990 | Loss: 10.7445\n",
      "Epoch 3/10 | Batch 2320/2990 | Loss: 10.2627\n",
      "Epoch 3/10 | Batch 2340/2990 | Loss: 9.8844\n",
      "Epoch 3/10 | Batch 2360/2990 | Loss: 12.0258\n",
      "Epoch 3/10 | Batch 2380/2990 | Loss: 10.5232\n",
      "Epoch 3/10 | Batch 2400/2990 | Loss: 10.2591\n",
      "Epoch 3/10 | Batch 2420/2990 | Loss: 10.1398\n",
      "Epoch 3/10 | Batch 2440/2990 | Loss: 10.8878\n",
      "Epoch 3/10 | Batch 2460/2990 | Loss: 10.2992\n",
      "Epoch 3/10 | Batch 2480/2990 | Loss: 10.0096\n",
      "Epoch 3/10 | Batch 2500/2990 | Loss: 9.9429\n",
      "Epoch 3/10 | Batch 2520/2990 | Loss: 10.3007\n",
      "Epoch 3/10 | Batch 2540/2990 | Loss: 9.8490\n",
      "Epoch 3/10 | Batch 2560/2990 | Loss: 10.6647\n",
      "Epoch 3/10 | Batch 2580/2990 | Loss: 10.0866\n",
      "Epoch 3/10 | Batch 2600/2990 | Loss: 10.2171\n",
      "Epoch 3/10 | Batch 2620/2990 | Loss: 10.5938\n",
      "Epoch 3/10 | Batch 2640/2990 | Loss: 9.8202\n",
      "Epoch 3/10 | Batch 2660/2990 | Loss: 9.9977\n",
      "Epoch 3/10 | Batch 2680/2990 | Loss: 9.8695\n",
      "Epoch 3/10 | Batch 2700/2990 | Loss: 10.4676\n",
      "Epoch 3/10 | Batch 2720/2990 | Loss: 10.1330\n",
      "Epoch 3/10 | Batch 2740/2990 | Loss: 10.5264\n",
      "Epoch 3/10 | Batch 2760/2990 | Loss: 13.9142\n",
      "Epoch 3/10 | Batch 2780/2990 | Loss: 9.8100\n",
      "Epoch 3/10 | Batch 2800/2990 | Loss: 11.2053\n",
      "Epoch 3/10 | Batch 2820/2990 | Loss: 10.1329\n",
      "Epoch 3/10 | Batch 2840/2990 | Loss: 10.0525\n",
      "Epoch 3/10 | Batch 2860/2990 | Loss: 10.4853\n",
      "Epoch 3/10 | Batch 2880/2990 | Loss: 10.5287\n",
      "Epoch 3/10 | Batch 2900/2990 | Loss: 10.1737\n",
      "Epoch 3/10 | Batch 2920/2990 | Loss: 10.6985\n",
      "Epoch 3/10 | Batch 2940/2990 | Loss: 10.2921\n",
      "Epoch 3/10 | Batch 2960/2990 | Loss: 9.7611\n",
      "Epoch 3/10 | Batch 2980/2990 | Loss: 10.2242\n",
      "✅ Epoch 3/10 | Train Loss: 10.4587 | Val Loss: 8.9184\n",
      "Epoch 4/10 | Batch 0/2990 | Loss: 10.5365\n",
      "Epoch 4/10 | Batch 20/2990 | Loss: 8.9374\n",
      "Epoch 4/10 | Batch 40/2990 | Loss: 10.4611\n",
      "Epoch 4/10 | Batch 60/2990 | Loss: 10.5076\n",
      "Epoch 4/10 | Batch 80/2990 | Loss: 9.6929\n",
      "Epoch 4/10 | Batch 100/2990 | Loss: 10.2384\n",
      "Epoch 4/10 | Batch 120/2990 | Loss: 10.4816\n",
      "Epoch 4/10 | Batch 140/2990 | Loss: 10.5928\n",
      "Epoch 4/10 | Batch 160/2990 | Loss: 9.9602\n",
      "Epoch 4/10 | Batch 180/2990 | Loss: 10.6899\n",
      "Epoch 4/10 | Batch 200/2990 | Loss: 10.4448\n",
      "Epoch 4/10 | Batch 220/2990 | Loss: 10.0686\n",
      "Epoch 4/10 | Batch 240/2990 | Loss: 10.2622\n",
      "Epoch 4/10 | Batch 260/2990 | Loss: 9.8739\n",
      "Epoch 4/10 | Batch 280/2990 | Loss: 9.5114\n",
      "Epoch 4/10 | Batch 300/2990 | Loss: 9.2841\n",
      "Epoch 4/10 | Batch 320/2990 | Loss: 10.1002\n",
      "Epoch 4/10 | Batch 340/2990 | Loss: 9.8292\n",
      "Epoch 4/10 | Batch 360/2990 | Loss: 9.9816\n",
      "Epoch 4/10 | Batch 380/2990 | Loss: 12.4328\n",
      "Epoch 4/10 | Batch 400/2990 | Loss: 10.1801\n",
      "Epoch 4/10 | Batch 420/2990 | Loss: 10.5305\n",
      "Epoch 4/10 | Batch 440/2990 | Loss: 9.3908\n",
      "Epoch 4/10 | Batch 460/2990 | Loss: 11.3389\n",
      "Epoch 4/10 | Batch 480/2990 | Loss: 10.6965\n",
      "Epoch 4/10 | Batch 500/2990 | Loss: 11.1860\n",
      "Epoch 4/10 | Batch 520/2990 | Loss: 9.9929\n",
      "Epoch 4/10 | Batch 540/2990 | Loss: 10.0545\n",
      "Epoch 4/10 | Batch 560/2990 | Loss: 10.3629\n",
      "Epoch 4/10 | Batch 580/2990 | Loss: 12.1262\n",
      "Epoch 4/10 | Batch 600/2990 | Loss: 9.6335\n",
      "Epoch 4/10 | Batch 620/2990 | Loss: 10.2448\n",
      "Epoch 4/10 | Batch 640/2990 | Loss: 9.2396\n",
      "Epoch 4/10 | Batch 660/2990 | Loss: 11.4631\n",
      "Epoch 4/10 | Batch 680/2990 | Loss: 10.3637\n",
      "Epoch 4/10 | Batch 700/2990 | Loss: 10.3998\n",
      "Epoch 4/10 | Batch 720/2990 | Loss: 9.6418\n",
      "Epoch 4/10 | Batch 740/2990 | Loss: 10.6204\n",
      "Epoch 4/10 | Batch 760/2990 | Loss: 9.7295\n",
      "Epoch 4/10 | Batch 780/2990 | Loss: 10.1498\n",
      "Epoch 4/10 | Batch 800/2990 | Loss: 10.0133\n",
      "Epoch 4/10 | Batch 820/2990 | Loss: 11.2994\n",
      "Epoch 4/10 | Batch 840/2990 | Loss: 9.3088\n",
      "Epoch 4/10 | Batch 860/2990 | Loss: 10.1018\n",
      "Epoch 4/10 | Batch 880/2990 | Loss: 11.6306\n",
      "Epoch 4/10 | Batch 900/2990 | Loss: 9.9123\n",
      "Epoch 4/10 | Batch 920/2990 | Loss: 9.8284\n",
      "Epoch 4/10 | Batch 940/2990 | Loss: 10.5267\n",
      "Epoch 4/10 | Batch 960/2990 | Loss: 9.3580\n",
      "Epoch 4/10 | Batch 980/2990 | Loss: 9.3777\n",
      "Epoch 4/10 | Batch 1000/2990 | Loss: 9.9137\n",
      "Epoch 4/10 | Batch 1020/2990 | Loss: 9.6173\n",
      "Epoch 4/10 | Batch 1040/2990 | Loss: 10.5030\n",
      "Epoch 4/10 | Batch 1060/2990 | Loss: 9.2564\n",
      "Epoch 4/10 | Batch 1080/2990 | Loss: 10.0048\n",
      "Epoch 4/10 | Batch 1100/2990 | Loss: 9.4252\n",
      "Epoch 4/10 | Batch 1120/2990 | Loss: 9.8961\n",
      "Epoch 4/10 | Batch 1140/2990 | Loss: 9.6311\n",
      "Epoch 4/10 | Batch 1160/2990 | Loss: 10.5909\n",
      "Epoch 4/10 | Batch 1180/2990 | Loss: 9.1938\n",
      "Epoch 4/10 | Batch 1200/2990 | Loss: 9.1657\n",
      "Epoch 4/10 | Batch 1220/2990 | Loss: 9.0071\n",
      "Epoch 4/10 | Batch 1240/2990 | Loss: 8.9309\n",
      "Epoch 4/10 | Batch 1260/2990 | Loss: 10.2943\n",
      "Epoch 4/10 | Batch 1280/2990 | Loss: 10.5146\n",
      "Epoch 4/10 | Batch 1300/2990 | Loss: 9.9299\n",
      "Epoch 4/10 | Batch 1320/2990 | Loss: 9.6765\n",
      "Epoch 4/10 | Batch 1340/2990 | Loss: 10.2216\n",
      "Epoch 4/10 | Batch 1360/2990 | Loss: 10.1490\n",
      "Epoch 4/10 | Batch 1380/2990 | Loss: 12.3932\n",
      "Epoch 4/10 | Batch 1400/2990 | Loss: 9.6846\n",
      "Epoch 4/10 | Batch 1420/2990 | Loss: 9.5120\n",
      "Epoch 4/10 | Batch 1440/2990 | Loss: 9.4772\n",
      "Epoch 4/10 | Batch 1460/2990 | Loss: 9.7869\n",
      "Epoch 4/10 | Batch 1480/2990 | Loss: 9.4976\n",
      "Epoch 4/10 | Batch 1500/2990 | Loss: 9.8859\n",
      "Epoch 4/10 | Batch 1520/2990 | Loss: 9.9598\n",
      "Epoch 4/10 | Batch 1540/2990 | Loss: 9.4576\n",
      "Epoch 4/10 | Batch 1560/2990 | Loss: 9.7984\n",
      "Epoch 4/10 | Batch 1580/2990 | Loss: 10.2563\n",
      "Epoch 4/10 | Batch 1600/2990 | Loss: 10.1220\n",
      "Epoch 4/10 | Batch 1620/2990 | Loss: 10.7971\n",
      "Epoch 4/10 | Batch 1640/2990 | Loss: 9.8432\n",
      "Epoch 4/10 | Batch 1660/2990 | Loss: 9.0966\n",
      "Epoch 4/10 | Batch 1680/2990 | Loss: 9.7857\n",
      "Epoch 4/10 | Batch 1700/2990 | Loss: 10.9380\n",
      "Epoch 4/10 | Batch 1720/2990 | Loss: 10.3639\n",
      "Epoch 4/10 | Batch 1740/2990 | Loss: 10.2982\n",
      "Epoch 4/10 | Batch 1760/2990 | Loss: 9.2155\n",
      "Epoch 4/10 | Batch 1780/2990 | Loss: 9.3734\n",
      "Epoch 4/10 | Batch 1800/2990 | Loss: 9.2894\n",
      "Epoch 4/10 | Batch 1820/2990 | Loss: 9.0813\n",
      "Epoch 4/10 | Batch 1840/2990 | Loss: 11.0176\n",
      "Epoch 4/10 | Batch 1860/2990 | Loss: 9.9010\n",
      "Epoch 4/10 | Batch 1880/2990 | Loss: 11.1627\n",
      "Epoch 4/10 | Batch 1900/2990 | Loss: 9.0212\n",
      "Epoch 4/10 | Batch 1920/2990 | Loss: 10.4577\n",
      "Epoch 4/10 | Batch 1940/2990 | Loss: 9.1895\n",
      "Epoch 4/10 | Batch 1960/2990 | Loss: 9.5062\n",
      "Epoch 4/10 | Batch 1980/2990 | Loss: 9.4995\n",
      "Epoch 4/10 | Batch 2000/2990 | Loss: 10.7808\n",
      "Epoch 4/10 | Batch 2020/2990 | Loss: 9.6852\n",
      "Epoch 4/10 | Batch 2040/2990 | Loss: 9.7324\n",
      "Epoch 4/10 | Batch 2060/2990 | Loss: 9.7800\n",
      "Epoch 4/10 | Batch 2080/2990 | Loss: 10.1994\n",
      "Epoch 4/10 | Batch 2100/2990 | Loss: 9.8648\n",
      "Epoch 4/10 | Batch 2120/2990 | Loss: 9.5233\n",
      "Epoch 4/10 | Batch 2140/2990 | Loss: 10.3864\n",
      "Epoch 4/10 | Batch 2160/2990 | Loss: 9.6957\n",
      "Epoch 4/10 | Batch 2180/2990 | Loss: 9.0190\n",
      "Epoch 4/10 | Batch 2200/2990 | Loss: 9.9970\n",
      "Epoch 4/10 | Batch 2220/2990 | Loss: 10.8150\n",
      "Epoch 4/10 | Batch 2240/2990 | Loss: 10.1939\n",
      "Epoch 4/10 | Batch 2260/2990 | Loss: 9.5891\n",
      "Epoch 4/10 | Batch 2280/2990 | Loss: 9.6839\n",
      "Epoch 4/10 | Batch 2300/2990 | Loss: 9.8823\n",
      "Epoch 4/10 | Batch 2320/2990 | Loss: 9.6782\n",
      "Epoch 4/10 | Batch 2340/2990 | Loss: 9.5338\n",
      "Epoch 4/10 | Batch 2360/2990 | Loss: 9.5593\n",
      "Epoch 4/10 | Batch 2380/2990 | Loss: 9.3639\n",
      "Epoch 4/10 | Batch 2400/2990 | Loss: 10.4501\n",
      "Epoch 4/10 | Batch 2420/2990 | Loss: 9.0915\n",
      "Epoch 4/10 | Batch 2440/2990 | Loss: 9.2841\n",
      "Epoch 4/10 | Batch 2460/2990 | Loss: 10.6511\n",
      "Epoch 4/10 | Batch 2480/2990 | Loss: 9.9048\n",
      "Epoch 4/10 | Batch 2500/2990 | Loss: 10.3159\n",
      "Epoch 4/10 | Batch 2520/2990 | Loss: 9.3288\n",
      "Epoch 4/10 | Batch 2540/2990 | Loss: 11.3802\n",
      "Epoch 4/10 | Batch 2560/2990 | Loss: 9.8049\n",
      "Epoch 4/10 | Batch 2580/2990 | Loss: 9.9160\n",
      "Epoch 4/10 | Batch 2600/2990 | Loss: 10.5328\n",
      "Epoch 4/10 | Batch 2620/2990 | Loss: 10.2082\n",
      "Epoch 4/10 | Batch 2640/2990 | Loss: 9.3864\n",
      "Epoch 4/10 | Batch 2660/2990 | Loss: 9.1721\n",
      "Epoch 4/10 | Batch 2680/2990 | Loss: 9.6965\n",
      "Epoch 4/10 | Batch 2700/2990 | Loss: 9.7706\n",
      "Epoch 4/10 | Batch 2720/2990 | Loss: 9.5895\n",
      "Epoch 4/10 | Batch 2740/2990 | Loss: 9.9474\n",
      "Epoch 4/10 | Batch 2760/2990 | Loss: 9.6058\n",
      "Epoch 4/10 | Batch 2780/2990 | Loss: 9.8606\n",
      "Epoch 4/10 | Batch 2800/2990 | Loss: 9.8243\n",
      "Epoch 4/10 | Batch 2820/2990 | Loss: 9.2326\n",
      "Epoch 4/10 | Batch 2840/2990 | Loss: 9.4802\n",
      "Epoch 4/10 | Batch 2860/2990 | Loss: 12.0548\n",
      "Epoch 4/10 | Batch 2880/2990 | Loss: 10.0711\n",
      "Epoch 4/10 | Batch 2900/2990 | Loss: 9.2404\n",
      "Epoch 4/10 | Batch 2920/2990 | Loss: 9.2715\n",
      "Epoch 4/10 | Batch 2940/2990 | Loss: 9.6803\n",
      "Epoch 4/10 | Batch 2960/2990 | Loss: 10.2636\n",
      "Epoch 4/10 | Batch 2980/2990 | Loss: 9.8863\n",
      "✅ Epoch 4/10 | Train Loss: 9.9792 | Val Loss: 8.7654\n",
      "Epoch 5/10 | Batch 0/2990 | Loss: 9.9401\n",
      "Epoch 5/10 | Batch 20/2990 | Loss: 9.9704\n",
      "Epoch 5/10 | Batch 40/2990 | Loss: 9.4226\n",
      "Epoch 5/10 | Batch 60/2990 | Loss: 9.7622\n",
      "Epoch 5/10 | Batch 80/2990 | Loss: 10.4694\n",
      "Epoch 5/10 | Batch 100/2990 | Loss: 9.2482\n",
      "Epoch 5/10 | Batch 120/2990 | Loss: 9.3554\n",
      "Epoch 5/10 | Batch 140/2990 | Loss: 9.9119\n",
      "Epoch 5/10 | Batch 160/2990 | Loss: 9.2293\n",
      "Epoch 5/10 | Batch 180/2990 | Loss: 9.7018\n",
      "Epoch 5/10 | Batch 200/2990 | Loss: 10.2589\n",
      "Epoch 5/10 | Batch 220/2990 | Loss: 9.8732\n",
      "Epoch 5/10 | Batch 240/2990 | Loss: 12.3284\n",
      "Epoch 5/10 | Batch 260/2990 | Loss: 9.6206\n",
      "Epoch 5/10 | Batch 280/2990 | Loss: 9.9286\n",
      "Epoch 5/10 | Batch 300/2990 | Loss: 9.2722\n",
      "Epoch 5/10 | Batch 320/2990 | Loss: 10.3735\n",
      "Epoch 5/10 | Batch 340/2990 | Loss: 9.1022\n",
      "Epoch 5/10 | Batch 360/2990 | Loss: 8.9407\n",
      "Epoch 5/10 | Batch 380/2990 | Loss: 10.4108\n",
      "Epoch 5/10 | Batch 400/2990 | Loss: 9.1930\n",
      "Epoch 5/10 | Batch 420/2990 | Loss: 9.5310\n",
      "Epoch 5/10 | Batch 440/2990 | Loss: 9.3072\n",
      "Epoch 5/10 | Batch 460/2990 | Loss: 10.1945\n",
      "Epoch 5/10 | Batch 480/2990 | Loss: 9.8134\n",
      "Epoch 5/10 | Batch 500/2990 | Loss: 9.7625\n",
      "Epoch 5/10 | Batch 520/2990 | Loss: 10.9349\n",
      "Epoch 5/10 | Batch 540/2990 | Loss: 8.7199\n",
      "Epoch 5/10 | Batch 560/2990 | Loss: 8.8031\n",
      "Epoch 5/10 | Batch 580/2990 | Loss: 9.4764\n",
      "Epoch 5/10 | Batch 600/2990 | Loss: 9.7135\n",
      "Epoch 5/10 | Batch 620/2990 | Loss: 9.4954\n",
      "Epoch 5/10 | Batch 640/2990 | Loss: 9.2220\n",
      "Epoch 5/10 | Batch 660/2990 | Loss: 9.2797\n",
      "Epoch 5/10 | Batch 680/2990 | Loss: 9.7386\n",
      "Epoch 5/10 | Batch 700/2990 | Loss: 9.3689\n",
      "Epoch 5/10 | Batch 720/2990 | Loss: 9.1046\n",
      "Epoch 5/10 | Batch 740/2990 | Loss: 9.2470\n",
      "Epoch 5/10 | Batch 760/2990 | Loss: 10.2864\n",
      "Epoch 5/10 | Batch 780/2990 | Loss: 9.8663\n",
      "Epoch 5/10 | Batch 800/2990 | Loss: 9.4817\n",
      "Epoch 5/10 | Batch 820/2990 | Loss: 10.1751\n",
      "Epoch 5/10 | Batch 840/2990 | Loss: 9.3363\n",
      "Epoch 5/10 | Batch 860/2990 | Loss: 10.0182\n",
      "Epoch 5/10 | Batch 880/2990 | Loss: 9.2913\n",
      "Epoch 5/10 | Batch 900/2990 | Loss: 10.6068\n",
      "Epoch 5/10 | Batch 920/2990 | Loss: 9.1657\n",
      "Epoch 5/10 | Batch 940/2990 | Loss: 9.0822\n",
      "Epoch 5/10 | Batch 960/2990 | Loss: 9.4003\n",
      "Epoch 5/10 | Batch 980/2990 | Loss: 10.1548\n",
      "Epoch 5/10 | Batch 1000/2990 | Loss: 9.1626\n",
      "Epoch 5/10 | Batch 1020/2990 | Loss: 10.0722\n",
      "Epoch 5/10 | Batch 1040/2990 | Loss: 8.8996\n",
      "Epoch 5/10 | Batch 1060/2990 | Loss: 9.3963\n",
      "Epoch 5/10 | Batch 1080/2990 | Loss: 12.0487\n",
      "Epoch 5/10 | Batch 1100/2990 | Loss: 9.3557\n",
      "Epoch 5/10 | Batch 1120/2990 | Loss: 9.3522\n",
      "Epoch 5/10 | Batch 1140/2990 | Loss: 9.2309\n",
      "Epoch 5/10 | Batch 1160/2990 | Loss: 8.8767\n",
      "Epoch 5/10 | Batch 1180/2990 | Loss: 9.6788\n",
      "Epoch 5/10 | Batch 1200/2990 | Loss: 9.0843\n",
      "Epoch 5/10 | Batch 1220/2990 | Loss: 8.8290\n",
      "Epoch 5/10 | Batch 1240/2990 | Loss: 12.0280\n",
      "Epoch 5/10 | Batch 1260/2990 | Loss: 9.3219\n",
      "Epoch 5/10 | Batch 1280/2990 | Loss: 10.8159\n",
      "Epoch 5/10 | Batch 1300/2990 | Loss: 9.4240\n",
      "Epoch 5/10 | Batch 1320/2990 | Loss: 9.9858\n",
      "Epoch 5/10 | Batch 1340/2990 | Loss: 10.7762\n",
      "Epoch 5/10 | Batch 1360/2990 | Loss: 9.3451\n",
      "Epoch 5/10 | Batch 1380/2990 | Loss: 8.8343\n",
      "Epoch 5/10 | Batch 1400/2990 | Loss: 11.5062\n",
      "Epoch 5/10 | Batch 1420/2990 | Loss: 9.6342\n",
      "Epoch 5/10 | Batch 1440/2990 | Loss: 9.2091\n",
      "Epoch 5/10 | Batch 1460/2990 | Loss: 9.7805\n",
      "Epoch 5/10 | Batch 1480/2990 | Loss: 9.5756\n",
      "Epoch 5/10 | Batch 1500/2990 | Loss: 9.4006\n",
      "Epoch 5/10 | Batch 1520/2990 | Loss: 10.2610\n",
      "Epoch 5/10 | Batch 1540/2990 | Loss: 9.7078\n",
      "Epoch 5/10 | Batch 1560/2990 | Loss: 9.5737\n",
      "Epoch 5/10 | Batch 1580/2990 | Loss: 9.7454\n",
      "Epoch 5/10 | Batch 1600/2990 | Loss: 9.6896\n",
      "Epoch 5/10 | Batch 1620/2990 | Loss: 9.8518\n",
      "Epoch 5/10 | Batch 1640/2990 | Loss: 9.8128\n",
      "Epoch 5/10 | Batch 1660/2990 | Loss: 9.9174\n",
      "Epoch 5/10 | Batch 1680/2990 | Loss: 10.8868\n",
      "Epoch 5/10 | Batch 1700/2990 | Loss: 9.6136\n",
      "Epoch 5/10 | Batch 1720/2990 | Loss: 8.7583\n",
      "Epoch 5/10 | Batch 1740/2990 | Loss: 9.7118\n",
      "Epoch 5/10 | Batch 1760/2990 | Loss: 9.5197\n",
      "Epoch 5/10 | Batch 1780/2990 | Loss: 10.2661\n",
      "Epoch 5/10 | Batch 1800/2990 | Loss: 9.0361\n",
      "Epoch 5/10 | Batch 1820/2990 | Loss: 8.6483\n",
      "Epoch 5/10 | Batch 1840/2990 | Loss: 8.8817\n",
      "Epoch 5/10 | Batch 1860/2990 | Loss: 10.2168\n",
      "Epoch 5/10 | Batch 1880/2990 | Loss: 8.6635\n",
      "Epoch 5/10 | Batch 1900/2990 | Loss: 8.9711\n",
      "Epoch 5/10 | Batch 1920/2990 | Loss: 9.6302\n",
      "Epoch 5/10 | Batch 1940/2990 | Loss: 8.8027\n",
      "Epoch 5/10 | Batch 1960/2990 | Loss: 9.6329\n",
      "Epoch 5/10 | Batch 1980/2990 | Loss: 9.9451\n",
      "Epoch 5/10 | Batch 2000/2990 | Loss: 9.0772\n",
      "Epoch 5/10 | Batch 2020/2990 | Loss: 9.7775\n",
      "Epoch 5/10 | Batch 2040/2990 | Loss: 9.9627\n",
      "Epoch 5/10 | Batch 2060/2990 | Loss: 9.9180\n",
      "Epoch 5/10 | Batch 2080/2990 | Loss: 9.0877\n",
      "Epoch 5/10 | Batch 2100/2990 | Loss: 10.3551\n",
      "Epoch 5/10 | Batch 2120/2990 | Loss: 9.2774\n",
      "Epoch 5/10 | Batch 2140/2990 | Loss: 8.8677\n",
      "Epoch 5/10 | Batch 2160/2990 | Loss: 9.5101\n",
      "Epoch 5/10 | Batch 2180/2990 | Loss: 10.4955\n",
      "Epoch 5/10 | Batch 2200/2990 | Loss: 10.4060\n",
      "Epoch 5/10 | Batch 2220/2990 | Loss: 9.1920\n",
      "Epoch 5/10 | Batch 2240/2990 | Loss: 9.5318\n",
      "Epoch 5/10 | Batch 2260/2990 | Loss: 10.4651\n",
      "Epoch 5/10 | Batch 2280/2990 | Loss: 8.8632\n",
      "Epoch 5/10 | Batch 2300/2990 | Loss: 11.5105\n",
      "Epoch 5/10 | Batch 2320/2990 | Loss: 9.1969\n",
      "Epoch 5/10 | Batch 2340/2990 | Loss: 8.9977\n",
      "Epoch 5/10 | Batch 2360/2990 | Loss: 9.7394\n",
      "Epoch 5/10 | Batch 2380/2990 | Loss: 9.6693\n",
      "Epoch 5/10 | Batch 2400/2990 | Loss: 9.7707\n",
      "Epoch 5/10 | Batch 2420/2990 | Loss: 10.6392\n",
      "Epoch 5/10 | Batch 2440/2990 | Loss: 9.8794\n",
      "Epoch 5/10 | Batch 2460/2990 | Loss: 9.5688\n",
      "Epoch 5/10 | Batch 2480/2990 | Loss: 9.9098\n",
      "Epoch 5/10 | Batch 2500/2990 | Loss: 9.4303\n",
      "Epoch 5/10 | Batch 2520/2990 | Loss: 8.9806\n",
      "Epoch 5/10 | Batch 2540/2990 | Loss: 9.7937\n",
      "Epoch 5/10 | Batch 2560/2990 | Loss: 9.5051\n",
      "Epoch 5/10 | Batch 2580/2990 | Loss: 9.3937\n",
      "Epoch 5/10 | Batch 2600/2990 | Loss: 9.1058\n",
      "Epoch 5/10 | Batch 2620/2990 | Loss: 8.6369\n",
      "Epoch 5/10 | Batch 2640/2990 | Loss: 9.2913\n",
      "Epoch 5/10 | Batch 2660/2990 | Loss: 9.6899\n",
      "Epoch 5/10 | Batch 2680/2990 | Loss: 8.7971\n",
      "Epoch 5/10 | Batch 2700/2990 | Loss: 12.8973\n",
      "Epoch 5/10 | Batch 2720/2990 | Loss: 9.2937\n",
      "Epoch 5/10 | Batch 2740/2990 | Loss: 9.6644\n",
      "Epoch 5/10 | Batch 2760/2990 | Loss: 8.9386\n",
      "Epoch 5/10 | Batch 2780/2990 | Loss: 12.0159\n",
      "Epoch 5/10 | Batch 2800/2990 | Loss: 10.1599\n",
      "Epoch 5/10 | Batch 2820/2990 | Loss: 8.7841\n",
      "Epoch 5/10 | Batch 2840/2990 | Loss: 9.2628\n",
      "Epoch 5/10 | Batch 2860/2990 | Loss: 8.8750\n",
      "Epoch 5/10 | Batch 2880/2990 | Loss: 8.8661\n",
      "Epoch 5/10 | Batch 2900/2990 | Loss: 9.5408\n",
      "Epoch 5/10 | Batch 2920/2990 | Loss: 9.5207\n",
      "Epoch 5/10 | Batch 2940/2990 | Loss: 10.2232\n",
      "Epoch 5/10 | Batch 2960/2990 | Loss: 8.7473\n",
      "Epoch 5/10 | Batch 2980/2990 | Loss: 9.4424\n",
      "✅ Epoch 5/10 | Train Loss: 9.5971 | Val Loss: 8.3901\n",
      "Epoch 6/10 | Batch 0/2990 | Loss: 8.7845\n",
      "Epoch 6/10 | Batch 20/2990 | Loss: 8.6419\n",
      "Epoch 6/10 | Batch 40/2990 | Loss: 10.0041\n",
      "Epoch 6/10 | Batch 60/2990 | Loss: 9.8722\n",
      "Epoch 6/10 | Batch 80/2990 | Loss: 9.1922\n",
      "Epoch 6/10 | Batch 100/2990 | Loss: 9.2032\n",
      "Epoch 6/10 | Batch 120/2990 | Loss: 9.2190\n",
      "Epoch 6/10 | Batch 140/2990 | Loss: 9.5534\n",
      "Epoch 6/10 | Batch 160/2990 | Loss: 8.9191\n",
      "Epoch 6/10 | Batch 180/2990 | Loss: 8.5196\n",
      "Epoch 6/10 | Batch 200/2990 | Loss: 10.2147\n",
      "Epoch 6/10 | Batch 220/2990 | Loss: 8.2652\n",
      "Epoch 6/10 | Batch 240/2990 | Loss: 9.4277\n",
      "Epoch 6/10 | Batch 260/2990 | Loss: 9.2854\n",
      "Epoch 6/10 | Batch 280/2990 | Loss: 9.0050\n",
      "Epoch 6/10 | Batch 300/2990 | Loss: 9.6466\n",
      "Epoch 6/10 | Batch 320/2990 | Loss: 10.0902\n",
      "Epoch 6/10 | Batch 340/2990 | Loss: 8.9132\n",
      "Epoch 6/10 | Batch 360/2990 | Loss: 9.2573\n",
      "Epoch 6/10 | Batch 380/2990 | Loss: 8.8316\n",
      "Epoch 6/10 | Batch 400/2990 | Loss: 8.8799\n",
      "Epoch 6/10 | Batch 420/2990 | Loss: 9.5753\n",
      "Epoch 6/10 | Batch 440/2990 | Loss: 8.7115\n",
      "Epoch 6/10 | Batch 460/2990 | Loss: 10.2241\n",
      "Epoch 6/10 | Batch 480/2990 | Loss: 8.9473\n",
      "Epoch 6/10 | Batch 500/2990 | Loss: 9.2463\n",
      "Epoch 6/10 | Batch 520/2990 | Loss: 9.5197\n",
      "Epoch 6/10 | Batch 540/2990 | Loss: 9.0572\n",
      "Epoch 6/10 | Batch 560/2990 | Loss: 8.9669\n",
      "Epoch 6/10 | Batch 580/2990 | Loss: 9.1322\n",
      "Epoch 6/10 | Batch 600/2990 | Loss: 8.5522\n",
      "Epoch 6/10 | Batch 620/2990 | Loss: 9.2481\n",
      "Epoch 6/10 | Batch 640/2990 | Loss: 10.2624\n",
      "Epoch 6/10 | Batch 660/2990 | Loss: 9.6494\n",
      "Epoch 6/10 | Batch 680/2990 | Loss: 9.8792\n",
      "Epoch 6/10 | Batch 700/2990 | Loss: 9.3623\n",
      "Epoch 6/10 | Batch 720/2990 | Loss: 9.5427\n",
      "Epoch 6/10 | Batch 740/2990 | Loss: 9.3182\n",
      "Epoch 6/10 | Batch 760/2990 | Loss: 9.0275\n",
      "Epoch 6/10 | Batch 780/2990 | Loss: 9.6550\n",
      "Epoch 6/10 | Batch 800/2990 | Loss: 8.8365\n",
      "Epoch 6/10 | Batch 820/2990 | Loss: 9.2245\n",
      "Epoch 6/10 | Batch 840/2990 | Loss: 8.8632\n",
      "Epoch 6/10 | Batch 860/2990 | Loss: 10.0743\n",
      "Epoch 6/10 | Batch 880/2990 | Loss: 9.2188\n",
      "Epoch 6/10 | Batch 900/2990 | Loss: 10.3363\n",
      "Epoch 6/10 | Batch 920/2990 | Loss: 8.5815\n",
      "Epoch 6/10 | Batch 940/2990 | Loss: 8.9940\n",
      "Epoch 6/10 | Batch 960/2990 | Loss: 9.0970\n",
      "Epoch 6/10 | Batch 980/2990 | Loss: 9.1307\n",
      "Epoch 6/10 | Batch 1000/2990 | Loss: 8.5965\n",
      "Epoch 6/10 | Batch 1020/2990 | Loss: 10.6561\n",
      "Epoch 6/10 | Batch 1040/2990 | Loss: 8.5703\n",
      "Epoch 6/10 | Batch 1060/2990 | Loss: 9.2248\n",
      "Epoch 6/10 | Batch 1080/2990 | Loss: 9.3273\n",
      "Epoch 6/10 | Batch 1100/2990 | Loss: 9.2211\n",
      "Epoch 6/10 | Batch 1120/2990 | Loss: 9.4522\n",
      "Epoch 6/10 | Batch 1140/2990 | Loss: 9.0928\n",
      "Epoch 6/10 | Batch 1160/2990 | Loss: 9.5388\n",
      "Epoch 6/10 | Batch 1180/2990 | Loss: 9.7900\n",
      "Epoch 6/10 | Batch 1200/2990 | Loss: 8.8527\n",
      "Epoch 6/10 | Batch 1220/2990 | Loss: 8.8003\n",
      "Epoch 6/10 | Batch 1240/2990 | Loss: 8.7032\n",
      "Epoch 6/10 | Batch 1260/2990 | Loss: 10.5833\n",
      "Epoch 6/10 | Batch 1280/2990 | Loss: 8.5116\n",
      "Epoch 6/10 | Batch 1300/2990 | Loss: 8.4692\n",
      "Epoch 6/10 | Batch 1320/2990 | Loss: 8.3482\n",
      "Epoch 6/10 | Batch 1340/2990 | Loss: 8.6956\n",
      "Epoch 6/10 | Batch 1360/2990 | Loss: 8.8530\n",
      "Epoch 6/10 | Batch 1380/2990 | Loss: 10.6580\n",
      "Epoch 6/10 | Batch 1400/2990 | Loss: 9.3115\n",
      "Epoch 6/10 | Batch 1420/2990 | Loss: 9.1290\n",
      "Epoch 6/10 | Batch 1440/2990 | Loss: 9.4682\n",
      "Epoch 6/10 | Batch 1460/2990 | Loss: 9.2783\n",
      "Epoch 6/10 | Batch 1480/2990 | Loss: 9.0197\n",
      "Epoch 6/10 | Batch 1500/2990 | Loss: 9.4857\n",
      "Epoch 6/10 | Batch 1520/2990 | Loss: 8.9233\n",
      "Epoch 6/10 | Batch 1540/2990 | Loss: 9.3506\n",
      "Epoch 6/10 | Batch 1560/2990 | Loss: 8.9207\n",
      "Epoch 6/10 | Batch 1580/2990 | Loss: 8.9203\n",
      "Epoch 6/10 | Batch 1600/2990 | Loss: 10.2672\n",
      "Epoch 6/10 | Batch 1620/2990 | Loss: 12.3147\n",
      "Epoch 6/10 | Batch 1640/2990 | Loss: 9.6297\n",
      "Epoch 6/10 | Batch 1660/2990 | Loss: 8.6607\n",
      "Epoch 6/10 | Batch 1680/2990 | Loss: 8.1674\n",
      "Epoch 6/10 | Batch 1700/2990 | Loss: 9.0424\n",
      "Epoch 6/10 | Batch 1720/2990 | Loss: 9.0166\n",
      "Epoch 6/10 | Batch 1740/2990 | Loss: 10.6585\n",
      "Epoch 6/10 | Batch 1760/2990 | Loss: 9.0121\n",
      "Epoch 6/10 | Batch 1780/2990 | Loss: 12.0651\n",
      "Epoch 6/10 | Batch 1800/2990 | Loss: 9.2379\n",
      "Epoch 6/10 | Batch 1820/2990 | Loss: 8.7719\n",
      "Epoch 6/10 | Batch 1840/2990 | Loss: 10.2240\n",
      "Epoch 6/10 | Batch 1860/2990 | Loss: 8.7696\n",
      "Epoch 6/10 | Batch 1880/2990 | Loss: 9.1931\n",
      "Epoch 6/10 | Batch 1900/2990 | Loss: 8.9159\n",
      "Epoch 6/10 | Batch 1920/2990 | Loss: 8.7554\n",
      "Epoch 6/10 | Batch 1940/2990 | Loss: 9.1162\n",
      "Epoch 6/10 | Batch 1960/2990 | Loss: 8.6273\n",
      "Epoch 6/10 | Batch 1980/2990 | Loss: 9.3724\n",
      "Epoch 6/10 | Batch 2000/2990 | Loss: 9.9356\n",
      "Epoch 6/10 | Batch 2020/2990 | Loss: 9.1430\n",
      "Epoch 6/10 | Batch 2040/2990 | Loss: 8.4850\n",
      "Epoch 6/10 | Batch 2060/2990 | Loss: 8.7398\n",
      "Epoch 6/10 | Batch 2080/2990 | Loss: 8.0924\n",
      "Epoch 6/10 | Batch 2100/2990 | Loss: 10.0078\n",
      "Epoch 6/10 | Batch 2120/2990 | Loss: 9.2574\n",
      "Epoch 6/10 | Batch 2140/2990 | Loss: 10.6100\n",
      "Epoch 6/10 | Batch 2160/2990 | Loss: 9.1762\n",
      "Epoch 6/10 | Batch 2180/2990 | Loss: 9.1293\n",
      "Epoch 6/10 | Batch 2200/2990 | Loss: 12.6602\n",
      "Epoch 6/10 | Batch 2220/2990 | Loss: 11.2518\n",
      "Epoch 6/10 | Batch 2240/2990 | Loss: 9.2252\n",
      "Epoch 6/10 | Batch 2260/2990 | Loss: 9.0177\n",
      "Epoch 6/10 | Batch 2280/2990 | Loss: 8.9299\n",
      "Epoch 6/10 | Batch 2300/2990 | Loss: 9.0498\n",
      "Epoch 6/10 | Batch 2320/2990 | Loss: 9.9704\n",
      "Epoch 6/10 | Batch 2340/2990 | Loss: 8.6876\n",
      "Epoch 6/10 | Batch 2360/2990 | Loss: 8.0967\n",
      "Epoch 6/10 | Batch 2380/2990 | Loss: 8.8082\n",
      "Epoch 6/10 | Batch 2400/2990 | Loss: 9.3515\n",
      "Epoch 6/10 | Batch 2420/2990 | Loss: 9.4171\n",
      "Epoch 6/10 | Batch 2440/2990 | Loss: 9.0371\n",
      "Epoch 6/10 | Batch 2460/2990 | Loss: 9.5131\n",
      "Epoch 6/10 | Batch 2480/2990 | Loss: 12.1450\n",
      "Epoch 6/10 | Batch 2500/2990 | Loss: 9.8734\n",
      "Epoch 6/10 | Batch 2520/2990 | Loss: 10.4184\n",
      "Epoch 6/10 | Batch 2540/2990 | Loss: 11.6736\n",
      "Epoch 6/10 | Batch 2560/2990 | Loss: 9.3312\n",
      "Epoch 6/10 | Batch 2580/2990 | Loss: 8.6382\n",
      "Epoch 6/10 | Batch 2600/2990 | Loss: 8.6447\n",
      "Epoch 6/10 | Batch 2620/2990 | Loss: 9.4311\n",
      "Epoch 6/10 | Batch 2640/2990 | Loss: 9.5415\n",
      "Epoch 6/10 | Batch 2660/2990 | Loss: 8.5191\n",
      "Epoch 6/10 | Batch 2680/2990 | Loss: 9.4058\n",
      "Epoch 6/10 | Batch 2700/2990 | Loss: 8.4606\n",
      "Epoch 6/10 | Batch 2720/2990 | Loss: 8.8726\n",
      "Epoch 6/10 | Batch 2740/2990 | Loss: 10.1749\n",
      "Epoch 6/10 | Batch 2760/2990 | Loss: 8.3285\n",
      "Epoch 6/10 | Batch 2780/2990 | Loss: 8.9389\n",
      "Epoch 6/10 | Batch 2800/2990 | Loss: 8.6668\n",
      "Epoch 6/10 | Batch 2820/2990 | Loss: 11.7156\n",
      "Epoch 6/10 | Batch 2840/2990 | Loss: 10.6175\n",
      "Epoch 6/10 | Batch 2860/2990 | Loss: 9.7510\n",
      "Epoch 6/10 | Batch 2880/2990 | Loss: 8.5169\n",
      "Epoch 6/10 | Batch 2900/2990 | Loss: 8.4179\n",
      "Epoch 6/10 | Batch 2920/2990 | Loss: 9.0721\n",
      "Epoch 6/10 | Batch 2940/2990 | Loss: 8.8485\n",
      "Epoch 6/10 | Batch 2960/2990 | Loss: 10.5553\n",
      "Epoch 6/10 | Batch 2980/2990 | Loss: 9.0460\n",
      "✅ Epoch 6/10 | Train Loss: 9.2886 | Val Loss: 8.7771\n",
      "Epoch 7/10 | Batch 0/2990 | Loss: 9.2613\n",
      "Epoch 7/10 | Batch 20/2990 | Loss: 9.1586\n",
      "Epoch 7/10 | Batch 40/2990 | Loss: 8.6648\n",
      "Epoch 7/10 | Batch 60/2990 | Loss: 8.2145\n",
      "Epoch 7/10 | Batch 80/2990 | Loss: 8.8582\n",
      "Epoch 7/10 | Batch 100/2990 | Loss: 8.7903\n",
      "Epoch 7/10 | Batch 120/2990 | Loss: 9.4565\n",
      "Epoch 7/10 | Batch 140/2990 | Loss: 9.2779\n",
      "Epoch 7/10 | Batch 160/2990 | Loss: 10.0324\n",
      "Epoch 7/10 | Batch 180/2990 | Loss: 9.6333\n",
      "Epoch 7/10 | Batch 200/2990 | Loss: 8.7913\n",
      "Epoch 7/10 | Batch 220/2990 | Loss: 9.4638\n",
      "Epoch 7/10 | Batch 240/2990 | Loss: 8.8612\n",
      "Epoch 7/10 | Batch 260/2990 | Loss: 8.3061\n",
      "Epoch 7/10 | Batch 280/2990 | Loss: 8.9988\n",
      "Epoch 7/10 | Batch 300/2990 | Loss: 8.7566\n",
      "Epoch 7/10 | Batch 320/2990 | Loss: 9.2548\n",
      "Epoch 7/10 | Batch 340/2990 | Loss: 8.4488\n",
      "Epoch 7/10 | Batch 360/2990 | Loss: 9.3989\n",
      "Epoch 7/10 | Batch 380/2990 | Loss: 9.1862\n",
      "Epoch 7/10 | Batch 400/2990 | Loss: 8.8113\n",
      "Epoch 7/10 | Batch 420/2990 | Loss: 8.0937\n",
      "Epoch 7/10 | Batch 440/2990 | Loss: 9.0374\n",
      "Epoch 7/10 | Batch 460/2990 | Loss: 8.5229\n",
      "Epoch 7/10 | Batch 480/2990 | Loss: 9.1528\n",
      "Epoch 7/10 | Batch 500/2990 | Loss: 9.9201\n",
      "Epoch 7/10 | Batch 520/2990 | Loss: 9.3192\n",
      "Epoch 7/10 | Batch 540/2990 | Loss: 8.7953\n",
      "Epoch 7/10 | Batch 560/2990 | Loss: 8.3100\n",
      "Epoch 7/10 | Batch 580/2990 | Loss: 10.1761\n",
      "Epoch 7/10 | Batch 600/2990 | Loss: 9.0444\n",
      "Epoch 7/10 | Batch 620/2990 | Loss: 9.1215\n",
      "Epoch 7/10 | Batch 640/2990 | Loss: 8.7804\n",
      "Epoch 7/10 | Batch 660/2990 | Loss: 8.6118\n",
      "Epoch 7/10 | Batch 680/2990 | Loss: 8.3497\n",
      "Epoch 7/10 | Batch 700/2990 | Loss: 9.2198\n",
      "Epoch 7/10 | Batch 720/2990 | Loss: 8.3315\n",
      "Epoch 7/10 | Batch 740/2990 | Loss: 9.5438\n",
      "Epoch 7/10 | Batch 760/2990 | Loss: 8.3797\n",
      "Epoch 7/10 | Batch 780/2990 | Loss: 8.9488\n",
      "Epoch 7/10 | Batch 800/2990 | Loss: 9.1466\n",
      "Epoch 7/10 | Batch 820/2990 | Loss: 9.2623\n",
      "Epoch 7/10 | Batch 840/2990 | Loss: 8.9484\n",
      "Epoch 7/10 | Batch 860/2990 | Loss: 8.7149\n",
      "Epoch 7/10 | Batch 880/2990 | Loss: 8.8577\n",
      "Epoch 7/10 | Batch 900/2990 | Loss: 10.6718\n",
      "Epoch 7/10 | Batch 920/2990 | Loss: 8.1932\n",
      "Epoch 7/10 | Batch 940/2990 | Loss: 11.2021\n",
      "Epoch 7/10 | Batch 960/2990 | Loss: 8.3038\n",
      "Epoch 7/10 | Batch 980/2990 | Loss: 8.5885\n",
      "Epoch 7/10 | Batch 1000/2990 | Loss: 9.0852\n",
      "Epoch 7/10 | Batch 1020/2990 | Loss: 9.9119\n",
      "Epoch 7/10 | Batch 1040/2990 | Loss: 8.4832\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m optimizer = optim.Adam(EfficientNetV2Model.parameters(), lr=\u001b[32m1e-4\u001b[39m)\n\u001b[32m      3\u001b[39m scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEfficientNetV2Model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs)\u001b[39m\n\u001b[32m     20\u001b[39m loss.backward()\n\u001b[32m     21\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_idx % \u001b[32m20\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "EfficientNetV2Model = EfficientNetV2SWheatCountWithConfidence().to(device)\n",
    "optimizer = optim.Adam(EfficientNetV2Model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model(EfficientNetV2Model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoAtNetModel = CoAtNetWheatCountWithConfidence().to(device)\n",
    "# optimizer = optim.Adam(CoAtNetModel.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "# train_model(CoAtNetModel, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
