{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from EfficientNetV2.Model import EfficientNetV2SWheatCountWithConfidence\n",
    "from DenseNet.Model import DenseNet121WheatModel\n",
    "from RepVGGA1.Model import RepVGGA1WheatModelWithConfidence\n",
    "from ConvNeXtTiny.model import ConvNeXtTinyWheatModelWithConfidence\n",
    "from dataLoaderFunc import loadSplitData, createLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Custom Gaussian NLL Loss\n",
    "def gaussian_nll_loss(pred_mean, pred_logvar, target):\n",
    "    precision = torch.exp(-pred_logvar)\n",
    "    return torch.mean(precision * (target - pred_mean)**2 + pred_logvar)\n",
    "\n",
    "# ✅ Laplace NLL Loss (Robust + Confidence-Aware)\n",
    "def laplace_nll_loss(pred_mean, pred_logvar, target):\n",
    "    scale = torch.exp(pred_logvar)  # predicted Laplace scale\n",
    "    loss = torch.abs(target - pred_mean) / scale + pred_logvar\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# ✅ Training Function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_batch, dsm_batch)  # output: [B, 2]\n",
    "            pred_mean = output[:, 0]\n",
    "            pred_logvar = output[:, 1]\n",
    "            loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                output = model(rgb_batch, dsm_batch)\n",
    "                pred_mean = output[:, 0]\n",
    "                pred_logvar = output[:, 1]\n",
    "                loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save model\n",
    "        torch.save(model.state_dict(), f\"EffNetV2S_EarCount_Conf_E{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "# ✅ Full Training Function\n",
    "def train_model_laplace(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for rgb_batch, dsm_batch, label_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            rgb_batch = rgb_batch.to(device)\n",
    "            dsm_batch = dsm_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_batch, dsm_batch)  # [B, 2]\n",
    "            pred_mean = output[:, 0]\n",
    "            pred_logvar = output[:, 1]\n",
    "\n",
    "            loss = laplace_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch = rgb_batch.to(device)\n",
    "                dsm_batch = dsm_batch.to(device)\n",
    "                label_batch = label_batch.to(device)\n",
    "\n",
    "                output = model(rgb_batch, dsm_batch)\n",
    "                pred_mean = output[:, 0]\n",
    "                pred_logvar = output[:, 1]\n",
    "\n",
    "                loss = laplace_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save Model\n",
    "        torch.save(model.state_dict(), f\"EffNetV2S_Laplace_Epoch{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ✅ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ✅ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ✅ Default to CPU if no GPU is available\n",
    "print(f\"✅ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pacha\\.cache\\huggingface\\hub\\models--timm--convnext_tiny.in12k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Epoch 1/10:   0%|          | 0/2990 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "ConvNeXtTinyModel = ConvNeXtTinyWheatModelWithConfidence().to(device)\n",
    "optimizer = optim.Adam(ConvNeXtTinyModel.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(ConvNeXtTinyModel, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNetModel = DenseNet121WheatModel().to(device)\n",
    "optimizer = optim.Adam(DenseNetModel.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(DenseNetModel, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RepVGGA1Model = RepVGGA1WheatModelWithConfidence().to(device)\n",
    "optimizer = optim.Adam(RepVGGA1Model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(RepVGGA1Model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNetV2Model = EfficientNetV2SWheatCountWithConfidence().to(device)\n",
    "optimizer = optim.Adam(EfficientNetV2Model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(EfficientNetV2Model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoAtNetModel = CoAtNetWheatCountWithConfidence().to(device)\n",
    "# optimizer = optim.Adam(CoAtNetModel.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "# train_model(CoAtNetModel, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
