{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Get current working directory instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from EfficientNetV2.Model import EfficientNetV2SWheatCountWithConfidence\n",
    "from DenseNet.Model import DenseNet121WheatModel\n",
    "from RepVGGA1.Model import RepVGGA1WheatModelWithConfidence\n",
    "from ConvNeXtTiny.model import ConvNeXtTinyWheatModelWithConfidence\n",
    "from dataLoaderFunc import loadSplitData, createLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Custom Gaussian NLL Loss\n",
    "def gaussian_nll_loss(pred_mean, pred_logvar, target):\n",
    "    precision = torch.exp(-pred_logvar)\n",
    "    return torch.mean(precision * (target - pred_mean)**2 + pred_logvar)\n",
    "\n",
    "# ✅ Laplace NLL Loss (Robust + Confidence-Aware)\n",
    "def laplace_nll_loss(pred_mean, pred_logvar, target):\n",
    "    scale = torch.exp(pred_logvar)  # predicted Laplace scale\n",
    "    loss = torch.abs(target - pred_mean) / scale + pred_logvar\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# ✅ Training Function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, fileName, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (rgb_batch, dsm_batch, label_batch) in enumerate(train_loader):\n",
    "            rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_batch, dsm_batch)  # output: [B, 2]\n",
    "            pred_mean = output[:, 0]\n",
    "            pred_logvar = output[:, 1]\n",
    "            loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch, dsm_batch, label_batch = rgb_batch.to(device), dsm_batch.to(device), label_batch.to(device)\n",
    "                output = model(rgb_batch, dsm_batch)\n",
    "                pred_mean = output[:, 0]\n",
    "                pred_logvar = output[:, 1]\n",
    "                loss = gaussian_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save model\n",
    "        torch.save(model.state_dict(), f\"{fileName}{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "# ✅ Full Training Function\n",
    "def train_model_laplace(model, train_loader, val_loader, optimizer, scheduler, device, fileName,  num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for rgb_batch, dsm_batch, label_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            rgb_batch = rgb_batch.to(device)\n",
    "            dsm_batch = dsm_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_batch, dsm_batch)  # [B, 2]\n",
    "            pred_mean = output[:, 0]\n",
    "            pred_logvar = output[:, 1]\n",
    "\n",
    "            loss = laplace_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ✅ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rgb_batch, dsm_batch, label_batch in val_loader:\n",
    "                rgb_batch = rgb_batch.to(device)\n",
    "                dsm_batch = dsm_batch.to(device)\n",
    "                label_batch = label_batch.to(device)\n",
    "\n",
    "                output = model(rgb_batch, dsm_batch)\n",
    "                pred_mean = output[:, 0]\n",
    "                pred_logvar = output[:, 1]\n",
    "\n",
    "                loss = laplace_nll_loss(pred_mean, pred_logvar, label_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # ✅ Save Model\n",
    "        torch.save(model.state_dict(), f\"{fileName}{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 47840, Validation Size: 5980, Test Size: 5980\n",
      "Train Batches: 2990, Validation Batches: 374, Test Batches: 374\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = loadSplitData(\"RGB_DSM_totEarNum.csv\")\n",
    "train_loader, val_loader, test_loader = createLoader(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # ✅ Use Apple Metal (Mac M1/M2)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # ✅ Use NVIDIA CUDA (Windows RTX 4060)\n",
    "else:\n",
    "    device = \"cpu\"  # ✅ Default to CPU if no GPU is available\n",
    "print(f\"✅ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNeXtTinyModel = ConvNeXtTinyWheatModelWithConfidence().to(device)\n",
    "# optimizer = optim.Adam(ConvNeXtTinyModel.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "# train_model_laplace(ConvNeXtTinyModel, train_loader, val_loader, optimizer, scheduler, device, \"ConvNeXtTinyModel\", num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RepVGGA1Model = RepVGGA1WheatModelWithConfidence().to(device)\n",
    "# optimizer = optim.Adam(RepVGGA1Model.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "# train_model_laplace(RepVGGA1Model, train_loader, val_loader, optimizer, scheduler, device, \"RepVGGA1LaplaceModel\", num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetV2Model = EfficientNetV2SWheatCountWithConfidence().to(device)\n",
    "# optimizer = optim.Adam(EfficientNetV2Model.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "# train_model_laplace(EfficientNetV2Model, train_loader, val_loader, optimizer, scheduler, device, \"EfficientNetV2LaplaceModel\",  num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pacha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/10: 100%|██████████| 2990/2990 [39:57<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 | Train Loss: 6.1249 | Val Loss: 4.6774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2990/2990 [41:44<00:00,  1.19it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 | Train Loss: 5.0245 | Val Loss: 4.4633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 2990/2990 [39:07<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 | Train Loss: 4.8982 | Val Loss: 4.6957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  72%|███████▏  | 2147/2990 [31:01<11:45,  1.20it/s]"
     ]
    }
   ],
   "source": [
    "DenseNetModel = DenseNet121WheatModel().to(device)\n",
    "optimizer = optim.Adam(DenseNetModel.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "train_model_laplace(DenseNetModel, train_loader, val_loader, optimizer, scheduler, device, \"DenseNetLaplaceModel\", num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
